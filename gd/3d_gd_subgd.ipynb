{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Maximilian Beck\n",
    "# Date: 18.05.22\n",
    "\n",
    "%matplotlib inline\n",
    "from typing import Tuple, Dict, Any\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import copy\n",
    "from IPython.core.display import HTML, display\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import noise\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "%matplotlib qt \n",
    "# for this run pip install pyqt5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Algorithms\n",
    "\n",
    "This snippet implements Gradient Descent, Stochastic Gradient Descent and Subspace Gradient Descent (a variant of gradient descent, which projects update steps into a subspace of the parameter space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, NamedTuple, List\n",
    "\n",
    "\n",
    "class DescentStep(NamedTuple):\n",
    "    theta: np.ndarray\n",
    "    value: float\n",
    "\n",
    "\n",
    "class GradientDescent(object):\n",
    "    def __init__(self, loss_fn: Callable[[np.ndarray], np.ndarray],\n",
    "                 theta: np.ndarray, lr: float, gradient_noise_std: float = 0.1, fd_h: float = 1e-3, seed: int = 1):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.theta = theta\n",
    "        self.lr = lr  # stepsize\n",
    "        self.finite_difference_h = fd_h\n",
    "        self.gradient_noise_std = gradient_noise_std\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.at_boundary = False\n",
    "        self.last_step = DescentStep(theta_0, loss_fn(theta_0))\n",
    "\n",
    "    def _compute_gradient(self):\n",
    "        # forward finite difference gradient\n",
    "        h = self.finite_difference_h\n",
    "        f = self.loss_fn\n",
    "        x = self.theta\n",
    "        gradient = np.zeros_like(x)\n",
    "        try:\n",
    "            for i in range(len(gradient)):\n",
    "                h_vec = np.zeros_like(x)\n",
    "                h_vec[i] = h\n",
    "                gradient[i] = (f(x+h_vec) - f(x))/(h)\n",
    "        except ValueError:\n",
    "            print(\n",
    "                f'Try to compute gradient at function support boundary at {str(x)}. Setting gradient to zero!')\n",
    "            gradient = np.zeros_like(x)\n",
    "        return gradient\n",
    "\n",
    "    def _safe_decent_step_creation(self, theta):\n",
    "        try:\n",
    "            loss = self.loss_fn(theta)\n",
    "            return DescentStep(theta.copy(), loss)\n",
    "        except ValueError:\n",
    "            print('Reached function support boundary!')\n",
    "            self.at_boundary = True\n",
    "            return None\n",
    "\n",
    "    def _get_descent_step(self, theta):\n",
    "        descent_step = self._safe_decent_step_creation(self.theta)\n",
    "        if descent_step is None or self.at_boundary:\n",
    "            print('At function support boundary, using last step.')\n",
    "            return copy.deepcopy(self.last_step)\n",
    "        else:\n",
    "            self.last_step = descent_step\n",
    "        return descent_step\n",
    "\n",
    "    def step(self):\n",
    "        if not self.at_boundary:\n",
    "            grad = self._compute_gradient()\n",
    "            self.theta = self.theta - self.lr * grad\n",
    "        return self._get_descent_step(self.theta)\n",
    "\n",
    "    def noisy_step(self):\n",
    "        if not self.at_boundary:\n",
    "            grad_noise = self.rng.normal(loc=0, scale=self.gradient_noise_std)\n",
    "            grad = self._compute_gradient() + grad_noise\n",
    "            self.theta = self.theta - self.lr * grad\n",
    "        return self._get_descent_step(self.theta)\n",
    "\n",
    "    def subspace_step(self, subspace):\n",
    "        if not self.at_boundary:\n",
    "            grad_noise = self.rng.normal(loc=0, scale=self.gradient_noise_std)\n",
    "            grad = self._compute_gradient() + grad_noise\n",
    "            update_vector = - self.lr * grad\n",
    "            projection = np.dot(subspace, update_vector)\n",
    "            self.theta = self.theta + projection * subspace\n",
    "        return self._get_descent_step(self.theta)\n",
    "            \n",
    "\n",
    "def do_gradient_descent(interp_loss_fn: RegularGridInterpolator, theta_0: np.ndarray, lr: float, num_steps: int,\n",
    "                        gradient_noise_std: float = 0.0, subspace: np.ndarray = None, finite_difference_h: float = 1e-3,\n",
    "                        seed: int = 1) -> List[DescentStep]:\n",
    "    \"\"\"Convenvience function to do a optimization.\"\"\"\n",
    "    optim = GradientDescent(loss_fn=interp_loss_fn, theta=theta_0, lr=lr,\n",
    "                            gradient_noise_std=gradient_noise_std, fd_h=finite_difference_h, seed=seed)\n",
    "    optim_trajectory = []\n",
    "    print('==NEW OPTIMIZATION==')\n",
    "    print(f'Start from theta_0: {theta_0}, Loss: {interp_loss_fn(theta_0)}')\n",
    "    optim_trajectory.append(DescentStep(theta_0.copy(), interp_loss_fn(theta_0))) \n",
    "    for i in range(num_steps):\n",
    "        if subspace is not None:\n",
    "            step = optim.subspace_step(subspace)\n",
    "        else:\n",
    "            step = optim.noisy_step()\n",
    "        optim_trajectory.append(step)\n",
    "        print(f'Step {i}: theta: {step.theta} Loss: {step.value}')\n",
    "    return optim_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss surface / function generation and interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_perlin_noise_loss_surface(shape, scale, octaves, persistence, lacunarity):\n",
    "    \"\"\"Generate a 2D loss surface from perlin noise. Output is a 2D array.\"\"\"\n",
    "    surface = np.zeros(shape)\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            surface[i][j] = noise.pnoise2(i/scale, \n",
    "                                        j/scale, \n",
    "                                        octaves=octaves, \n",
    "                                        persistence=persistence, \n",
    "                                        lacunarity=lacunarity, \n",
    "                                        repeatx=1024, \n",
    "                                        repeaty=1024, \n",
    "                                        base=42)\n",
    "    return surface\n",
    "\n",
    "def interpolate_loss_surface(surface, shape):\n",
    "    \"\"\"Interpolate the discrete loss surface, such that finite difference gradients are applicable.\"\"\"\n",
    "    lin_x = np.linspace(0, 1, shape[0], endpoint=False)\n",
    "    lin_y = np.linspace(0, 1, shape[1], endpoint=False)\n",
    "    interp_loss_surface = RegularGridInterpolator((lin_x, lin_y), surface)\n",
    "    return interp_loss_surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interp_loss_surface(interp_surface: RegularGridInterpolator, shape: Tuple[int, int],\n",
    "                             optim_trajectories: List[DescentStep] = None, styles: List[Dict[str, Any]] = None, xlim: List[float] = None, ylim: List[float] = None):\n",
    "\n",
    "    lin_x = np.linspace(0, 1, shape[0], endpoint=False)\n",
    "    lin_y = np.linspace(0, 1, shape[1], endpoint=False)\n",
    "    x, y = np.meshgrid(lin_x, lin_y)\n",
    "    xy = np.stack([x, y], axis=2)\n",
    "    z = interp_surface(xy)\n",
    "    fig = plt.figure()\n",
    "    # contour plot\n",
    "    ax0 = plt.subplot(1, 2, 1)\n",
    "    ax0.contourf(x, y, z, cmap='terrain')\n",
    "    # 3d plot\n",
    "    # limit 3d surface\n",
    "    if xlim:\n",
    "        assert all([x < 1.0 for x in xlim]) and len(xlim) == 2\n",
    "        x_min_ind = int(x.shape[0] * xlim[0])\n",
    "        x_max_ind = int(x.shape[0] * xlim[1])\n",
    "        x = x[x_min_ind:x_max_ind, :]\n",
    "        y = y[x_min_ind:x_max_ind, :]\n",
    "        z = z[x_min_ind:x_max_ind, :]\n",
    "    if ylim:\n",
    "        assert all([y < 1.0 for y in ylim]) and len(ylim) == 2\n",
    "        y_min_ind = int(y.shape[1] * ylim[0])\n",
    "        y_max_ind = int(y.shape[1] * ylim[1])\n",
    "        x = x[:, y_min_ind:y_max_ind]\n",
    "        y = y[:, y_min_ind:y_max_ind]\n",
    "        z = z[:, y_min_ind:y_max_ind]\n",
    "\n",
    "    ax1 = plt.subplot(1, 2, 2, projection='3d')\n",
    "    ax1.plot_surface(x, y, z, alpha=0.6, cmap='terrain')\n",
    "    if optim_trajectories:\n",
    "        for i, optim_trajectory in enumerate(optim_trajectories):\n",
    "            if styles:\n",
    "                style = styles[i]\n",
    "            else: \n",
    "                style = {'lw': 3}\n",
    "\n",
    "            thetas = np.stack([x.theta for x in optim_trajectory])\n",
    "            ax0.plot(thetas[:, 0], thetas[:, 1], 'o-', **style)\n",
    "            loss_values = np.array([x.value for x in optim_trajectory])\n",
    "            ax1.plot3D(thetas[:, 0],\n",
    "                       thetas[:, 1],\n",
    "                       loss_values[:, 0],\n",
    "                       'o-',\n",
    "                       **style)\n",
    "\n",
    "    if xlim:\n",
    "        ax0.set_xlim(xlim)\n",
    "        ax1.set_xlim(xlim)\n",
    "    if ylim:\n",
    "        ax0.set_ylim(ylim)\n",
    "        ax1.set_ylim(ylim)\n",
    "    # plt.figlegend()\n",
    "    ax0.legend()\n",
    "    ax1.legend()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting with Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.11.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotly plot\n",
    "from typing import Tuple, Dict, Any\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "# Convert Matplotlib colourmap to plotly colorscale\n",
    "def matplotlib_cmap_to_plotly(cmap_name: str = 'terrain', pl_entries: int = 255) -> List[List[Any]]:\n",
    "    cmap = mpl.cm.get_cmap(cmap_name)\n",
    "    h = 1.0/(pl_entries-1)\n",
    "    pl_colorscale = []\n",
    "\n",
    "    for k in range(pl_entries):\n",
    "        C = list(map(np.uint8, np.array(cmap(k*h)[:3])*255))\n",
    "        pl_colorscale.append([k*h, 'rgb'+str((C[0], C[1], C[2]))])\n",
    "\n",
    "    return pl_colorscale\n",
    "\n",
    "\n",
    "def plotly_interp_loss_surface(interp_surface: RegularGridInterpolator, shape: Tuple[int, int],\n",
    "                               optim_trajectories: List[DescentStep] = None,\n",
    "                               optimum: DescentStep = None, styles: List[Dict[str, Any]] = None,\n",
    "                               xlim: List[float] = None, ylim: List[float] = None):\n",
    "    # 3D surface data\n",
    "    lin_x = np.linspace(0, 1, shape[0], endpoint=False)\n",
    "    lin_y = np.linspace(0, 1, shape[1], endpoint=False)\n",
    "    x, y = np.meshgrid(lin_x, lin_y)\n",
    "    xy = np.stack([x, y], axis=2)\n",
    "    z = interp_surface(xy)\n",
    "    # limit 3D surface bounds\n",
    "    if xlim:\n",
    "        assert all([x < 1.0 for x in xlim]) and len(xlim) == 2\n",
    "        x_min_ind = int(x.shape[0] * xlim[0])\n",
    "        x_max_ind = int(x.shape[0] * xlim[1])\n",
    "        x = x[x_min_ind:x_max_ind, :]\n",
    "        y = y[x_min_ind:x_max_ind, :]\n",
    "        z = z[x_min_ind:x_max_ind, :]\n",
    "    if ylim:\n",
    "        assert all([y < 1.0 for y in ylim]) and len(ylim) == 2\n",
    "        y_min_ind = int(y.shape[1] * ylim[0])\n",
    "        y_max_ind = int(y.shape[1] * ylim[1])\n",
    "        x = x[:, y_min_ind:y_max_ind]\n",
    "        y = y[:, y_min_ind:y_max_ind]\n",
    "        z = z[:, y_min_ind:y_max_ind]\n",
    "\n",
    "    # create plotly plot\n",
    "    go_plots = []\n",
    "    pl_cmap = matplotlib_cmap_to_plotly('terrain', 255)\n",
    "    # plot 3D surface\n",
    "    pl_surface = go.Surface(x=x, y=y, z=z, colorscale=pl_cmap, opacity=0.7, name='Loss Surface')\n",
    "    go_plots.append(pl_surface)\n",
    "\n",
    "    # plot optimum marker\n",
    "    if optimum:\n",
    "        optimum_color = '#de425b'\n",
    "\n",
    "        # show a cone pointing in upward direction\n",
    "        # pl_optimum = go.Cone(x=[optimum.theta[0]], y=[optimum.theta[1]], z=[optimum.value[0]],\n",
    "        #                      u=[0], v=[0], w=[0.1], sizemode='absolute', sizeref=1, anchor='tail')\n",
    "        # show a maker at optimum position\n",
    "        pl_optimum = go.Scatter3d(x=[optimum.theta[0]], y=[optimum.theta[1]], z=[optimum.value[0]],\n",
    "                                mode='markers', name='Optimum', marker=dict(color=optimum_color, size=8, symbol='diamond'))\n",
    "        go_plots.append(pl_optimum)\n",
    "\n",
    "    # plot trajectories\n",
    "    if optim_trajectories:\n",
    "        for i, optim_trajectory in enumerate(optim_trajectories):\n",
    "            if styles:\n",
    "                style = styles[i]\n",
    "            else:\n",
    "                style = {'lw': 3}\n",
    "\n",
    "            thetas = np.stack([x.theta for x in optim_trajectory])\n",
    "            loss_values = np.array([x.value for x in optim_trajectory])\n",
    "\n",
    "            pl_optim_traj = go.Scatter3d(x=thetas[:, 0], y=thetas[:, 1], z=loss_values[:, 0], mode='lines',\n",
    "                                         line=dict(color=style['c'], width=style['lw']),\n",
    "                                         name=style['label'])\n",
    "            go_plots.append(pl_optim_traj)\n",
    "\n",
    "    # if xlim:\n",
    "    #     ax0.set_xlim(xlim)\n",
    "    #     ax1.set_xlim(xlim)\n",
    "    # if ylim:\n",
    "    #     ax0.set_ylim(ylim)\n",
    "    #     ax1.set_ylim(ylim)\n",
    "\n",
    "    fig = go.Figure(data=go_plots)\n",
    "    fig.update_layout(title='SubGD optimization trajectory on random loss surface')\n",
    "    # legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            x=0,\n",
    "            y=1,\n",
    "            traceorder=\"reversed\",\n",
    "            title_font_family=\"Times New Roman\",\n",
    "            font=dict(\n",
    "                # family=\"Courier\",\n",
    "                size=14,\n",
    "                color=\"black\"\n",
    "            ),\n",
    "            bgcolor=\"LightSteelBlue\",\n",
    "            bordercolor=\"Black\",\n",
    "            borderwidth=2\n",
    "        ))\n",
    "    # scene camera\n",
    "    fig.update_layout(\n",
    "        scene_camera=dict(\n",
    "            up=dict(x=0, y=0, z=1),\n",
    "            center=dict(x=0, y=0, z=0),\n",
    "            eye=dict(x=1.25, y=-1.25, z=1.25)\n",
    "        ))\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization of a quadratic loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==NEW OPTIMIZATION==\n",
      "Start from theta_0: [0.8 0.6], Loss: [1.]\n",
      "Step 0: theta: [0.638 0.478] Loss: [0.6356]\n",
      "Step 1: theta: [0.512 0.384] Loss: [0.40976]\n",
      "Step 2: theta: [0.41  0.306] Loss: [0.26192]\n",
      "Step 3: theta: [0.328 0.244] Loss: [0.16728]\n",
      "Step 4: theta: [0.262 0.194] Loss: [0.1064]\n",
      "Step 5: theta: [0.208 0.156] Loss: [0.06776]\n",
      "Step 6: theta: [0.166 0.126] Loss: [0.0436]\n",
      "Step 7: theta: [0.132 0.1  ] Loss: [0.02752]\n",
      "Step 8: theta: [0.106 0.078] Loss: [0.01744]\n",
      "Step 9: theta: [0.084 0.064] Loss: [0.01128]\n"
     ]
    }
   ],
   "source": [
    "## Generate quadratic loss\n",
    "# optimize quadratic loss for testing\n",
    "shape = (50,50)\n",
    "def quadratic_loss(x):\n",
    "    assert len(x) == 2\n",
    "    z = x[0]**2 + x[1]**2\n",
    "    return z\n",
    "lin_x = np.linspace(0,1,shape[0],endpoint=False)\n",
    "lin_y = np.linspace(0,1,shape[1],endpoint=False)\n",
    "x,y = np.meshgrid(lin_x, lin_y, sparse=False)\n",
    "z = x**2+y**2\n",
    "interp_quadratic_loss = interpolate_loss_surface(z, shape)\n",
    "\n",
    "## Do optimization\n",
    "theta_0 = np.array([0.8,0.6])\n",
    "optim_trajectory = do_gradient_descent(interp_quadratic_loss, theta_0, lr=0.1, num_steps=10)\n",
    "fig = plot_interp_loss_surface(interp_quadratic_loss, shape, [optim_trajectory])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization of a Perlin noise loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==NEW OPTIMIZATION==\n",
      "Start from theta_0: [0.29 0.36], Loss: [0.06060266]\n",
      "Step 0: theta: [0.29153945 0.35873151] Loss: [0.06012539]\n",
      "Step 1: theta: [0.29309408 0.35683724] Loss: [0.05952135]\n",
      "Step 2: theta: [0.29467139 0.35492436] Loss: [0.05890304]\n",
      "Step 3: theta: [0.2962716 0.3529926] Loss: [0.0582701]\n",
      "Step 4: theta: [0.29789493 0.35104169] Loss: [0.05762219]\n",
      "Step 5: theta: [0.29954161 0.34907135] Loss: [0.05695892]\n",
      "Step 6: theta: [0.30266215 0.34708129] Loss: [0.05529945]\n",
      "Step 7: theta: [0.30711638 0.34494365] Loss: [0.05280766]\n",
      "Step 8: theta: [0.3116847  0.34256823] Loss: [0.05009851]\n",
      "Step 9: theta: [0.31637983 0.33994896] Loss: [0.04713438]\n",
      "Step 10: theta: [0.32121462 0.33699895] Loss: [0.04341777]\n",
      "Step 11: theta: [0.32617934 0.33236202] Loss: [0.03877331]\n",
      "Step 12: theta: [0.3312035  0.32766147] Loss: [0.0340093]\n",
      "Step 13: theta: [0.3362879  0.32289652] Loss: [0.02912266]\n",
      "Step 14: theta: [0.34143337 0.3180664 ] Loss: [0.02456943]\n",
      "Step 15: theta: [0.34533662 0.31465506] Loss: [0.02193621]\n",
      "Step 16: theta: [0.34910144 0.3114021 ] Loss: [0.01951035]\n",
      "Step 17: theta: [0.35273426 0.30830192] Loss: [0.0172752]\n",
      "Step 18: theta: [0.35624128 0.30534914] Loss: [0.01521541]\n",
      "Step 19: theta: [0.35962848 0.30253868] Loss: [0.01331685]\n",
      "Step 20: theta: [0.36393714 0.29986566] Loss: [0.01050852]\n",
      "Step 21: theta: [0.36880242 0.29670784] Loss: [0.00613662]\n",
      "Step 22: theta: [0.37337032 0.29035913] Loss: [0.00029256]\n",
      "Step 23: theta: [0.37734031 0.28444061] Loss: [-0.00456514]\n",
      "Step 24: theta: [0.38075293 0.27889597] Loss: [-0.00838424]\n",
      "Step 25: theta: [0.38255082 0.27515745] Loss: [-0.01000476]\n",
      "Step 26: theta: [0.3837904  0.27168742] Loss: [-0.01129829]\n",
      "Step 27: theta: [0.38451177 0.26840251] Loss: [-0.01239401]\n",
      "Step 28: theta: [0.38474258 0.26522532] Loss: [-0.01339784]\n",
      "Step 29: theta: [0.38449892 0.26208261] Loss: [-0.01440288]\n",
      "Step 30: theta: [0.38378592 0.2589035 ] Loss: [-0.01518304]\n",
      "Step 31: theta: [0.38278348 0.25849243] Loss: [-0.01529962]\n",
      "Step 32: theta: [0.38178911 0.25810108] Loss: [-0.01541305]\n",
      "Step 33: theta: [0.38080244 0.25772927] Loss: [-0.0155235]\n",
      "Step 34: theta: [0.37982308 0.25737686] Loss: [-0.01558989]\n",
      "Step 35: theta: [0.37926328 0.25703854] Loss: [-0.0155254]\n",
      "Step 36: theta: [0.38000847 0.25669489] Loss: [-0.0156361]\n",
      "Step 37: theta: [0.37904944 0.25635809] Loss: [-0.01551991]\n",
      "Step 38: theta: [0.38028576 0.25601242] Loss: [-0.01563286]\n",
      "Step 39: theta: [0.37934015 0.25567016] Loss: [-0.01558275]\n",
      "Step 40: theta: [0.37990737 0.25532725] Loss: [-0.01567053]\n",
      "Step 41: theta: [0.37918573 0.25498973] Loss: [-0.01558543]\n",
      "Step 42: theta: [0.38010264 0.25464535] Loss: [-0.01569647]\n",
      "Step 43: theta: [0.37918391 0.2543067 ] Loss: [-0.01560871]\n",
      "Step 44: theta: [0.38010211 0.2539623 ] Loss: [-0.01571966]\n",
      "Step 45: theta: [0.3791968  0.25362366] Loss: [-0.01563394]\n",
      "Step 46: theta: [0.38008351 0.25327938] Loss: [-0.01574444]\n",
      "Step 47: theta: [0.37919163 0.25294111] Loss: [-0.01565676]\n",
      "Step 48: theta: [0.38008716 0.25259678] Loss: [-0.01576721]\n",
      "Step 49: theta: [0.3792087  0.25225843] Loss: [-0.0156825]\n",
      "Step 50: theta: [0.38006429 0.25191427] Loss: [-0.01579228]\n",
      "Step 51: theta: [0.37919925 0.25157637] Loss: [-0.01570474]\n",
      "Step 52: theta: [0.38007294 0.25123212] Loss: [-0.01581459]\n",
      "Step 53: theta: [0.37922131 0.25089405] Loss: [-0.0157311]\n",
      "Step 54: theta: [0.3800452  0.25055001] Loss: [-0.01583998]\n",
      "Step 55: theta: [0.37920698 0.25021249] Loss: [-0.0157527]\n",
      "Step 56: theta: [0.38005922 0.24986831] Loss: [-0.01586183]\n",
      "Step 57: theta: [0.3792344  0.24953051] Loss: [-0.01577969]\n",
      "Step 58: theta: [0.38002659 0.24918659] Loss: [-0.01588751]\n",
      "Step 59: theta: [0.37921518 0.24884943] Loss: [-0.01580066]\n",
      "Step 60: theta: [0.3800456  0.24850533] Loss: [-0.01590896]\n",
      "Step 61: theta: [0.37924758 0.2481678 ] Loss: [-0.01582823]\n",
      "Step 62: theta: [0.38000893 0.24782401] Loss: [-0.01593484]\n",
      "Step 63: theta: [0.3792243 0.2474872] Loss: [-0.01584869]\n",
      "Step 64: theta: [0.38003167 0.24714318] Loss: [-0.01595601]\n",
      "Step 65: theta: [0.37926043 0.24680592] Loss: [-0.01587667]\n",
      "Step 66: theta: [0.37999266 0.24646225] Loss: [-0.01598046]\n",
      "Step 67: theta: [0.37924959 0.24612555] Loss: [-0.0158987]\n",
      "Step 68: theta: [0.38000224 0.24578178] Loss: [-0.01600412]\n",
      "Step 69: theta: [0.37925776 0.2454451 ] Loss: [-0.01592311]\n",
      "Step 70: theta: [0.37999268 0.2451014 ] Loss: [-0.01602628]\n",
      "Step 71: theta: [0.37927603 0.2447647 ] Loss: [-0.01594876]\n",
      "Step 72: theta: [0.3799737  0.24442118] Loss: [-0.01604684]\n",
      "Step 73: theta: [0.37930737 0.24408429] Loss: [-0.015976]\n",
      "Step 74: theta: [0.37994311 0.24374107] Loss: [-0.01606598]\n",
      "Step 75: theta: [0.37934881 0.24340389] Loss: [-0.01600443]\n",
      "Step 76: theta: [0.37990443 0.24306106] Loss: [-0.01608418]\n",
      "Step 77: theta: [0.37939643 0.24272352] Loss: [-0.01603357]\n",
      "Step 78: theta: [0.37986178 0.24238114] Loss: [-0.01610194]\n",
      "Step 79: theta: [0.37944606 0.24204319] Loss: [-0.01606289]\n",
      "Step 80: theta: [0.37981926 0.24170128] Loss: [-0.01611976]\n",
      "Step 81: theta: [0.3794939  0.24136293] Loss: [-0.01609192]\n",
      "Step 82: theta: [0.37978025 0.24102148] Loss: [-0.01613807]\n",
      "Step 83: theta: [0.37953704 0.24068275] Loss: [-0.01612033]\n",
      "Step 84: theta: [0.37974712 0.24034172] Loss: [-0.01615713]\n",
      "Step 85: theta: [0.37957372 0.24000268] Loss: [-0.01614791]\n",
      "Step 86: theta: [0.37972102 0.23966199] Loss: [-0.01626725]\n",
      "Step 87: theta: [0.37961229 0.23842079] Loss: [-0.01662787]\n",
      "Step 88: theta: [0.37973572 0.23541125] Loss: [-0.01754731]\n",
      "Step 89: theta: [0.37976674 0.23240398] Loss: [-0.01845495]\n",
      "Step 90: theta: [0.37987714 0.22939729] Loss: [-0.01937]\n",
      "Step 91: theta: [0.38000236 0.22639263] Loss: [-0.02028444]\n",
      "Step 92: theta: [0.38018468 0.22339009] Loss: [-0.02119256]\n",
      "Step 93: theta: [0.38054642 0.22037665] Loss: [-0.02212024]\n",
      "Step 94: theta: [0.38108823 0.2173416 ] Loss: [-0.02402403]\n",
      "Step 95: theta: [0.3817038  0.21072522] Loss: [-0.02844742]\n",
      "Step 96: theta: [0.38244693 0.20409698] Loss: [-0.0329055]\n",
      "Step 97: theta: [0.38331784 0.19745441] Loss: [-0.03689537]\n",
      "Step 98: theta: [0.38420953 0.19279654] Loss: [-0.03913497]\n",
      "Step 99: theta: [0.38499474 0.18815905] Loss: [-0.04133893]\n",
      "Step 100: theta: [0.38567393 0.18353952] Loss: [-0.04351189]\n",
      "Step 101: theta: [0.3862475  0.17893551] Loss: [-0.04534057]\n",
      "Step 102: theta: [0.38672387 0.17733072] Loss: [-0.04561963]\n",
      "Step 103: theta: [0.38717571 0.17573321] Loss: [-0.04589415]\n",
      "Step 104: theta: [0.38760311 0.17414261] Loss: [-0.04616438]\n",
      "Step 105: theta: [0.38800618 0.17255855] Loss: [-0.04643057]\n",
      "Step 106: theta: [0.38838503 0.17098065] Loss: [-0.04669299]\n",
      "Step 107: theta: [0.38873975 0.16940855] Loss: [-0.04695187]\n",
      "Step 108: theta: [0.38907043 0.16784187] Loss: [-0.04720746]\n",
      "Step 109: theta: [0.38937715 0.16628025] Loss: [-0.04746]\n",
      "Step 110: theta: [0.38965998 0.16472332] Loss: [-0.04770973]\n",
      "Step 111: theta: [0.38991901 0.16317072] Loss: [-0.04795688]\n",
      "Step 112: theta: [0.39015429 0.16162207] Loss: [-0.04820169]\n",
      "Step 113: theta: [0.39036589 0.16007703] Loss: [-0.04844438]\n",
      "Step 114: theta: [0.39055386 0.15853522] Loss: [-0.04874663]\n",
      "Step 115: theta: [0.39073578 0.15657686] Loss: [-0.04913334]\n",
      "Step 116: theta: [0.39091119 0.1546191 ] Loss: [-0.04951958]\n",
      "Step 117: theta: [0.39108009 0.15266193] Loss: [-0.04990537]\n",
      "Step 118: theta: [0.39124249 0.15070532] Loss: [-0.05029074]\n",
      "Step 119: theta: [0.39139837 0.14874925] Loss: [-0.05067569]\n",
      "Step 120: theta: [0.39154776 0.1467937 ] Loss: [-0.05106024]\n",
      "Step 121: theta: [0.39169064 0.14483865] Loss: [-0.05144441]\n",
      "Step 122: theta: [0.39182703 0.14288407] Loss: [-0.05182822]\n",
      "Step 123: theta: [0.39195691 0.14092994] Loss: [-0.05221168]\n",
      "Step 124: theta: [0.3920803  0.13897625] Loss: [-0.05271664]\n",
      "Step 125: theta: [0.39220042 0.13583305] Loss: [-0.05370604]\n",
      "Step 126: theta: [0.39232001 0.13268988] Loss: [-0.05469542]\n",
      "Step 127: theta: [0.39243904 0.12954672] Loss: [-0.05568477]\n",
      "Step 128: theta: [0.39255754 0.12640359] Loss: [-0.0566741]\n",
      "Step 129: theta: [0.3926755  0.12326047] Loss: [-0.0576634]\n",
      "Step 130: theta: [0.39279292 0.12011738] Loss: [-0.05865268]\n",
      "Step 131: theta: [0.3929098  0.11697431] Loss: [-0.05915178]\n",
      "Step 132: theta: [0.39313355 0.11545119] Loss: [-0.05938998]\n",
      "Step 133: theta: [0.3934111  0.11392018] Loss: [-0.05963359]\n",
      "Step 134: theta: [0.39374275 0.11237935] Loss: [-0.05988381]\n",
      "Step 135: theta: [0.39412883 0.11082681] Loss: [-0.06014187]\n",
      "Step 136: theta: [0.39456976 0.10926063] Loss: [-0.06040904]\n",
      "Step 137: theta: [0.39506602 0.10767887] Loss: [-0.06068664]\n",
      "Step 138: theta: [0.39561816 0.10607958] Loss: [-0.06097602]\n",
      "Step 139: theta: [0.39622679 0.10446079] Loss: [-0.06127859]\n",
      "Step 140: theta: [0.39689262 0.10282049] Loss: [-0.06159584]\n",
      "Step 141: theta: [0.3976164  0.10115667] Loss: [-0.06192931]\n",
      "Step 142: theta: [0.39839896 0.09946728] Loss: [-0.06222965]\n",
      "Step 143: theta: [0.39921818 0.09825993] Loss: [-0.06238778]\n",
      "Step 144: theta: [0.39991615 0.09750612] Loss: [-0.0625007]\n",
      "Step 145: theta: [0.40032146 0.09675781] Loss: [-0.06257752]\n",
      "Step 146: theta: [0.40076106 0.09597976] Loss: [-0.06266061]\n",
      "Step 147: theta: [0.40127422 0.09516015] Loss: [-0.0627581]\n",
      "Step 148: theta: [0.40186489 0.09429201] Loss: [-0.0628732]\n",
      "Step 149: theta: [0.40253764 0.09336803] Loss: [-0.06300972]\n",
      "Step 150: theta: [0.40329776 0.09238043] Loss: [-0.06317213]\n",
      "Step 151: theta: [0.40415127 0.09132095] Loss: [-0.06336577]\n",
      "Step 152: theta: [0.40510496 0.09018078] Loss: [-0.06359701]\n",
      "Step 153: theta: [0.40616645 0.08895042] Loss: [-0.06387341]\n",
      "Step 154: theta: [0.40734428 0.0876197 ] Loss: [-0.06420404]\n",
      "Step 155: theta: [0.40864794 0.0861776 ] Loss: [-0.06459974]\n",
      "Step 156: theta: [0.41008796 0.08461224] Loss: [-0.06507346]\n",
      "Step 157: theta: [0.411676   0.08291071] Loss: [-0.06564071]\n",
      "Step 158: theta: [0.41342492 0.08105902] Loss: [-0.06632008]\n",
      "Step 159: theta: [0.41534893 0.07904197] Loss: [-0.06715736]\n",
      "Step 160: theta: [0.41726299 0.07660749] Loss: [-0.06806536]\n",
      "Step 161: theta: [0.41889732 0.07438262] Loss: [-0.06878568]\n",
      "Step 162: theta: [0.420276   0.07234556] Loss: [-0.06923995]\n",
      "Step 163: theta: [0.41712724 0.07045721] Loss: [-0.06942112]\n",
      "Step 164: theta: [0.41805486 0.06821675] Loss: [-0.06998525]\n",
      "Step 165: theta: [0.41872505 0.06608288] Loss: [-0.07046907]\n",
      "Step 166: theta: [0.41915003 0.06402601] Loss: [-0.07090016]\n",
      "Step 167: theta: [0.41873842 0.06201798] Loss: [-0.07130511]\n",
      "Step 168: theta: [0.41869632 0.05996266] Loss: [-0.07173276]\n",
      "Step 169: theta: [0.41841947 0.05786207] Loss: [-0.07240507]\n",
      "Step 170: theta: [0.41798045 0.05469798] Loss: [-0.07343621]\n",
      "Step 171: theta: [0.41729717 0.05150001] Loss: [-0.07452247]\n",
      "Step 172: theta: [0.41636701 0.04824928] Loss: [-0.07568906]\n",
      "Step 173: theta: [0.41518589 0.04492674] Loss: [-0.07696278]\n",
      "Step 174: theta: [0.41374828 0.04151303] Loss: [-0.07837269]\n",
      "Step 175: theta: [0.41204713 0.03798833] Loss: [-0.07996424]\n",
      "Step 176: theta: [0.41036292 0.03426507] Loss: [-0.08159247]\n",
      "Step 177: theta: [0.40892622 0.03065378] Loss: [-0.08306853]\n",
      "Step 178: theta: [0.40772961 0.027138  ] Loss: [-0.08441982]\n",
      "Step 179: theta: [0.40676671 0.02370177] Loss: [-0.08567131]\n",
      "Step 180: theta: [0.40603226 0.02032956] Loss: [-0.08684597]\n",
      "Step 181: theta: [0.40552199 0.01700616] Loss: [-0.08766428]\n",
      "Step 182: theta: [0.40507104 0.01472194] Loss: [-0.0882051]\n",
      "Step 183: theta: [0.40464865 0.01244334] Loss: [-0.08874093]\n",
      "Step 184: theta: [0.40425475 0.01017003] Loss: [-0.08927212]\n",
      "Step 185: theta: [0.40388925 0.00790164] Loss: [-0.089799]\n",
      "Step 186: theta: [0.40355211 0.00563782] Loss: [-0.09032191]\n",
      "Step 187: theta: [0.40324327 0.00337822] Loss: [-0.09084115]\n",
      "Step 188: theta: [0.40296267 0.00112247] Loss: [-0.09135707]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 189: theta: [0.40296267 0.00112247] Loss: [-0.09135707]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 190: theta: [0.40296267 0.00112247] Loss: [-0.09135707]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 191: theta: [0.40296267 0.00112247] Loss: [-0.09135707]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 192: theta: [0.40296267 0.00112247] Loss: [-0.09135707]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 193: theta: [0.40296267 0.00112247] Loss: [-0.09135707]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 194: theta: [0.40296267 0.00112247] Loss: [-0.09135707]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 195: theta: [0.40296267 0.00112247] Loss: [-0.09135707]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 196: theta: [0.40296267 0.00112247] Loss: [-0.09135707]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 197: theta: [0.40296267 0.00112247] Loss: [-0.09135707]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 198: theta: [0.40296267 0.00112247] Loss: [-0.09135707]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 199: theta: [0.40296267 0.00112247] Loss: [-0.09135707]\n",
      "==NEW OPTIMIZATION==\n",
      "Start from theta_0: [0.29 0.36], Loss: [0.06060266]\n",
      "Step 0: theta: [0.29021608 0.35740813] Loss: [0.06008254]\n",
      "Step 1: theta: [0.29544579 0.35918894] Loss: [0.05960688]\n",
      "Step 2: theta: [0.29988639 0.36031433] Loss: [0.05910611]\n",
      "Step 3: theta: [0.32049407 0.37659635] Loss: [0.05219366]\n",
      "Step 4: theta: [0.31208968 0.3634832 ] Loss: [0.05473251]\n",
      "Step 5: theta: [0.30789402 0.35484072] Loss: [0.05484446]\n",
      "Step 6: theta: [0.314212   0.35470175] Loss: [0.0522537]\n",
      "Step 7: theta: [0.3128428  0.34653093] Loss: [0.05061721]\n",
      "Step 8: theta: [0.31535793 0.34188137] Loss: [0.04818051]\n",
      "Step 9: theta: [0.32396648 0.34294278] Loss: [0.04441186]\n",
      "Step 10: theta: [0.3219067  0.33284383] Loss: [0.04114379]\n",
      "Step 11: theta: [0.32909857 0.33037193] Loss: [0.03636374]\n",
      "Step 12: theta: [0.33645    0.32793572] Loss: [0.03147429]\n",
      "Step 13: theta: [0.34707592 0.32864855] Loss: [0.02687299]\n",
      "Step 14: theta: [0.34833628 0.32096526] Loss: [0.02290823]\n",
      "Step 15: theta: [0.35306408 0.31722858] Loss: [0.01978342]\n",
      "Step 16: theta: [0.35311631 0.31047216] Loss: [0.01777867]\n",
      "Step 17: theta: [0.36096169 0.31178519] Loss: [0.01516468]\n",
      "Step 18: theta: [0.36515073 0.30825568] Loss: [0.01213054]\n",
      "Step 19: theta: [0.37641392 0.31193058] Loss: [0.00735446]\n",
      "Step 20: theta: [0.37560295 0.3036594 ] Loss: [0.00578575]\n",
      "Step 21: theta: [0.3792264  0.29994853] Loss: [0.00311434]\n",
      "Step 22: theta: [0.38173785 0.29514495] Loss: [-0.00050306]\n",
      "Step 23: theta: [0.38287687 0.28721247] Loss: [-0.00485081]\n",
      "Step 24: theta: [0.3928911  0.28938298] Loss: [-0.00702351]\n",
      "Step 25: theta: [0.39064152 0.28035128] Loss: [-0.00977701]\n",
      "Step 26: theta: [0.3782549  0.26210047] Loss: [-0.0144456]\n",
      "Step 27: theta: [0.39127785 0.26957816] Loss: [-0.01229544]\n",
      "Step 28: theta: [0.40379011 0.27951729] Loss: [-0.01248306]\n",
      "Step 29: theta: [0.41615001 0.28845289] Loss: [-0.01286479]\n",
      "Step 30: theta: [0.41230472 0.28033631] Loss: [-0.01395526]\n",
      "Step 31: theta: [0.41326725 0.27712299] Loss: [-0.01422352]\n",
      "Step 32: theta: [0.40739641 0.26953343] Loss: [-0.01354631]\n",
      "Step 33: theta: [0.40354613 0.26407104] Loss: [-0.01343358]\n",
      "Step 34: theta: [0.40294001 0.26195282] Loss: [-0.0135254]\n",
      "Step 35: theta: [0.40168822 0.25928273] Loss: [-0.01363785]\n",
      "Step 36: theta: [0.40351544 0.25969307] Loss: [-0.0137199]\n",
      "Step 37: theta: [0.39806163 0.25290658] Loss: [-0.0141668]\n",
      "Step 38: theta: [0.4050851  0.26012287] Loss: [-0.0137838]\n",
      "Step 39: theta: [0.40866106 0.26252713] Loss: [-0.01392797]\n",
      "Step 40: theta: [0.4077326  0.26049964] Loss: [-0.01393424]\n",
      "Step 41: theta: [0.39576898 0.24750519] Loss: [-0.01471888]\n",
      "Step 42: theta: [0.40034187 0.25220978] Loss: [-0.01407598]\n",
      "Step 43: theta: [0.40876654 0.25902214] Loss: [-0.0141166]\n",
      "Step 44: theta: [0.41335701 0.26196278] Loss: [-0.01429255]\n",
      "Step 45: theta: [0.4073092  0.25514227] Loss: [-0.01439857]\n",
      "Step 46: theta: [0.40972916 0.25581736] Loss: [-0.01451525]\n",
      "Step 47: theta: [0.40121147 0.24549748] Loss: [-0.01466834]\n",
      "Step 48: theta: [0.41541093 0.25783556] Loss: [-0.01469347]\n",
      "Step 49: theta: [0.40819771 0.2486998 ] Loss: [-0.01511112]\n",
      "Step 50: theta: [0.4019402  0.24045659] Loss: [-0.01514793]\n",
      "Step 51: theta: [0.41312822 0.24959368] Loss: [-0.01549375]\n",
      "Step 52: theta: [0.41301034 0.2473575 ] Loss: [-0.01574125]\n",
      "Step 53: theta: [0.4055307  0.23768996] Loss: [-0.01662728]\n",
      "Step 54: theta: [0.40621768 0.23280504] Loss: [-0.01880673]\n",
      "Step 55: theta: [0.40058574 0.22152869] Loss: [-0.02277929]\n",
      "Step 56: theta: [0.38547423 0.20069934] Loss: [-0.03544589]\n",
      "Step 57: theta: [0.38449312 0.19208088] Loss: [-0.03948866]\n",
      "Step 58: theta: [0.38722464 0.18941255] Loss: [-0.04091771]\n",
      "Step 59: theta: [0.39332985 0.19024136] Loss: [-0.04098278]\n",
      "Step 60: theta: [0.38952019 0.18127593] Loss: [-0.04475476]\n",
      "Step 61: theta: [0.39141912 0.17813697] Loss: [-0.04570877]\n",
      "Step 62: theta: [0.3931345  0.17786249] Loss: [-0.04582955]\n",
      "Step 63: theta: [0.39433129 0.17709984] Loss: [-0.04599756]\n",
      "Step 64: theta: [0.3902305  0.17106959] Loss: [-0.04674472]\n",
      "Step 65: theta: [0.39805096 0.17699008] Loss: [-0.04617994]\n",
      "Step 66: theta: [0.40920671 0.18627494] Loss: [-0.04359628]\n",
      "Step 67: theta: [0.4267136  0.19914798] Loss: [-0.03579364]\n",
      "Step 68: theta: [0.41367739 0.18645197] Loss: [-0.04373354]\n",
      "Step 69: theta: [0.41363274 0.18182589] Loss: [-0.04563278]\n",
      "Step 70: theta: [0.40347734 0.16714499] Loss: [-0.04753276]\n",
      "Step 71: theta: [0.40323201 0.16600288] Loss: [-0.04767944]\n",
      "Step 72: theta: [0.40805678 0.16997978] Loss: [-0.04712097]\n",
      "Step 73: theta: [0.40455788 0.16567926] Loss: [-0.04766743]\n",
      "Step 74: theta: [0.40470603 0.16506951] Loss: [-0.04773105]\n",
      "Step 75: theta: [0.4130641  0.17271103] Loss: [-0.04685791]\n",
      "Step 76: theta: [0.41925514 0.17822459] Loss: [-0.04666539]\n",
      "Step 77: theta: [0.40604455 0.16551293] Loss: [-0.04762746]\n",
      "Step 78: theta: [0.40316705 0.16196769] Loss: [-0.04817484]\n",
      "Step 79: theta: [0.39966193 0.15783126] Loss: [-0.04904805]\n",
      "Step 80: theta: [0.40108473 0.15783904] Loss: [-0.04895866]\n",
      "Step 81: theta: [0.40504657 0.16082469] Loss: [-0.0481918]\n",
      "Step 82: theta: [0.39815221 0.15346389] Loss: [-0.0498652]\n",
      "Step 83: theta: [0.39904966 0.15226317] Loss: [-0.05011176]\n",
      "Step 84: theta: [0.40445188 0.15564682] Loss: [-0.04901335]\n",
      "Step 85: theta: [0.40435368 0.15500582] Loss: [-0.04912529]\n",
      "Step 86: theta: [0.40960585 0.15975751] Loss: [-0.04799454]\n",
      "Step 87: theta: [0.40755888 0.15748208] Loss: [-0.04844562]\n",
      "Step 88: theta: [0.39876025 0.1482398 ] Loss: [-0.05088404]\n",
      "Step 89: theta: [0.40476054 0.15216132] Loss: [-0.04952362]\n",
      "Step 90: theta: [0.39345191 0.14060568] Loss: [-0.05229332]\n",
      "Step 91: theta: [0.39824756 0.1433303 ] Loss: [-0.05182537]\n",
      "Step 92: theta: [0.39730504 0.14032363] Loss: [-0.05239505]\n",
      "Step 93: theta: [0.40328112 0.14424243] Loss: [-0.05104063]\n",
      "Step 94: theta: [0.40291061 0.14412691] Loss: [-0.05113164]\n",
      "Step 95: theta: [0.40064361 0.14209503] Loss: [-0.05195168]\n",
      "Step 96: theta: [0.40159284 0.14326104] Loss: [-0.05154274]\n",
      "Step 97: theta: [0.40451038 0.14637848] Loss: [-0.05046735]\n",
      "Step 98: theta: [0.40749595 0.14954834] Loss: [-0.04951642]\n",
      "Step 99: theta: [0.41173154 0.15395387] Loss: [-0.04842976]\n",
      "Step 100: theta: [0.42155302 0.16393204] Loss: [-0.0466948]\n",
      "Step 101: theta: [0.42064715 0.16531462] Loss: [-0.04689273]\n",
      "Step 102: theta: [0.41495474 0.1621385 ] Loss: [-0.04746899]\n",
      "Step 103: theta: [0.40801483 0.15520091] Loss: [-0.0487019]\n",
      "Step 104: theta: [0.40241324 0.14936905] Loss: [-0.05031637]\n",
      "Step 105: theta: [0.38368538 0.13042887] Loss: [-0.05530364]\n",
      "Step 106: theta: [0.38157326 0.12505346] Loss: [-0.05696915]\n",
      "Step 107: theta: [0.38488533 0.12510279] Loss: [-0.05699262]\n",
      "Step 108: theta: [0.37190162 0.10885692] Loss: [-0.05739083]\n",
      "Step 109: theta: [0.38211946 0.11427115] Loss: [-0.05921903]\n",
      "Step 110: theta: [0.37565936 0.1063499 ] Loss: [-0.05885743]\n",
      "Step 111: theta: [0.38496978 0.11151368] Loss: [-0.05965267]\n",
      "Step 112: theta: [0.38290766 0.10779229] Loss: [-0.06000206]\n",
      "Step 113: theta: [0.39723457 0.12040131] Loss: [-0.05861537]\n",
      "Step 114: theta: [0.39105658 0.11096409] Loss: [-0.05998639]\n",
      "Step 115: theta: [0.39260039 0.11061418] Loss: [-0.06010663]\n",
      "Step 116: theta: [0.3998226  0.11587576] Loss: [-0.05950061]\n",
      "Step 117: theta: [0.38656848 0.1023595 ] Loss: [-0.06090849]\n",
      "Step 118: theta: [0.38195106 0.09570293] Loss: [-0.06121719]\n",
      "Step 119: theta: [0.38241996 0.09449239] Loss: [-0.06136149]\n",
      "Step 120: theta: [0.38841808 0.0988243 ] Loss: [-0.06146594]\n",
      "Step 121: theta: [0.38953577 0.09828892] Loss: [-0.06160138]\n",
      "Step 122: theta: [0.39149321 0.09860631] Loss: [-0.06173406]\n",
      "Step 123: theta: [0.38723221 0.09271818] Loss: [-0.06188735]\n",
      "Step 124: theta: [0.38241172 0.08628339] Loss: [-0.06208837]\n",
      "Step 125: theta: [0.38780134 0.09007143] Loss: [-0.06215428]\n",
      "Step 126: theta: [0.39239211 0.09307324] Loss: [-0.06225394]\n",
      "Step 127: theta: [0.3968854  0.09599009] Loss: [-0.06237417]\n",
      "Step 128: theta: [0.40712244 0.10466312] Loss: [-0.06116621]\n",
      "Step 129: theta: [0.41080333 0.10612455] Loss: [-0.06053681]\n",
      "Step 130: theta: [0.41080416 0.10362359] Loss: [-0.06132404]\n",
      "Step 131: theta: [0.4056396  0.09563901] Loss: [-0.06295461]\n",
      "Step 132: theta: [0.39694233 0.08511545] Loss: [-0.06321792]\n",
      "Step 133: theta: [0.40330687 0.09000212] Loss: [-0.06342509]\n",
      "Step 134: theta: [0.40052842 0.08508495] Loss: [-0.06351766]\n",
      "Step 135: theta: [0.40518296 0.08739853] Loss: [-0.06394965]\n",
      "Step 136: theta: [0.39198121 0.07163449] Loss: [-0.06740252]\n",
      "Step 137: theta: [0.39224044 0.06705041] Loss: [-0.06964751]\n",
      "Step 138: theta: [0.38823402 0.05863354] Loss: [-0.07423296]\n",
      "Step 139: theta: [0.39362019 0.06021046] Loss: [-0.07284827]\n",
      "Step 140: theta: [0.39819218 0.06110662] Loss: [-0.07195243]\n",
      "Step 141: theta: [0.39575757 0.0553247 ] Loss: [-0.07491899]\n",
      "Step 142: theta: [0.39716926 0.05328635] Loss: [-0.0756927]\n",
      "Step 143: theta: [0.39335196 0.04613341] Loss: [-0.07964391]\n",
      "Step 144: theta: [0.40270325 0.05225967] Loss: [-0.07554741]\n",
      "Step 145: theta: [0.3971977  0.04324827] Loss: [-0.08038566]\n",
      "Step 146: theta: [0.4018169  0.04486566] Loss: [-0.07891188]\n",
      "Step 147: theta: [0.38831761 0.02836291] Loss: [-0.08583544]\n",
      "Step 148: theta: [0.38945582 0.02836713] Loss: [-0.08573427]\n",
      "Step 149: theta: [0.38089356 0.01858283] Loss: [-0.08800175]\n",
      "Step 150: theta: [0.39042935 0.02616561] Loss: [-0.08612818]\n",
      "Step 151: theta: [0.38671631 0.02098393] Loss: [-0.08737165]\n",
      "Step 152: theta: [0.39262454 0.02530946] Loss: [-0.08617339]\n",
      "Step 153: theta: [0.39538885 0.02636813] Loss: [-0.08572392]\n",
      "Step 154: theta: [0.39449053 0.02363171] Loss: [-0.08647175]\n",
      "Step 155: theta: [0.39619644 0.02335676] Loss: [-0.08645636]\n",
      "Step 156: theta: [0.39385439 0.01888001] Loss: [-0.08763931]\n",
      "Step 157: theta: [0.38839118 0.01144186] Loss: [-0.08939749]\n",
      "Step 158: theta: [0.38360498 0.0046773 ] Loss: [-0.09098049]\n",
      "Step 159: theta: [0.38401235 0.00310287] Loss: [-0.09131655]\n",
      "Step 160: theta: [0.39335408 0.01045938] Loss: [-0.08950644]\n",
      "Step 161: theta: [0.39877158 0.01388821] Loss: [-0.08863112]\n",
      "Step 162: theta: [0.39992091 0.01304543] Loss: [-0.08879241]\n",
      "Step 163: theta: [0.41438664 0.02568314] Loss: [-0.08434313]\n",
      "Step 164: theta: [0.40825591 0.01653979] Loss: [-0.08764912]\n",
      "Step 165: theta: [0.40948826 0.01589886] Loss: [-0.08774385]\n",
      "Step 166: theta: [0.41081039 0.01532429] Loss: [-0.0878211]\n",
      "Step 167: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 168: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 169: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 170: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 171: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 172: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 173: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 174: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 175: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 176: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 177: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 178: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 179: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 180: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 181: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 182: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 183: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 184: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 185: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 186: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 187: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 188: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 189: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 190: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 191: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 192: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 193: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 194: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 195: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 196: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 197: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 198: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 199: theta: [0.40374184 0.00633533] Loss: [-0.09015827]\n",
      "==NEW OPTIMIZATION==\n",
      "Start from theta_0: [0.29 0.36], Loss: [0.06060266]\n",
      "Step 0: theta: [0.2909768  0.35689676] Loss: [0.05986655]\n",
      "Step 1: theta: [0.29278816 0.35114212] Loss: [0.05848228]\n",
      "Step 2: theta: [0.29393721 0.34749166] Loss: [0.05759124]\n",
      "Step 3: theta: [0.29285004 0.35094556] Loss: [0.05843456]\n",
      "Step 4: theta: [0.29478928 0.34478467] Loss: [0.056924]\n",
      "Step 5: theta: [0.29611131 0.34058464] Loss: [0.05587782]\n",
      "Step 6: theta: [0.29609159 0.34064729] Loss: [0.05589352]\n",
      "Step 7: theta: [0.29760765 0.33583084] Loss: [0.05398823]\n",
      "Step 8: theta: [0.29930526 0.33043758] Loss: [0.05173554]\n",
      "Step 9: theta: [0.3010025  0.32504554] Loss: [0.04910243]\n",
      "Step 10: theta: [0.30259528 0.31998533] Loss: [0.04631667]\n",
      "Step 11: theta: [0.30494584 0.31251772] Loss: [0.04213136]\n",
      "Step 12: theta: [0.30554045 0.31062865] Loss: [0.04107164]\n",
      "Step 13: theta: [0.30692355 0.30623461] Loss: [0.03860516]\n",
      "Step 14: theta: [0.30786953 0.30322927] Loss: [0.03691696]\n",
      "Step 15: theta: [0.3103014  0.29550333] Loss: [0.0316288]\n",
      "Step 16: theta: [0.31254486 0.28837595] Loss: [0.02627245]\n",
      "Step 17: theta: [0.31425878 0.28293091] Loss: [0.02230319]\n",
      "Step 18: theta: [0.31524439 0.27979967] Loss: [0.02011883]\n",
      "Step 19: theta: [0.31677504 0.27493684] Loss: [0.01794567]\n",
      "Step 20: theta: [0.31803401 0.27093717] Loss: [0.01623991]\n",
      "Step 21: theta: [0.31885041 0.2683435 ] Loss: [0.01517317]\n",
      "Step 22: theta: [0.32178886 0.25900817] Loss: [0.01103214]\n",
      "Step 23: theta: [0.32418109 0.25140816] Loss: [0.00841204]\n",
      "Step 24: theta: [0.32142582 0.26016153] Loss: [0.01146328]\n",
      "Step 25: theta: [0.32028506 0.26378567] Loss: [0.01327807]\n",
      "Step 26: theta: [0.32145898 0.26005617] Loss: [0.01140976]\n",
      "Step 27: theta: [0.32233439 0.25727504] Loss: [0.01042612]\n",
      "Step 28: theta: [0.32362513 0.25317442] Loss: [0.00901232]\n",
      "Step 29: theta: [0.32490124 0.24912027] Loss: [0.00764224]\n",
      "Step 30: theta: [0.32876866 0.23683362] Loss: [0.00238949]\n",
      "Step 31: theta: [0.32928183 0.23520332] Loss: [0.00123174]\n",
      "Step 32: theta: [0.33078992 0.2304122 ] Loss: [-0.00212466]\n",
      "Step 33: theta: [0.3355819  0.21518827] Loss: [-0.01342362]\n",
      "Step 34: theta: [0.33900169 0.20432373] Loss: [-0.02308679]\n",
      "Step 35: theta: [0.34247531 0.19328821] Loss: [-0.03057244]\n",
      "Step 36: theta: [0.34336825 0.19045138] Loss: [-0.0321683]\n",
      "Step 37: theta: [0.34272743 0.19248722] Loss: [-0.03102062]\n",
      "Step 38: theta: [0.34456333 0.18665464] Loss: [-0.03434159]\n",
      "Step 39: theta: [0.34636857 0.18091949] Loss: [-0.03770563]\n",
      "Step 40: theta: [0.34638681 0.18086154] Loss: [-0.03774013]\n",
      "Step 41: theta: [0.34715302 0.17842732] Loss: [-0.03854603]\n",
      "Step 42: theta: [0.34758598 0.17705184] Loss: [-0.03880161]\n",
      "Step 43: theta: [0.3468205  0.17948372] Loss: [-0.03834988]\n",
      "Step 44: theta: [0.34778923 0.1764061 ] Loss: [-0.03892166]\n",
      "Step 45: theta: [0.34845294 0.17429752] Loss: [-0.03931398]\n",
      "Step 46: theta: [0.34903502 0.1724483 ] Loss: [-0.03965844]\n",
      "Step 47: theta: [0.34887327 0.17296216] Loss: [-0.03956269]\n",
      "Step 48: theta: [0.35022251 0.16867569] Loss: [-0.04036231]\n",
      "Step 49: theta: [0.35198162 0.16308705] Loss: [-0.0414078]\n",
      "Step 50: theta: [0.352959   0.15998198] Loss: [-0.04198938]\n",
      "Step 51: theta: [0.35237039 0.16185197] Loss: [-0.0416393]\n",
      "Step 52: theta: [0.35391246 0.15695286] Loss: [-0.04242709]\n",
      "Step 53: theta: [0.35363177 0.1578446 ] Loss: [-0.04229949]\n",
      "Step 54: theta: [0.35525099 0.15270042] Loss: [-0.04302124]\n",
      "Step 55: theta: [0.3541708  0.15613214] Loss: [-0.04254361]\n",
      "Step 56: theta: [0.35583207 0.15085435] Loss: [-0.04327177]\n",
      "Step 57: theta: [0.3561897  0.14971817] Loss: [-0.04342374]\n",
      "Step 58: theta: [0.35485519 0.15395785] Loss: [-0.04284803]\n",
      "Step 59: theta: [0.35482086 0.15406693] Loss: [-0.0428329]\n",
      "Step 60: theta: [0.35529246 0.15256867] Loss: [-0.04303927]\n",
      "Step 61: theta: [0.35605884 0.15013391] Loss: [-0.04336833]\n",
      "Step 62: theta: [0.35509207 0.15320528] Loss: [-0.04295194]\n",
      "Step 63: theta: [0.3539649  0.15678627] Loss: [-0.04245082]\n",
      "Step 64: theta: [0.35464659 0.15462057] Loss: [-0.0427559]\n",
      "Step 65: theta: [0.35440477 0.15538881] Loss: [-0.04264838]\n",
      "Step 66: theta: [0.35513055 0.15308305] Loss: [-0.04296875]\n",
      "Step 67: theta: [0.35656749 0.14851794] Loss: [-0.04358243]\n",
      "Step 68: theta: [0.35467893 0.15451783] Loss: [-0.04277022]\n",
      "Step 69: theta: [0.35542736 0.15214008] Loss: [-0.04309776]\n",
      "Step 70: theta: [0.35749971 0.14555632] Loss: [-0.0439659]\n",
      "Step 71: theta: [0.35745625 0.14569441] Loss: [-0.04394828]\n",
      "Step 72: theta: [0.35670819 0.14807095] Loss: [-0.04364104]\n",
      "Step 73: theta: [0.35811628 0.14359753] Loss: [-0.0442132]\n",
      "Step 74: theta: [0.35882232 0.14135446] Loss: [-0.04449019]\n",
      "Step 75: theta: [0.36040245 0.13633446] Loss: [-0.04613755]\n",
      "Step 76: theta: [0.36104332 0.13429844] Loss: [-0.04693308]\n",
      "Step 77: theta: [0.36013066 0.13719792] Loss: [-0.04580185]\n",
      "Step 78: theta: [0.36109131 0.13414597] Loss: [-0.04699288]\n",
      "Step 79: theta: [0.36160239 0.13252228] Loss: [-0.04763164]\n",
      "Step 80: theta: [0.36379724 0.12554934] Loss: [-0.05041505]\n",
      "Step 81: theta: [0.36521986 0.12102974] Loss: [-0.05225407]\n",
      "Step 82: theta: [0.36415357 0.12441732] Loss: [-0.05087309]\n",
      "Step 83: theta: [0.36367273 0.1259449 ] Loss: [-0.05025541]\n",
      "Step 84: theta: [0.36604191 0.11841813] Loss: [-0.0532792]\n",
      "Step 85: theta: [0.36805118 0.11203478] Loss: [-0.05554179]\n",
      "Step 86: theta: [0.36812644 0.11179567] Loss: [-0.05562132]\n",
      "Step 87: theta: [0.36907525 0.10878133] Loss: [-0.05659149]\n",
      "Step 88: theta: [0.37058061 0.10399888] Loss: [-0.05800765]\n",
      "Step 89: theta: [0.3720269  0.09940407] Loss: [-0.05925596]\n",
      "Step 90: theta: [0.37402782 0.09304724] Loss: [-0.06091516]\n",
      "Step 91: theta: [0.37501985 0.08989559] Loss: [-0.06156029]\n",
      "Step 92: theta: [0.37542237 0.08861682] Loss: [-0.0617885]\n",
      "Step 93: theta: [0.37555619 0.08819167] Loss: [-0.06186008]\n",
      "Step 94: theta: [0.37748146 0.08207517] Loss: [-0.06265295]\n",
      "Step 95: theta: [0.37465693 0.09104859] Loss: [-0.06133792]\n",
      "Step 96: theta: [0.37503835 0.08983683] Loss: [-0.0615712]\n",
      "Step 97: theta: [0.37561447 0.08800651] Loss: [-0.06189059]\n",
      "Step 98: theta: [0.37412515 0.09273803] Loss: [-0.06098366]\n",
      "Step 99: theta: [0.37521155 0.08928659] Loss: [-0.06167139]\n",
      "Step 100: theta: [0.374829   0.09050194] Loss: [-0.06144532]\n",
      "Step 101: theta: [0.3765671  0.08498005] Loss: [-0.06233165]\n",
      "Step 102: theta: [0.37676061 0.08436528] Loss: [-0.06240798]\n",
      "Step 103: theta: [0.37802502 0.08034831] Loss: [-0.06279658]\n",
      "Step 104: theta: [0.37990828 0.07436527] Loss: [-0.06588295]\n",
      "Step 105: theta: [0.38214861 0.06724782] Loss: [-0.07002898]\n",
      "Step 106: theta: [0.3825651  0.06592468] Loss: [-0.07077225]\n",
      "Step 107: theta: [0.382083   0.06745626] Loss: [-0.06991099]\n",
      "Step 108: theta: [0.38611449 0.05464842] Loss: [-0.0764914]\n",
      "Step 109: theta: [0.38728977 0.0509146 ] Loss: [-0.07820844]\n",
      "Step 110: theta: [0.38764972 0.04977105] Loss: [-0.07872849]\n",
      "Step 111: theta: [0.38914689 0.04501459] Loss: [-0.0808623]\n",
      "Step 112: theta: [0.39015452 0.04181341] Loss: [-0.0822718]\n",
      "Step 113: theta: [0.39257655 0.03411872] Loss: [-0.08410998]\n",
      "Step 114: theta: [0.39317526 0.03221665] Loss: [-0.08448551]\n",
      "Step 115: theta: [0.39377288 0.03031803] Loss: [-0.08487799]\n",
      "Step 116: theta: [0.39339657 0.03151355] Loss: [-0.0846288]\n",
      "Step 117: theta: [0.3946302  0.02759435] Loss: [-0.0854718]\n",
      "Step 118: theta: [0.39385342 0.03006215] Loss: [-0.08493223]\n",
      "Step 119: theta: [0.39537707 0.02522159] Loss: [-0.08601867]\n",
      "Step 120: theta: [0.39814721 0.01642098] Loss: [-0.08808467]\n",
      "Step 121: theta: [0.39666591 0.02112702] Loss: [-0.08702712]\n",
      "Step 122: theta: [0.39401203 0.02955827] Loss: [-0.08503998]\n",
      "Step 123: theta: [0.3954754 0.0249092] Loss: [-0.08609272]\n",
      "Step 124: theta: [0.39965654 0.01162588] Loss: [-0.08911259]\n",
      "Step 125: theta: [0.39888638 0.01407268] Loss: [-0.08858776]\n",
      "Step 126: theta: [0.39778219 0.01758062] Loss: [-0.08783646]\n",
      "Step 127: theta: [0.39920409 0.01306332] Loss: [-0.08880418]\n",
      "Step 128: theta: [0.39866006 0.01479169] Loss: [-0.08843366]\n",
      "Step 129: theta: [0.39857861 0.01505043] Loss: [-0.08837822]\n",
      "Step 130: theta: [0.39871401 0.01462028] Loss: [-0.08847039]\n",
      "Step 131: theta: [0.40005857 0.01034866] Loss: [-0.08938591]\n",
      "Step 132: theta: [0.40010328 0.01020663] Loss: [-0.08941575]\n",
      "Step 133: theta: [0.40108681 0.007082  ] Loss: [-0.09007617]\n",
      "Step 134: theta: [0.40145314 0.00591818] Loss: [-0.09032412]\n",
      "Step 135: theta: [0.40090426 0.00766194] Loss: [-0.08995301]\n",
      "Step 136: theta: [0.40107244 0.00712763] Loss: [-0.09006647]\n",
      "Step 137: theta: [0.4003756  0.00934147] Loss: [-0.08959784]\n",
      "Step 138: theta: [0.4009883  0.00739496] Loss: [-0.09000967]\n",
      "Step 139: theta: [0.40005256 0.01036776] Loss: [-0.0893819]\n",
      "Step 140: theta: [0.39915267 0.01322668] Loss: [-0.08876915]\n",
      "Step 141: theta: [0.40176577 0.00492498] Loss: [-0.09053656]\n",
      "Step 142: theta: [0.40230641 0.00320739] Loss: [-0.09090578]\n",
      "Step 143: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 144: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 145: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 146: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 147: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 148: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 149: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 150: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 151: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 152: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 153: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 154: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 155: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 156: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 157: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 158: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 159: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 160: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 161: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 162: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 163: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 164: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 165: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 166: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 167: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 168: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 169: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 170: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 171: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 172: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 173: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 174: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 175: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 176: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 177: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 178: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 179: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 180: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 181: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 182: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 183: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 184: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 185: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 186: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 187: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 188: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 189: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 190: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 191: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 192: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 193: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 194: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 195: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 196: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 197: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 198: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 199: theta: [0.40284993 0.00148064] Loss: [-0.0912793]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.11.1.min.js\"></script>                <div id=\"f7cd8089-4167-4a70-97cc-aae26b70e885\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f7cd8089-4167-4a70-97cc-aae26b70e885\")) {                    Plotly.newPlot(                        \"f7cd8089-4167-4a70-97cc-aae26b70e885\",                        [{\"colorscale\":[[0.0,\"rgb(51, 51, 153)\"],[0.003937007874015748,\"rgb(49, 53, 155)\"],[0.007874015748031496,\"rgb(48, 56, 158)\"],[0.011811023622047244,\"rgb(47, 59, 161)\"],[0.015748031496062992,\"rgb(45, 61, 163)\"],[0.01968503937007874,\"rgb(44, 64, 166)\"],[0.023622047244094488,\"rgb(43, 67, 169)\"],[0.027559055118110236,\"rgb(41, 69, 171)\"],[0.031496062992125984,\"rgb(40, 72, 174)\"],[0.03543307086614173,\"rgb(39, 75, 177)\"],[0.03937007874015748,\"rgb(37, 77, 179)\"],[0.04330708661417323,\"rgb(36, 80, 182)\"],[0.047244094488188976,\"rgb(35, 83, 185)\"],[0.051181102362204724,\"rgb(33, 85, 187)\"],[0.05511811023622047,\"rgb(32, 88, 190)\"],[0.05905511811023622,\"rgb(31, 91, 193)\"],[0.06299212598425197,\"rgb(29, 93, 195)\"],[0.06692913385826771,\"rgb(28, 96, 198)\"],[0.07086614173228346,\"rgb(27, 98, 201)\"],[0.07480314960629922,\"rgb(25, 101, 203)\"],[0.07874015748031496,\"rgb(24, 104, 206)\"],[0.0826771653543307,\"rgb(23, 107, 209)\"],[0.08661417322834646,\"rgb(21, 109, 211)\"],[0.09055118110236221,\"rgb(20, 112, 214)\"],[0.09448818897637795,\"rgb(19, 115, 217)\"],[0.09842519685039369,\"rgb(17, 117, 219)\"],[0.10236220472440945,\"rgb(16, 120, 222)\"],[0.1062992125984252,\"rgb(14, 123, 225)\"],[0.11023622047244094,\"rgb(13, 125, 227)\"],[0.11417322834645668,\"rgb(12, 128, 230)\"],[0.11811023622047244,\"rgb(11, 131, 233)\"],[0.1220472440944882,\"rgb(9, 133, 235)\"],[0.12598425196850394,\"rgb(8, 136, 238)\"],[0.12992125984251968,\"rgb(7, 138, 241)\"],[0.13385826771653542,\"rgb(5, 141, 243)\"],[0.1377952755905512,\"rgb(4, 144, 246)\"],[0.14173228346456693,\"rgb(3, 147, 249)\"],[0.14566929133858267,\"rgb(1, 149, 251)\"],[0.14960629921259844,\"rgb(0, 152, 254)\"],[0.15354330708661418,\"rgb(0, 154, 250)\"],[0.15748031496062992,\"rgb(0, 156, 244)\"],[0.16141732283464566,\"rgb(0, 158, 238)\"],[0.1653543307086614,\"rgb(0, 160, 232)\"],[0.16929133858267717,\"rgb(0, 162, 226)\"],[0.1732283464566929,\"rgb(0, 164, 220)\"],[0.17716535433070865,\"rgb(0, 166, 214)\"],[0.18110236220472442,\"rgb(0, 168, 208)\"],[0.18503937007874016,\"rgb(0, 170, 202)\"],[0.1889763779527559,\"rgb(0, 172, 196)\"],[0.19291338582677164,\"rgb(0, 174, 190)\"],[0.19685039370078738,\"rgb(0, 176, 184)\"],[0.20078740157480315,\"rgb(0, 178, 178)\"],[0.2047244094488189,\"rgb(0, 180, 172)\"],[0.20866141732283464,\"rgb(0, 182, 166)\"],[0.2125984251968504,\"rgb(0, 184, 160)\"],[0.21653543307086615,\"rgb(0, 186, 154)\"],[0.2204724409448819,\"rgb(0, 188, 148)\"],[0.22440944881889763,\"rgb(0, 190, 142)\"],[0.22834645669291337,\"rgb(0, 192, 136)\"],[0.23228346456692914,\"rgb(0, 194, 130)\"],[0.23622047244094488,\"rgb(0, 196, 124)\"],[0.24015748031496062,\"rgb(0, 198, 118)\"],[0.2440944881889764,\"rgb(0, 200, 112)\"],[0.24803149606299213,\"rgb(0, 202, 106)\"],[0.25196850393700787,\"rgb(1, 204, 102)\"],[0.2559055118110236,\"rgb(5, 205, 103)\"],[0.25984251968503935,\"rgb(8, 205, 103)\"],[0.2637795275590551,\"rgb(13, 206, 104)\"],[0.26771653543307083,\"rgb(17, 207, 105)\"],[0.27165354330708663,\"rgb(21, 208, 106)\"],[0.2755905511811024,\"rgb(25, 209, 107)\"],[0.2795275590551181,\"rgb(29, 209, 107)\"],[0.28346456692913385,\"rgb(33, 210, 108)\"],[0.2874015748031496,\"rgb(37, 211, 109)\"],[0.29133858267716534,\"rgb(40, 212, 110)\"],[0.2952755905511811,\"rgb(45, 213, 111)\"],[0.2992125984251969,\"rgb(49, 213, 111)\"],[0.3031496062992126,\"rgb(53, 214, 112)\"],[0.30708661417322836,\"rgb(57, 215, 113)\"],[0.3110236220472441,\"rgb(61, 216, 114)\"],[0.31496062992125984,\"rgb(65, 217, 115)\"],[0.3188976377952756,\"rgb(69, 217, 115)\"],[0.3228346456692913,\"rgb(72, 218, 116)\"],[0.32677165354330706,\"rgb(77, 219, 117)\"],[0.3307086614173228,\"rgb(81, 220, 118)\"],[0.3346456692913386,\"rgb(85, 221, 119)\"],[0.33858267716535434,\"rgb(89, 221, 119)\"],[0.3425196850393701,\"rgb(93, 222, 120)\"],[0.3464566929133858,\"rgb(97, 223, 121)\"],[0.35039370078740156,\"rgb(101, 224, 122)\"],[0.3543307086614173,\"rgb(104, 225, 122)\"],[0.35826771653543305,\"rgb(109, 225, 123)\"],[0.36220472440944884,\"rgb(113, 226, 124)\"],[0.3661417322834646,\"rgb(117, 227, 125)\"],[0.3700787401574803,\"rgb(121, 228, 126)\"],[0.37401574803149606,\"rgb(125, 229, 127)\"],[0.3779527559055118,\"rgb(129, 229, 127)\"],[0.38188976377952755,\"rgb(133, 230, 128)\"],[0.3858267716535433,\"rgb(136, 231, 129)\"],[0.38976377952755903,\"rgb(141, 232, 130)\"],[0.39370078740157477,\"rgb(145, 233, 131)\"],[0.39763779527559057,\"rgb(149, 233, 131)\"],[0.4015748031496063,\"rgb(153, 234, 132)\"],[0.40551181102362205,\"rgb(157, 235, 133)\"],[0.4094488188976378,\"rgb(161, 236, 134)\"],[0.41338582677165353,\"rgb(165, 237, 135)\"],[0.41732283464566927,\"rgb(168, 237, 135)\"],[0.421259842519685,\"rgb(173, 238, 136)\"],[0.4251968503937008,\"rgb(177, 239, 137)\"],[0.42913385826771655,\"rgb(181, 240, 138)\"],[0.4330708661417323,\"rgb(185, 241, 139)\"],[0.43700787401574803,\"rgb(189, 241, 139)\"],[0.4409448818897638,\"rgb(193, 242, 140)\"],[0.4448818897637795,\"rgb(197, 243, 141)\"],[0.44881889763779526,\"rgb(200, 244, 142)\"],[0.452755905511811,\"rgb(205, 245, 143)\"],[0.45669291338582674,\"rgb(209, 245, 143)\"],[0.46062992125984253,\"rgb(213, 246, 144)\"],[0.4645669291338583,\"rgb(217, 247, 145)\"],[0.468503937007874,\"rgb(221, 248, 146)\"],[0.47244094488188976,\"rgb(225, 249, 147)\"],[0.4763779527559055,\"rgb(229, 249, 147)\"],[0.48031496062992124,\"rgb(232, 250, 148)\"],[0.484251968503937,\"rgb(237, 251, 149)\"],[0.4881889763779528,\"rgb(241, 252, 150)\"],[0.4921259842519685,\"rgb(245, 253, 151)\"],[0.49606299212598426,\"rgb(249, 253, 151)\"],[0.5,\"rgb(254, 253, 152)\"],[0.5039370078740157,\"rgb(252, 251, 151)\"],[0.5078740157480315,\"rgb(250, 248, 150)\"],[0.5118110236220472,\"rgb(248, 246, 149)\"],[0.515748031496063,\"rgb(246, 243, 148)\"],[0.5196850393700787,\"rgb(244, 240, 147)\"],[0.5236220472440944,\"rgb(242, 238, 145)\"],[0.5275590551181102,\"rgb(240, 235, 144)\"],[0.5314960629921259,\"rgb(238, 233, 143)\"],[0.5354330708661417,\"rgb(236, 230, 142)\"],[0.5393700787401575,\"rgb(234, 228, 141)\"],[0.5433070866141733,\"rgb(232, 225, 140)\"],[0.547244094488189,\"rgb(230, 223, 139)\"],[0.5511811023622047,\"rgb(228, 220, 138)\"],[0.5551181102362205,\"rgb(226, 217, 137)\"],[0.5590551181102362,\"rgb(224, 215, 136)\"],[0.562992125984252,\"rgb(222, 212, 135)\"],[0.5669291338582677,\"rgb(220, 210, 134)\"],[0.5708661417322834,\"rgb(218, 207, 133)\"],[0.5748031496062992,\"rgb(216, 205, 131)\"],[0.5787401574803149,\"rgb(214, 202, 130)\"],[0.5826771653543307,\"rgb(211, 199, 129)\"],[0.5866141732283464,\"rgb(210, 197, 128)\"],[0.5905511811023622,\"rgb(208, 194, 127)\"],[0.5944881889763779,\"rgb(206, 192, 126)\"],[0.5984251968503937,\"rgb(204, 189, 125)\"],[0.6023622047244095,\"rgb(202, 187, 124)\"],[0.6062992125984252,\"rgb(200, 184, 123)\"],[0.610236220472441,\"rgb(198, 182, 122)\"],[0.6141732283464567,\"rgb(195, 179, 121)\"],[0.6181102362204725,\"rgb(194, 176, 120)\"],[0.6220472440944882,\"rgb(192, 174, 118)\"],[0.6259842519685039,\"rgb(190, 171, 117)\"],[0.6299212598425197,\"rgb(188, 169, 116)\"],[0.6338582677165354,\"rgb(186, 166, 115)\"],[0.6377952755905512,\"rgb(184, 164, 114)\"],[0.6417322834645669,\"rgb(182, 161, 113)\"],[0.6456692913385826,\"rgb(179, 159, 112)\"],[0.6496062992125984,\"rgb(178, 156, 111)\"],[0.6535433070866141,\"rgb(176, 153, 110)\"],[0.6574803149606299,\"rgb(174, 151, 109)\"],[0.6614173228346456,\"rgb(172, 148, 108)\"],[0.6653543307086615,\"rgb(170, 146, 107)\"],[0.6692913385826772,\"rgb(168, 143, 106)\"],[0.6732283464566929,\"rgb(166, 141, 104)\"],[0.6771653543307087,\"rgb(163, 138, 103)\"],[0.6811023622047244,\"rgb(162, 135, 102)\"],[0.6850393700787402,\"rgb(160, 133, 101)\"],[0.6889763779527559,\"rgb(158, 130, 100)\"],[0.6929133858267716,\"rgb(156, 128, 99)\"],[0.6968503937007874,\"rgb(154, 125, 98)\"],[0.7007874015748031,\"rgb(152, 123, 97)\"],[0.7047244094488189,\"rgb(150, 120, 96)\"],[0.7086614173228346,\"rgb(147, 118, 95)\"],[0.7125984251968503,\"rgb(146, 115, 94)\"],[0.7165354330708661,\"rgb(144, 112, 93)\"],[0.7204724409448818,\"rgb(142, 110, 91)\"],[0.7244094488188977,\"rgb(140, 107, 90)\"],[0.7283464566929134,\"rgb(138, 105, 89)\"],[0.7322834645669292,\"rgb(136, 102, 88)\"],[0.7362204724409449,\"rgb(134, 100, 87)\"],[0.7401574803149606,\"rgb(131, 97, 86)\"],[0.7440944881889764,\"rgb(130, 95, 85)\"],[0.7480314960629921,\"rgb(128, 92, 84)\"],[0.7519685039370079,\"rgb(129, 93, 86)\"],[0.7559055118110236,\"rgb(131, 96, 88)\"],[0.7598425196850394,\"rgb(133, 98, 91)\"],[0.7637795275590551,\"rgb(135, 101, 94)\"],[0.7677165354330708,\"rgb(136, 103, 96)\"],[0.7716535433070866,\"rgb(139, 106, 99)\"],[0.7755905511811023,\"rgb(141, 109, 102)\"],[0.7795275590551181,\"rgb(143, 111, 104)\"],[0.7834645669291338,\"rgb(145, 114, 107)\"],[0.7874015748031495,\"rgb(147, 116, 110)\"],[0.7913385826771654,\"rgb(149, 119, 112)\"],[0.7952755905511811,\"rgb(151, 121, 115)\"],[0.7992125984251969,\"rgb(153, 124, 118)\"],[0.8031496062992126,\"rgb(155, 127, 121)\"],[0.8070866141732284,\"rgb(157, 129, 123)\"],[0.8110236220472441,\"rgb(159, 132, 126)\"],[0.8149606299212598,\"rgb(161, 134, 129)\"],[0.8188976377952756,\"rgb(163, 137, 131)\"],[0.8228346456692913,\"rgb(165, 139, 134)\"],[0.8267716535433071,\"rgb(167, 142, 137)\"],[0.8307086614173228,\"rgb(168, 144, 139)\"],[0.8346456692913385,\"rgb(171, 147, 142)\"],[0.8385826771653543,\"rgb(173, 150, 145)\"],[0.84251968503937,\"rgb(175, 152, 147)\"],[0.8464566929133858,\"rgb(177, 155, 150)\"],[0.8503937007874016,\"rgb(179, 157, 153)\"],[0.8543307086614174,\"rgb(181, 160, 155)\"],[0.8582677165354331,\"rgb(183, 162, 158)\"],[0.8622047244094488,\"rgb(185, 165, 161)\"],[0.8661417322834646,\"rgb(187, 167, 163)\"],[0.8700787401574803,\"rgb(189, 170, 166)\"],[0.8740157480314961,\"rgb(191, 173, 169)\"],[0.8779527559055118,\"rgb(193, 175, 171)\"],[0.8818897637795275,\"rgb(195, 178, 174)\"],[0.8858267716535433,\"rgb(196, 180, 177)\"],[0.889763779527559,\"rgb(199, 183, 179)\"],[0.8937007874015748,\"rgb(200, 185, 182)\"],[0.8976377952755905,\"rgb(203, 188, 185)\"],[0.9015748031496063,\"rgb(205, 191, 187)\"],[0.905511811023622,\"rgb(207, 193, 190)\"],[0.9094488188976377,\"rgb(209, 196, 193)\"],[0.9133858267716535,\"rgb(211, 198, 196)\"],[0.9173228346456693,\"rgb(212, 201, 198)\"],[0.9212598425196851,\"rgb(215, 203, 201)\"],[0.9251968503937008,\"rgb(217, 206, 204)\"],[0.9291338582677166,\"rgb(219, 208, 206)\"],[0.9330708661417323,\"rgb(221, 211, 209)\"],[0.937007874015748,\"rgb(223, 214, 212)\"],[0.9409448818897638,\"rgb(225, 216, 214)\"],[0.9448818897637795,\"rgb(227, 219, 217)\"],[0.9488188976377953,\"rgb(228, 221, 220)\"],[0.952755905511811,\"rgb(231, 224, 222)\"],[0.9566929133858267,\"rgb(232, 226, 225)\"],[0.9606299212598425,\"rgb(235, 229, 228)\"],[0.9645669291338582,\"rgb(237, 231, 230)\"],[0.968503937007874,\"rgb(239, 234, 233)\"],[0.9724409448818897,\"rgb(241, 237, 236)\"],[0.9763779527559056,\"rgb(243, 239, 238)\"],[0.9803149606299213,\"rgb(244, 242, 241)\"],[0.984251968503937,\"rgb(247, 244, 244)\"],[0.9881889763779528,\"rgb(249, 247, 246)\"],[0.9921259842519685,\"rgb(251, 249, 249)\"],[0.9960629921259843,\"rgb(253, 252, 252)\"],[1.0,\"rgb(255, 255, 255)\"]],\"name\":\"Loss Surface\",\"opacity\":0.7,\"x\":[[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58],[0.0,0.02,0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58]],\"y\":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.02],[0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04,0.04],[0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06,0.06],[0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08,0.08],[0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1],[0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12,0.12],[0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14,0.14],[0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16,0.16],[0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18,0.18],[0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2],[0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22,0.22],[0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24,0.24],[0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26,0.26],[0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28,0.28],[0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3],[0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32,0.32],[0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34,0.34],[0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36,0.36],[0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38,0.38],[0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4,0.4],[0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42,0.42],[0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44,0.44],[0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46,0.46],[0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48,0.48],[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5],[0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52,0.52],[0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54,0.54],[0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56],[0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58,0.58]],\"z\":[[0.0,-0.026055242866277695,-0.04022997245192528,-0.05251088738441467,-0.06405401229858398,-0.06982800364494324,-0.07487376034259796,-0.08252570778131485,-0.088084876537323,-0.09046395868062973,-0.09077435731887817,-0.09331303089857101,-0.09325568377971649,-0.09178968518972397,-0.09297418594360352,-0.0981902927160263,-0.0994970053434372,-0.09852036833763123,-0.09594127535820007,-0.09207430481910706,-0.09168050438165665,-0.09120375663042068,-0.08562473952770233,-0.07669571787118912,-0.0729847252368927,-0.0744047611951828,-0.07587684690952301,-0.07319462299346924,-0.06671939045190811,-0.06309422850608826],[0.026055242866277695,0.00041995063656941056,-0.015869908034801483,-0.03058764711022377,-0.044817209243774414,-0.05501371622085571,-0.06194769963622093,-0.06960310786962509,-0.07578612864017487,-0.08029993623495102,-0.08231694251298904,-0.08199037611484528,-0.07955029606819153,-0.07910966128110886,-0.08121377229690552,-0.0874277874827385,-0.0904337540268898,-0.09228135645389557,-0.09131953120231628,-0.08771320432424545,-0.08725009113550186,-0.08627336472272873,-0.07887572795152664,-0.06932738423347473,-0.06627751141786575,-0.06661121547222137,-0.06611280143260956,-0.062826968729496,-0.05787954851984978,-0.055820535868406296],[0.04022997245192528,0.018534695729613304,0.004046501126140356,-0.011285792104899883,-0.027011284604668617,-0.039005525410175323,-0.0446578748524189,-0.05037790536880493,-0.05709841474890709,-0.06484664231538773,-0.0687052384018898,-0.06908777356147766,-0.06902336329221725,-0.07093986123800278,-0.07260532677173615,-0.07821350544691086,-0.08407188206911087,-0.09079599380493164,-0.09046141058206558,-0.08497509360313416,-0.08140534907579422,-0.0777694508433342,-0.06875122338533401,-0.061256006360054016,-0.060923002660274506,-0.05778292194008827,-0.051968950778245926,-0.048564281314611435,-0.048358287662267685,-0.05093444883823395],[0.05251088738441467,0.033445097506046295,0.019712576642632484,0.004609395284205675,-0.010965337976813316,-0.022734303027391434,-0.02736714854836464,-0.0328536257147789,-0.04281802475452423,-0.05441025644540787,-0.05957259237766266,-0.06161000579595566,-0.06340795755386353,-0.06553258746862411,-0.06422293931245804,-0.06693173199892044,-0.07402196526527405,-0.08205940574407578,-0.08054814487695694,-0.07447654753923416,-0.07223325222730637,-0.0716853067278862,-0.06341878324747086,-0.05655977129936218,-0.055942218750715256,-0.05132988467812538,-0.04477093368768692,-0.04408793896436691,-0.04684760048985481,-0.05011000484228134],[0.06562713533639908,0.04728320240974426,0.03261580318212509,0.019002307206392288,0.0048823668621480465,-0.008865753188729286,-0.01546410284936428,-0.02135447785258293,-0.03314187005162239,-0.04690436273813248,-0.05276624113321304,-0.053420402109622955,-0.05293617025017738,-0.05387589707970619,-0.05140295252203941,-0.0524975024163723,-0.05915060639381409,-0.06647083163261414,-0.06609367579221725,-0.06248466670513153,-0.06381628662347794,-0.06786458194255829,-0.06278788298368454,-0.05638367682695389,-0.052458181977272034,-0.04773930087685585,-0.04352341964840889,-0.045538391917943954,-0.04887625575065613,-0.050150834023952484],[0.07416171580553055,0.05552302300930023,0.040521349757909775,0.032178718596696854,0.022596590220928192,0.005019799340516329,-0.006408232729882002,-0.011373178102076054,-0.01978868618607521,-0.033600304275751114,-0.041519686579704285,-0.040489863604307175,-0.03609658405184746,-0.0365971140563488,-0.0391601026058197,-0.041942764073610306,-0.0452658049762249,-0.05008384585380554,-0.056757133454084396,-0.06067414581775665,-0.06232098489999771,-0.06258704513311386,-0.06193358823657036,-0.05869641155004501,-0.05029137060046196,-0.04542621597647667,-0.042962729930877686,-0.043133657425642014,-0.045718543231487274,-0.047380752861499786],[0.07613785564899445,0.05791836977005005,0.04386133700609207,0.03867889195680618,0.03258773684501648,0.017816506326198578,0.006767454091459513,0.002447723411023617,-0.004940563812851906,-0.019502811133861542,-0.02901371568441391,-0.029524780809879303,-0.025217384099960327,-0.025870244950056076,-0.031003644689917564,-0.034699197858572006,-0.0370200052857399,-0.04041314125061035,-0.050428345799446106,-0.05854007601737976,-0.058773789554834366,-0.05395173653960228,-0.05472376197576523,-0.05562547594308853,-0.05055646598339081,-0.04725547507405281,-0.04382461681962013,-0.03814995661377907,-0.037592001259326935,-0.03994423523545265],[0.08059637248516083,0.06340236961841583,0.04880091920495033,0.04281146451830864,0.03766319155693054,0.02805187553167343,0.020474158227443695,0.016338756307959557,0.007052809465676546,-0.0087891248986125,-0.018739284947514534,-0.022714819759130478,-0.021315479651093483,-0.021652625873684883,-0.024467013776302338,-0.027153203263878822,-0.03144196793437004,-0.03600125014781952,-0.044992007315158844,-0.05224952846765518,-0.052490122616291046,-0.04797634482383728,-0.04768393933773041,-0.0493277981877327,-0.048628997057676315,-0.04670099541544914,-0.04227077215909958,-0.03376597538590431,-0.030020520091056824,-0.030525827780365944],[0.09204737097024918,0.07709035277366638,0.059581458568573,0.05065733194351196,0.045194368809461594,0.03636198863387108,0.030412273481488228,0.027599113062024117,0.018830791115760803,0.004302168730646372,-0.00591065501794219,-0.014929833821952343,-0.016835026443004608,-0.014702841639518738,-0.013530217111110687,-0.016928918659687042,-0.024558186531066895,-0.03562293201684952,-0.045448679476976395,-0.04826263338327408,-0.048636216670274734,-0.0472407303750515,-0.04253038391470909,-0.03783825412392616,-0.03625908121466637,-0.03511865437030792,-0.03306322917342186,-0.02878376469016075,-0.022300487384200096,-0.01758396439254284],[0.09904944151639938,0.08659420162439346,0.06904041767120361,0.05872713401913643,0.05254266783595085,0.04204057902097702,0.03693469613790512,0.03653140738606453,0.028256600722670555,0.013561315834522247,0.0017778686014935374,-0.009060164913535118,-0.010872251354157925,-0.006909339223057032,-0.005538587458431721,-0.011742213740944862,-0.02065381035208702,-0.03500567376613617,-0.04476354271173477,-0.04486196115612984,-0.04584726691246033,-0.046633560210466385,-0.03794076293706894,-0.02705148234963417,-0.022920828312635422,-0.021753735840320587,-0.021503811702132225,-0.021158842369914055,-0.014608645811676979,-0.007890701293945312],[0.09571556001901627,0.08465413749217987,0.06908419728279114,0.05931653454899788,0.05409639701247215,0.0464484840631485,0.044539276510477066,0.046524763107299805,0.03774286434054375,0.021183570846915245,0.0072633964009583,-0.004099735990166664,-0.005701255984604359,-0.0021609500981867313,-0.0017330573173239827,-0.007640343625098467,-0.014803354628384113,-0.02674918808043003,-0.03461936116218567,-0.03539451211690903,-0.03729429841041565,-0.0385688878595829,-0.02925124019384384,-0.01775069162249565,-0.014244960620999336,-0.013384822756052017,-0.012938950210809708,-0.012712196446955204,-0.0084692919626832,-0.0047643352299928665],[0.09230887144804001,0.08181660622358322,0.06646276265382767,0.05686967819929123,0.05392224341630936,0.051559120416641235,0.05167168751358986,0.053653497248888016,0.04659680277109146,0.032534483820199966,0.019196463748812675,0.009030456654727459,0.004302559420466423,0.003290393855422735,0.00400075688958168,0.003886755555868149,-0.001234514405950904,-0.012534177862107754,-0.02055094949901104,-0.0222037211060524,-0.02333236299455166,-0.02639753557741642,-0.021920498460531235,-0.012403417378664017,-0.005860988982021809,-0.004892658907920122,-0.0062689101323485374,-0.00748506560921669,-0.00683223083615303,-0.007697308901697397],[0.09290139377117157,0.08221908658742905,0.06517716497182846,0.054996322840452194,0.05309617891907692,0.051908597350120544,0.051050059497356415,0.05217881128191948,0.048513080924749374,0.03939448669552803,0.028874075040221214,0.02217957004904747,0.015898684039711952,0.010879430919885635,0.011075487360358238,0.014621039852499962,0.009789778850972652,-0.003408940974622965,-0.01380973681807518,-0.01619892008602619,-0.014937303029000759,-0.01748201996088028,-0.01778523251414299,-0.010334284044802189,0.0002927382884081453,0.0018393161008134484,-0.0008781708311289549,-0.001857266528531909,-0.0025493763387203217,-0.005726831499487162],[0.09146595001220703,0.0811559185385704,0.06472375243902206,0.05466541647911072,0.05193905904889107,0.04821821302175522,0.0456680990755558,0.04653428867459297,0.044407956302165985,0.038264013826847076,0.030982285737991333,0.0285332053899765,0.0248965285718441,0.019992133602499962,0.01824997179210186,0.018969185650348663,0.012434323318302631,-0.0018291014712303877,-0.01275604497641325,-0.01552564837038517,-0.01347763929516077,-0.014708307571709156,-0.016076017171144485,-0.009905995801091194,0.0006329523748718202,0.0022901413030922413,0.0003584538062568754,0.001986609073355794,0.0031190013978630304,0.0009259398793801665],[0.08939267694950104,0.07881534844636917,0.06785295158624649,0.05727364495396614,0.05025198683142662,0.04662412777543068,0.04429169371724129,0.04337340220808983,0.043924834579229355,0.042624615132808685,0.0375741571187973,0.03346157446503639,0.02947973646223545,0.027182038873434067,0.02828759141266346,0.027393288910388947,0.017930878326296806,0.006128213834017515,-0.0018348883604630828,-0.007823719643056393,-0.011749236844480038,-0.015461289323866367,-0.016781816259026527,-0.010736248455941677,0.0008162409067153931,0.004756948910653591,0.0033832434564828873,0.0019442117772996426,0.004004475194960833,0.007174248807132244],[0.0962560698390007,0.08500505983829498,0.07763173431158066,0.06476671993732452,0.052699875086545944,0.05111408233642578,0.05028655752539635,0.046872250735759735,0.04732906445860863,0.04829484596848488,0.04428459331393242,0.03910748288035393,0.03618287667632103,0.03716730698943138,0.04205707088112831,0.04031873866915703,0.028576979413628578,0.018860772252082825,0.012520479038357735,0.002764605451375246,-0.006574684754014015,-0.011180618777871132,-0.012357883155345917,-0.009037443436682224,-0.001356674823909998,0.003932665567845106,0.0046615940518677235,0.002753452630713582,0.0071013993583619595,0.014608415775001049],[0.10929950326681137,0.09869761019945145,0.08981431275606155,0.07478645443916321,0.06033818796277046,0.057275768369436264,0.055899728089571,0.05157821625471115,0.04915965721011162,0.04798632860183716,0.04408830404281616,0.04303692281246185,0.044713858515024185,0.047237176448106766,0.05134645476937294,0.047836776822805405,0.03616497665643692,0.02579977922141552,0.017836369574069977,0.007405424956232309,-0.0003280762757640332,-0.0018121326575055718,-0.003930851351469755,-0.004990905988961458,-0.003400831250473857,0.0012662606313824654,0.005580609664320946,0.010561231523752213,0.018455905839800835,0.02464316412806511],[0.11750493943691254,0.10812710970640182,0.09899599105119705,0.08335481584072113,0.06669150292873383,0.06022525206208229,0.060351114720106125,0.0589129664003849,0.05503341555595398,0.05137592926621437,0.04776005446910858,0.04606693983078003,0.04845569282770157,0.05389194190502167,0.0586298331618309,0.055072132498025894,0.04540769010782242,0.035555168986320496,0.025448698550462723,0.0140878576785326,0.004954494535923004,0.002764658536761999,0.0012488483916968107,0.00043712626211345196,0.000570911739487201,0.005272942595183849,0.012767360545694828,0.02138751931488514,0.02695881389081478,0.02620958350598812],[0.11423718184232712,0.10657580941915512,0.10030091553926468,0.0862041711807251,0.06809817254543304,0.05848219245672226,0.06141602620482445,0.06443792581558228,0.06027747318148613,0.05515177547931671,0.052792880684137344,0.0470486581325531,0.04693271219730377,0.055912867188453674,0.06214211508631706,0.05906321480870247,0.05153397098183632,0.043626148253679276,0.03286908566951752,0.020192285999655724,0.006421316415071487,-0.0004342917527537793,-0.0007962698000483215,0.002041056053712964,0.005046964623034,0.010530304163694382,0.019537631422281265,0.027005566284060478,0.026438452303409576,0.018948616459965706],[0.10703443735837936,0.10018885135650635,0.0955020859837532,0.08280517905950546,0.0653107538819313,0.05558350682258606,0.059054184705019,0.06261268258094788,0.05677381157875061,0.0507231131196022,0.04995068535208702,0.04664698243141174,0.049098506569862366,0.06002872809767723,0.06561016291379929,0.06066914647817612,0.052578628063201904,0.044093187898397446,0.03237375244498253,0.01845412515103817,0.003880245378240943,-0.0025117502082139254,-0.0022417439613491297,0.000649818335659802,0.002499207155779004,0.00753386365249753,0.017403189092874527,0.026253320276737213,0.02489495649933815,0.0166443083435297],[0.09551643580198288,0.08963074535131454,0.08835869282484055,0.07932732254266739,0.06418296694755554,0.05474691092967987,0.057432934641838074,0.0614115335047245,0.055452682077884674,0.047346021980047226,0.04597831889986992,0.04791613295674324,0.05606602877378464,0.06707712262868881,0.07163340598344803,0.06636039912700653,0.05771414563059807,0.0454300157725811,0.03148316964507103,0.019418606534600258,0.0068962182849645615,-0.001198957790620625,-0.003226857865229249,-0.000744409509934485,0.0031330420169979334,0.008879357017576694,0.01878228783607483,0.02881169319152832,0.028023336082696915,0.02155260555446148],[0.08632608503103256,0.08001133799552917,0.07892902940511703,0.07285104691982269,0.06062855198979378,0.05105288699269295,0.05229679122567177,0.05813004449009895,0.05831094831228256,0.05102177709341049,0.0473640002310276,0.05116475373506546,0.06213896721601486,0.07131654769182205,0.07639895379543304,0.07457049190998077,0.06682367622852325,0.051413096487522125,0.037484850734472275,0.030361443758010864,0.021648524329066277,0.009518858045339584,0.0030302973464131355,0.003820844693109393,0.009684992022812366,0.015481129288673401,0.02311268262565136,0.03080877475440502,0.030205681920051575,0.0273492019623518],[0.08489466458559036,0.07684739679098129,0.07044269144535065,0.062344953417778015,0.05037320777773857,0.04047675430774689,0.041391387581825256,0.04861416667699814,0.054589252918958664,0.05162586644291878,0.04890179634094238,0.05129534751176834,0.06041654199361801,0.0684373676776886,0.07381328195333481,0.07397085428237915,0.06823951750993729,0.05582275241613388,0.04579710215330124,0.0409713089466095,0.03518189862370491,0.02524326741695404,0.018571509048342705,0.01580667309463024,0.01530527975410223,0.01711242087185383,0.02186591923236847,0.027651382610201836,0.028155677020549774,0.02894843928515911],[0.08530562371015549,0.0775923877954483,0.06452414393424988,0.0518118254840374,0.03994768485426903,0.032897111028432846,0.03459673747420311,0.040615227073431015,0.04679505527019501,0.04786542430520058,0.04874878749251366,0.05082554742693901,0.05568471550941467,0.06143951043486595,0.06727400422096252,0.07062772661447525,0.06775781512260437,0.05859878286719322,0.050072357058525085,0.04508759081363678,0.041780985891819,0.03659098222851753,0.030745074152946472,0.025167196989059448,0.021380295976996422,0.021632038056850433,0.02347494661808014,0.02437628246843815,0.025035260245203972,0.02941272221505642],[0.08050071448087692,0.07807464152574539,0.06455692648887634,0.048994023352861404,0.03761589899659157,0.033298902213573456,0.03421667218208313,0.03619919344782829,0.03715365380048752,0.041316475719213486,0.04780099540948868,0.052964482456445694,0.05329429730772972,0.05533495917916298,0.06082746759057045,0.06782622635364532,0.0675964504480362,0.06078687682747841,0.05160228908061981,0.04494002088904381,0.042571935802698135,0.04037630185484886,0.03440583124756813,0.027825992554426193,0.027893193066120148,0.031617533415555954,0.03254484385251999,0.028563907369971275,0.02900056540966034,0.03579104319214821],[0.0744047611951828,0.07532712817192078,0.06589563935995102,0.05138561129570007,0.039092980325222015,0.03078330308198929,0.028589127585291862,0.029406076297163963,0.0297465231269598,0.03510859236121178,0.04237278550863266,0.04828328639268875,0.0472627654671669,0.04790012910962105,0.051316238939762115,0.05749291554093361,0.05834248661994934,0.05588528513908386,0.0514378547668457,0.045961644500494,0.04471466317772865,0.04439741373062134,0.039985157549381256,0.033011361956596375,0.03172903507947922,0.0358281210064888,0.038361016660928726,0.03736506402492523,0.0387614369392395,0.044749900698661804],[0.0690612867474556,0.06884092837572098,0.06152625381946564,0.04915580898523331,0.03696652501821518,0.026145564392209053,0.0213151928037405,0.021199174225330353,0.02284594252705574,0.029109148308634758,0.035111457109451294,0.04033626616001129,0.03995650261640549,0.04110413044691086,0.04237395152449608,0.046565864235162735,0.04908403754234314,0.05065291374921799,0.04928349703550339,0.04459834843873978,0.04456271603703499,0.048558205366134644,0.04861463978886604,0.043040644377470016,0.03678886592388153,0.037016693502664566,0.040484435856342316,0.0457790307700634,0.048804737627506256,0.052320681512355804],[0.06286187469959259,0.056512508541345596,0.049651455134153366,0.04140271618962288,0.03313480690121651,0.02768516167998314,0.02179461531341076,0.017083575949072838,0.016386089846491814,0.023041727021336555,0.028654644265770912,0.03302232176065445,0.03403420001268387,0.03692949563264847,0.03749625384807587,0.041788484901189804,0.04862910881638527,0.052964936941862106,0.04805144667625427,0.040890883654356,0.03825868293642998,0.04542570188641548,0.05161665380001068,0.0530368909239769,0.04699130356311798,0.0413917601108551,0.04229145869612694,0.0511152483522892,0.055092547088861465,0.056004736572504044],[0.058912649750709534,0.049154289066791534,0.04467020183801651,0.04111795872449875,0.03666577488183975,0.03371873497962952,0.02550065517425537,0.016170622780919075,0.01210738718509674,0.018523870036005974,0.025043245404958725,0.028018547222018242,0.027598680928349495,0.030154293403029442,0.03047827258706093,0.03591140732169151,0.04609598591923714,0.05545354261994362,0.05319232866168022,0.04610591381788254,0.03940412402153015,0.041690099984407425,0.04760724678635597,0.05307585746049881,0.051794104278087616,0.04614898934960365,0.04571910947561264,0.053433772176504135,0.05742063373327255,0.05832286924123764],[0.060203488916158676,0.05141661316156387,0.050881341099739075,0.05025692284107208,0.04693559184670448,0.041262246668338776,0.029105709865689278,0.01617451198399067,0.010056216269731522,0.01687413826584816,0.025274259969592094,0.028673525899648666,0.026938553899526596,0.0279538594186306,0.027304403483867645,0.0315973162651062,0.04162457212805748,0.055715207010507584,0.06143862009048462,0.0572572760283947,0.047918081283569336,0.04299182444810867,0.04572056606411934,0.05087845399975777,0.052282728254795074,0.05020054802298546,0.05153011158108711,0.058376461267471313,0.06365776807069778,0.06641313433647156]],\"type\":\"surface\"},{\"marker\":{\"color\":\"#de425b\",\"size\":8,\"symbol\":\"diamond\"},\"mode\":\"markers\",\"name\":\"Optimum\",\"x\":[0.4029626682505181],\"y\":[0.0011224708483585644],\"z\":[-0.0913570749160749],\"type\":\"scatter3d\"},{\"line\":{\"color\":\"#bc5090\",\"width\":10},\"mode\":\"lines\",\"name\":\"Gradient Descent\",\"x\":[0.29,0.29153945013880717,0.29309408417350613,0.2946713926179468,0.29627159822185206,0.2978949269837603,0.2995416081829403,0.30266215317206246,0.3071163752480134,0.3116847048331602,0.31637983387948987,0.32121461636093535,0.32617934113840036,0.3312034969479322,0.33628789936039244,0.341433373709564,0.3453366170023384,0.3491014352376824,0.3527342553666023,0.3562412764139938,0.35962848006104653,0.3639371375944278,0.3688024223643235,0.3733703166207313,0.37734031475029944,0.3807529300002237,0.3825508153516568,0.38379039654103203,0.38451176980089996,0.38474258012853196,0.38449891550749193,0.38378592344318907,0.38278347585851363,0.3817891097563955,0.3808024376830703,0.3798230753083293,0.3792632806644549,0.38000846921902737,0.3790494424022664,0.3802857630948676,0.379340153669228,0.379907374472311,0.3791857268359891,0.38010264257977616,0.3791839093204662,0.38010210574527864,0.37919680113392673,0.3800835097948165,0.3791916312134903,0.3800871556177927,0.37920869684565706,0.3800642908124802,0.3791992500678343,0.3800729437044851,0.3792213139162117,0.38004519724805397,0.379206977635184,0.3800592192276657,0.37923440174649886,0.38002659080195833,0.3792151757444864,0.38004560248914493,0.37924758083900223,0.3800089303360931,0.3792243033825149,0.3800316698800791,0.37926042780128677,0.37999265928910364,0.3792495876221996,0.3800022416778628,0.3792577645766755,0.3799926762511358,0.37927603379843094,0.3799737026653547,0.37930736936534404,0.3799431125824665,0.37934880575654634,0.37990442621578857,0.379396432238125,0.37986178353751454,0.3794460629570852,0.37981925740289185,0.3794938992341733,0.3797802532841733,0.3795370361104169,0.37974711958199225,0.3795737216415832,0.379721022368856,0.37961228720985773,0.3797357177757061,0.37976674275577565,0.37987713524939093,0.3800023568300216,0.38018467690225805,0.380546418257276,0.3810882319301787,0.38170380318195335,0.3824469289498103,0.383317838019921,0.3842095338750663,0.38499474139657214,0.3856739266485897,0.38624750003683606,0.3867238736183444,0.3871757050058723,0.38760310561259503,0.3880061811118151,0.38838503146301917,0.3887397509365931,0.3890704281371995,0.3893771460258263,0.389659981940509,0.38991900761573317,0.39015428920052264,0.3903658872752162,0.39055385686693866,0.3907357784982284,0.3909111891181698,0.39108009073769173,0.39124248529575156,0.39139837465935723,0.3915477606235888,0.39169064491161937,0.3918270291747341,0.3919569149923503,0.39208030387203485,0.3922004248453963,0.39232000514105736,0.39243904476257235,0.3925575437134795,0.3926755019973012,0.39279291961754376,0.39290979657769715,0.39313354539910106,0.3934111030024384,0.3937427486435516,0.3941288287358064,0.39456975719862514,0.39506601588983736,0.39561815512228277,0.3962267942652077,0.3968926224310978,0.3976163992486989,0.3983989557230829,0.39921817719467084,0.399916146174631,0.40032145731448154,0.40076105554343233,0.40127422264544294,0.4018648889864706,0.40253764269807585,0.40329776481809293,0.40415127031225995,0.4051049552909961,0.40616645078809,0.40734428352345925,0.40864794413080263,0.4100879633934166,0.4116759970981843,0.4134249201893505,0.41534893098074155,0.41726299490834995,0.4188973221955185,0.4202759995871628,0.41712724191853867,0.41805486499432465,0.4187250457744377,0.4191500320314032,0.4187384174853859,0.41869632327801876,0.4184194678052089,0.4179804495099636,0.41729716741597034,0.41636700512453095,0.4151858905083542,0.4137482801187165,0.4120471349169771,0.4103629202192006,0.4089262249620168,0.40772960576407735,0.40676671315539226,0.4060322586813308,0.4055219868411578,0.4050710449825714,0.40464865476184714,0.40425474572533365,0.4038892518801917,0.40355211168338784,0.40324326803138294,0.4029626682505181,0.4029626682505181,0.4029626682505181,0.4029626682505181,0.4029626682505181,0.4029626682505181,0.4029626682505181,0.4029626682505181,0.4029626682505181,0.4029626682505181,0.4029626682505181,0.4029626682505181],\"y\":[0.36,0.3587315051257611,0.35683723683394586,0.35492435955772556,0.35299260188419684,0.35104168973413985,0.3490713463231299,0.34708129212226746,0.34494364532621985,0.3425682323157383,0.3399489620360805,0.3369989472343641,0.33236202288271477,0.3276614661111517,0.3228965151976142,0.31806639796694486,0.3146550577928663,0.31140210307711075,0.30830191682003977,0.30534914281425973,0.3025386763958565,0.2998656556250421,0.29670784119708765,0.2903591349333649,0.2844406149422888,0.2788959737237734,0.27515745053862933,0.2716874202382149,0.2684025067079698,0.265225321730802,0.26208260554255997,0.2589035009289061,0.2584924345716547,0.25810107613388455,0.25772926673526125,0.257376855112714,0.257038536612056,0.25669489418267377,0.2563580918216972,0.2560124156828487,0.2556701617796057,0.25532725045117755,0.25498973367798417,0.25464535367267194,0.2543066998822149,0.25396230259141556,0.25362365935501757,0.2532793846718508,0.25294110702812816,0.25259678317641676,0.2522584338565419,0.25191427230757896,0.25157637250521403,0.25123212111264087,0.2508940511961457,0.2505500096418721,0.25021248521517375,0.24986830731565424,0.24953050721963468,0.24918659013721223,0.24884943150953126,0.2485053315781561,0.24816779918449866,0.247824007441877,0.24748719601543087,0.2471431828924859,0.24680592441114263,0.24646225484959622,0.2461255491779216,0.24578177652081779,0.24544509659205782,0.24510140170190534,0.24476469619154817,0.24442117505088679,0.24408428909218227,0.2437410659685287,0.24340388908274452,0.24306106004002007,0.2427235152274237,0.24238113913650233,0.2420431887705799,0.24170128469276753,0.2413629298819078,0.24102148075115942,0.24068275499150577,0.24034171611415484,0.24000267523632215,0.23966198525683094,0.23842078918995083,0.2354112507744441,0.2324039847516375,0.22939728990775846,0.22639262742186875,0.2233900860760046,0.22037664992555017,0.21734159738735082,0.21072522228281476,0.2040969798201115,0.19745441092545576,0.1927965388632133,0.1881590527635861,0.18353951808886054,0.17893551095648758,0.1773307182466572,0.17573321074741693,0.17414261313347917,0.1725585517834049,0.17098065469182328,0.16940855138205035,0.1678418728190847,0.16628025132296173,0.16472332048244415,0.16317071506903022,0.16162207095125808,0.16007702500928678,0.15853521504973453,0.15657685621582296,0.15461910222193626,0.15266193142072967,0.15070532217154436,0.14874925284016777,0.14679370179859474,0.14483864742478864,0.14288406810244125,0.14092994222073474,0.13897624817410248,0.13583305239608753,0.13268987728071543,0.12954672273498127,0.12640358866588083,0.1232604749804106,0.12011738158556752,0.11697430838834913,0.11545119440511203,0.11392017579249653,0.11237935158639537,0.11082681095711235,0.1092606308368118,0.10767887353465559,0.10607958433666499,0.10446078908733308,0.10282049174999065,0.10115667194290669,0.09946728244808092,0.09825993374348328,0.09750612174346793,0.0967578100734097,0.09597976353333273,0.09516015039712417,0.09429201429546091,0.09336802721981938,0.0923804273268719,0.09132095342012564,0.09018077555272674,0.08895042116563959,0.08761969614073481,0.08617760010840234,0.08461223530383782,0.08291070821478522,0.081059023205836,0.07904196723993492,0.07660748522311897,0.07438262446738786,0.07234555775635851,0.07045720511997117,0.06821674552841495,0.06608287541298635,0.06402601310399131,0.062017984274535254,0.059962658455632034,0.05786207160790688,0.05469798433241885,0.051500005363314394,0.04824927782397029,0.04492674285487552,0.04151302723310449,0.03798832978804026,0.0342650737162066,0.030653783050938743,0.02713800287420548,0.023701773098723372,0.020329555745501465,0.017006164408286117,0.014721935895462002,0.012443343915310157,0.010170031587588418,0.007901642912687551,0.005637822715874041,0.003378216591669314,0.0011224708483585644,0.0011224708483585644,0.0011224708483585644,0.0011224708483585644,0.0011224708483585644,0.0011224708483585644,0.0011224708483585644,0.0011224708483585644,0.0011224708483585644,0.0011224708483585644,0.0011224708483585644,0.0011224708483585644],\"z\":[0.060602664947509766,0.060125387312662315,0.0595213483374301,0.05890303660885399,0.05827010185975168,0.057622185542795426,0.05695892062739995,0.055299451341465335,0.05280766251016197,0.05009851412028249,0.047134375217976414,0.043417772975334355,0.03877331114769974,0.03400930475957501,0.02912266281931479,0.024569434690461762,0.021936210359571398,0.019510348174587574,0.01727519502638345,0.015215408145260899,0.01331684977939333,0.010508521337818119,0.006136618124757418,0.0002925578370537678,-0.004565137667336505,-0.008384244409491846,-0.010004762456306783,-0.011298293557231257,-0.012394009287195515,-0.013397835694008133,-0.01440287529210727,-0.015183041479785782,-0.015299619024527322,-0.015413046493570825,-0.015523501666943523,-0.015589893653586283,-0.01552539579909462,-0.01563609786276906,-0.015519907068654382,-0.015632862729508076,-0.01558274779612832,-0.015670534706558326,-0.015585431238711075,-0.01569647485117155,-0.015608711708221154,-0.015719655234820492,-0.01563394252064111,-0.01574444032125971,-0.015656759845339603,-0.0157672109332899,-0.01568250351482186,-0.015792281540939115,-0.015704744933452478,-0.01581459451027577,-0.01573109826732166,-0.015839980409468903,-0.01575269512674645,-0.015861832970762036,-0.015779691935466016,-0.01588750890318768,-0.015800656759534522,-0.015908960949476345,-0.015828234531026823,-0.015934835242106843,-0.015848686648714173,-0.01595601242155338,-0.015876672504066283,-0.015980455384877183,-0.0158986968194596,-0.016004117868361287,-0.01592310807580413,-0.016026278190756253,-0.01594875830219608,-0.016046835448868032,-0.015975996763854382,-0.01606598426544162,-0.016004433762916152,-0.016084178427966077,-0.016033571736425854,-0.016101938178800592,-0.016062889773064512,-0.01611976494054136,-0.016091924589600834,-0.016138069142853362,-0.016120330207512426,-0.0161571256297201,-0.016147906087334466,-0.01626725282171525,-0.01662787380037991,-0.01754730811593064,-0.018454945358902554,-0.019369999291999597,-0.020284441292620126,-0.021192562016980276,-0.02212024140148715,-0.02402403119950906,-0.028447417837816052,-0.032905497149400255,-0.03689537409985199,-0.039134967943909416,-0.04133892581317092,-0.04351189213289477,-0.04534057446925061,-0.045619634487034544,-0.045894148808964,-0.046164376357202615,-0.04643057192272667,-0.04669298640560307,-0.04695186705151518,-0.04720745768475949,-0.047459998937932756,-0.04770972847852606,-0.04795688123263906,-0.048201689606025326,-0.04844438370267645,-0.04874662806493166,-0.049133336095780134,-0.04951957887961701,-0.04990537350451235,-0.050290737038343,-0.05067568652954777,-0.05106023900788156,-0.051444411485168796,-0.05182822095605606,-0.05221168439876384,-0.05271663623061254,-0.05370604061065659,-0.05469541907055965,-0.05568477172742316,-0.056674098698345485,-0.057663400100421924,-0.05865267605074477,-0.059151783048946235,-0.059389980988201294,-0.05963358786434926,-0.05988380596573168,-0.06014186757644942,-0.06040904105155952,-0.060686637057138004,-0.06097601500594004,-0.06127858972028824,-0.06159583835487525,-0.061929307613381994,-0.062229652588812016,-0.06238777636493595,-0.06250070003246379,-0.06257751871586469,-0.06266061309263722,-0.06275810071538983,-0.0628732040418156,-0.06300971675116232,-0.06317212894658458,-0.06336577497939778,-0.06359700876738587,-0.06387341237366076,-0.06420404469676558,-0.064599738432061,-0.06507345504165253,-0.06564070936671235,-0.0663200777940208,-0.06715735657467718,-0.06806535899654903,-0.06878568054292493,-0.0692399522208038,-0.06942111791977817,-0.06998525135357594,-0.07046907328265085,-0.07090015849598016,-0.07130510913277836,-0.07173275929828313,-0.07240506820516007,-0.07343621036785826,-0.07452247364036656,-0.07568905942165512,-0.07696278154809767,-0.0783726853843603,-0.0799642375813726,-0.08159247148585468,-0.08306853122539322,-0.08441982382861496,-0.08567131141405482,-0.08684597354539217,-0.08766427807769468,-0.08820509541069274,-0.08874093188438181,-0.08927212391115816,-0.08979900489588814,-0.09032190544520004,-0.09084115357496177,-0.0913570749160749,-0.0913570749160749,-0.0913570749160749,-0.0913570749160749,-0.0913570749160749,-0.0913570749160749,-0.0913570749160749,-0.0913570749160749,-0.0913570749160749,-0.0913570749160749,-0.0913570749160749,-0.0913570749160749],\"type\":\"scatter3d\"},{\"line\":{\"color\":\"#ffa600\",\"width\":10},\"mode\":\"lines\",\"name\":\"Stochastic Gradient Descent\",\"x\":[0.29,0.2902160764662524,0.2954457903869539,0.2998863936890577,0.3204940657053405,0.3120896838962058,0.30789402053477005,0.31421200466906185,0.31284280072243337,0.3153579264625449,0.3239664803100807,0.321906698309256,0.32909857472611337,0.33645000474483555,0.34707591933750853,0.3483362770844944,0.3530640833120768,0.35311630853832954,0.3609616936071282,0.3651507315380679,0.3764139159554607,0.3756029477386559,0.37922639776648287,0.38173785426260726,0.38287686846901636,0.3928910962494347,0.3906415204657274,0.3782549035603087,0.3912778501171705,0.40379011018755706,0.41615001137822094,0.4123047224122145,0.41326725449606766,0.40739640975727304,0.40354613005602935,0.4029400069417843,0.40168821648588743,0.403515437168473,0.39806163300271347,0.4050850953919297,0.40866106345367864,0.40773259523158284,0.39576897858281884,0.40034186841858793,0.4087665438653212,0.41335701215699544,0.4073092008596895,0.40972915603840726,0.4012114659887404,0.41541093216374697,0.4081977088972471,0.40194020482987686,0.4131282234258999,0.4130103444710295,0.4055306966131062,0.4062176786086252,0.40058574362015015,0.38547423107304146,0.3844931162304821,0.3872246385050772,0.3933298463813671,0.3895201852021847,0.3914191189263236,0.3931345049496725,0.3943312872138931,0.39023050019427974,0.3980509569747826,0.4092067144285307,0.42671359863373887,0.4136773881527138,0.41363274302688186,0.4034773431929658,0.40323201412547544,0.40805678330223366,0.4045578772845563,0.4047060252067853,0.4130641024297493,0.41925514187165475,0.40604454511399374,0.40316704750525345,0.39966192542506795,0.4010847340873487,0.40504656549713697,0.39815221390033645,0.39904965990866204,0.40445188499738394,0.4043536793102983,0.40960584575960957,0.4075588780983578,0.39876025126499465,0.4047605396080265,0.3934519064917552,0.3982475569334228,0.3973050412038636,0.4032811199474399,0.40291060522187816,0.4006436078498961,0.40159283605958196,0.40451038432618397,0.40749594747175855,0.4117315412139906,0.42155301846187804,0.42064715356607296,0.4149547423346403,0.4080148290038124,0.402413242610286,0.3836853838497441,0.3815732630982751,0.38488532639700135,0.37190161846531145,0.38211945821317606,0.37565936131668337,0.38496978138465127,0.3829076624360208,0.3972345721249393,0.39105658197165577,0.39260039409031294,0.3998226048750251,0.38656847664697824,0.3819510567155697,0.38241995842527415,0.3884180790715428,0.389535771682853,0.3914932084227432,0.38723220953150866,0.3824117185895498,0.38780133931888217,0.39239211088294995,0.3968854015311003,0.4071224418047198,0.4108033342957708,0.41080416080744864,0.4056395959248371,0.3969423266005586,0.4033068698869689,0.40052842233179026,0.4051829556558393,0.39198121403943936,0.392240439035551,0.38823401799200663,0.39362019203129683,0.39819217966394505,0.3957575664870198,0.397169261197322,0.3933519603596114,0.40270324823505044,0.39719769634012575,0.4018169033445068,0.38831761367746104,0.38945581575285776,0.3808935640210366,0.3904293463885099,0.38671630892681796,0.39262454215791986,0.3953888460417557,0.3944905290888395,0.3961964375360433,0.3938543859593637,0.3883911821048423,0.3836049802363717,0.38401234548814334,0.39335408212561224,0.3987715804389176,0.3999209079210053,0.4143866441001585,0.40825590549506713,0.40948826037849,0.41081039433508515,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585,0.40374184154203585],\"y\":[0.36,0.3574081314532063,0.3591889430473936,0.3603143312821142,0.3765963484221749,0.363483202726602,0.3548407165963346,0.3547017527664512,0.34653092920523126,0.34188136512001527,0.3429427782566786,0.33284382546463315,0.33037192658904996,0.32793572159804135,0.3286485539651119,0.3209652557158457,0.31722857590896375,0.31047216463439514,0.3117851934472482,0.30825568353697724,0.31193058431435616,0.3036593982190196,0.29994853479265654,0.2951449497050404,0.28721246847932597,0.2893829762545215,0.2803512834559649,0.26210047058581504,0.26957815716662464,0.27951728642351703,0.28845288951972853,0.2803363061481671,0.27712299460746853,0.2695334251899924,0.2640710412167349,0.26195282008232357,0.25928273400977003,0.2596930654156238,0.2529065813926975,0.260122869166618,0.2625271316089123,0.26049964406272413,0.24750518531960317,0.2522097832995085,0.2590221438531535,0.26196277500675197,0.25514226534753154,0.2558173558898238,0.2454974802780007,0.25783555690298576,0.24869979517558316,0.2404565949203258,0.249593684793322,0.24735750161215803,0.2376899606574375,0.232805035320841,0.22152869473380635,0.2006993350183825,0.19208087816421548,0.18941255152580677,0.19024136190745874,0.18127593230401967,0.17813696885085617,0.17786249062352213,0.17709983975388616,0.1710695853322946,0.17699008217490989,0.18627493589539118,0.19914797860491879,0.18645196502660163,0.18182589265831914,0.16714499291418536,0.1660028820324776,0.1699797839015645,0.1656792570659568,0.16506950816880717,0.17271102766775007,0.17822459366112534,0.16551292544983376,0.16196769303316355,0.15783125739948894,0.15783904244856728,0.16082468934895522,0.1534638861014902,0.1522631749101659,0.1556468190966457,0.15500582263743412,0.15975751281085676,0.15748207920340293,0.14823980245247922,0.1521613238145723,0.14060568155991707,0.1433302975946993,0.1403236330852871,0.14424242578320262,0.14412690618551208,0.14209502521292441,0.14326104078527727,0.14637847625980346,0.1495483439487805,0.15395387435215013,0.1639320404600379,0.1653146221643827,0.16213849818368475,0.15520090862032138,0.1493690508758491,0.13042887209425416,0.12505346081618002,0.1251027949241335,0.10885691904094448,0.11427115015556011,0.10634989547516248,0.11151368009845158,0.10779229046938628,0.120401310533264,0.11096408541458902,0.11061417536074726,0.11587576229354331,0.10235949838919207,0.09570292864913635,0.09449238861665801,0.09882430232628844,0.09828891850989653,0.09860630585631937,0.09271818194658128,0.08628338851081609,0.09007142822340139,0.09307323999644727,0.09599009261799549,0.10466311795192978,0.10612454969690709,0.103623593771854,0.09563901264349128,0.08511544841674415,0.09000212276660849,0.08508495219891885,0.0873985336800301,0.07163448946581966,0.06705040947481745,0.0586335435632417,0.06021045577088619,0.06110661740222963,0.055324697382897,0.05328634579700775,0.046133406443173694,0.052259669679804346,0.043248274033905514,0.04486566027684517,0.02836291086924567,0.028367132220347167,0.018582828262844277,0.02616561289093584,0.0209839304604855,0.025309455260698326,0.02636812842336355,0.023631711603015436,0.023356762716496404,0.018880008969338058,0.011441863580055564,0.0046772979610385885,0.0031028713163987927,0.010459381971235172,0.01388821426503823,0.013045429729793244,0.025683141967169858,0.01653979284849546,0.015898859157241273,0.015324289431303217,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544,0.006335325171507544],\"z\":[0.060602664947509766,0.06008253733843565,0.05960688430841411,0.05910610990235098,0.05219365572147084,0.05473250586223018,0.05484445511255302,0.052253701079903375,0.050617207616715135,0.04818051223317039,0.04441186145760603,0.041143789065167906,0.03636374059355555,0.03147429466668349,0.02687298835779784,0.022908230948217596,0.01978341716474709,0.017778668790444895,0.015164683183084407,0.012130535456851601,0.007354464730623,0.005785745769885264,0.003114342265112434,-0.0005030583441256296,-0.00485081174032846,-0.00702351282284434,-0.009777012334756694,-0.014445602991352979,-0.012295443226409158,-0.012483057942331788,-0.012864794424896532,-0.013955261475571511,-0.014223519394024227,-0.01354631121404496,-0.013433580798033161,-0.013525400964497171,-0.013637847331167031,-0.013719901885929135,-0.014166797715104573,-0.013783800136843768,-0.013927968228665811,-0.013934240327048411,-0.014718876036702834,-0.01407597949728486,-0.014116603162895135,-0.014292553538265631,-0.014398574380367618,-0.0145152535925293,-0.014668342356020348,-0.01469347331512813,-0.015111117104526601,-0.0151479326320302,-0.015493751339421297,-0.015741247538911513,-0.0166272753534388,-0.018806730358545614,-0.022779293990694085,-0.03544588599946851,-0.039488657716421224,-0.040917709365324934,-0.040982783838872605,-0.04475475536029279,-0.04570877052498484,-0.04582954929561528,-0.045997558268139796,-0.046744718947631436,-0.04617994383881204,-0.043596283406815386,-0.03579363976020574,-0.043733541653985616,-0.0456327800949245,-0.04753275513551929,-0.047679442223733805,-0.0471209690735306,-0.04766742613297893,-0.047731054868762536,-0.04685791280101008,-0.04666538815499373,-0.04762745836594515,-0.0481748391617859,-0.0490480519879058,-0.04895866276088527,-0.048191795754845466,-0.04986519536863708,-0.05011175946722494,-0.04901334679604187,-0.04912529327758819,-0.047994542957322375,-0.0484456192008143,-0.05088404131367444,-0.04952362134490268,-0.05229332037180712,-0.05182536819009494,-0.05239505050140176,-0.05104063062689061,-0.05113163663443557,-0.05195167753223131,-0.051542743370985314,-0.05046735488423106,-0.04951642016885093,-0.048429762458713,-0.0466947993390311,-0.04689272762767719,-0.047468988610211774,-0.04870189570307424,-0.05031636833877722,-0.055303637562605465,-0.05696914564327019,-0.0569926244715076,-0.05739082534178999,-0.05921902722370226,-0.05885742969375124,-0.05965266819873246,-0.06000205963513845,-0.05861536948159808,-0.05998639375198367,-0.06010662894382765,-0.05950060573637121,-0.06090848766294234,-0.061217189652422864,-0.061361488943098404,-0.06146593875659024,-0.06160137916111516,-0.06173406171116129,-0.06188735280067804,-0.06208837291631577,-0.06215427850821079,-0.062253944818126045,-0.06237416597521345,-0.061166209267471496,-0.06053680844535365,-0.061324035346576974,-0.06295461078289963,-0.06321792059228923,-0.06342508598882854,-0.06351766303083038,-0.06394965483309191,-0.06740252294168167,-0.0696475069717017,-0.07423296001761372,-0.07284827213805131,-0.07195242831129497,-0.0749189943825167,-0.07569269730616865,-0.07964391399136604,-0.07554741248682391,-0.08038566477392814,-0.0789118770374491,-0.08583543668902319,-0.08573426533639454,-0.08800175411097752,-0.08612818267841832,-0.08737165372665087,-0.08617339149579593,-0.08572392311736922,-0.08647174567594945,-0.08645635552670694,-0.0876393053313327,-0.08939748948888782,-0.09098049255282854,-0.09131654764371949,-0.08950643863183537,-0.08863112185227316,-0.08879240826312576,-0.08434313179313598,-0.08764911787260349,-0.0877438456344237,-0.08782109790081623,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316,-0.09015827226267316],\"type\":\"scatter3d\"},{\"line\":{\"color\":\"#003f5c\",\"width\":10},\"mode\":\"lines\",\"name\":\"SubGD\",\"x\":[0.29,0.2909767964517507,0.2927881638958009,0.2939372075805374,0.2928500360548762,0.29478927862064697,0.2961113074839728,0.2960915881257825,0.2976076451431782,0.29930526449343,0.3010024978623395,0.3025952826980272,0.3049458368745373,0.3055404528229629,0.3069235507408032,0.30786953147357016,0.3103013986305369,0.31254486016557953,0.3142587775309707,0.31524438657491827,0.3167750446841677,0.318034006461569,0.31885040605571197,0.3217888561654324,0.3241810858457774,0.32142581851842356,0.3202850595723385,0.3214589827553177,0.3223343892185692,0.3236251265489922,0.3249012389455067,0.3287686643932123,0.329281828070823,0.33078991501340094,0.3355818996258257,0.3390016940296464,0.3424753077663251,0.3433682459087136,0.34272743353673196,0.3445633341111654,0.34636856636740365,0.3463868094036335,0.3471530188810152,0.34758597588810036,0.34682050157288014,0.34778923290933095,0.3484529422924502,0.3490350156650162,0.3488732689152949,0.3502225062292148,0.35198162382524606,0.3529589958799293,0.35237038605101567,0.3539124636431454,0.35363177205958896,0.3552509886805997,0.3541707978642833,0.35583207070345313,0.3561897012359505,0.35485519184574943,0.3548208556337626,0.35529245956248146,0.35605884114408937,0.3550920749804926,0.35396489972804746,0.3546465892520109,0.3544047746751585,0.3551305500748149,0.3565674934565442,0.3546789279868724,0.35542736459928664,0.35749971169047606,0.3574562470570935,0.3567081909570076,0.35811627621609915,0.35882231997277564,0.36040244662092863,0.361043317074884,0.3601306583997704,0.3610913089689914,0.3616023926856536,0.36379724294308263,0.3652198626917933,0.3641535650622571,0.3636727332573323,0.3660419080495791,0.3680511766291888,0.36812643975680115,0.36907525337105657,0.37058060690005973,0.3720269019820589,0.37402782042354593,0.3750198525980522,0.3754223686958265,0.37555618947330294,0.37748146189346926,0.37465692618271235,0.3750383488589568,0.3756144714981577,0.37412514947218056,0.37521154546330354,0.3748289954426085,0.3765671007492768,0.37676061016409457,0.3780250188986178,0.3799082810180473,0.3821486147991565,0.38256509525528815,0.3820830044819613,0.38611448543052446,0.3872897686331722,0.38764971993513964,0.3891468945295325,0.3901545187016585,0.3925765492868682,0.3931752573288626,0.3937728796355685,0.3933965705302732,0.39463020291240597,0.3938534229827227,0.3953770683470951,0.39814720772210344,0.39666590532677587,0.39401202680079384,0.39547540103639595,0.3996565446195573,0.39888637604057214,0.3977821918982916,0.3992040871424614,0.39866005544823774,0.3985786115397295,0.39871400743672114,0.4000585713116492,0.4001032780947697,0.4010868068399841,0.40145313884695333,0.4009042624991985,0.40107244381491347,0.4003756009574618,0.40098829725166507,0.4000525582977152,0.39915266644919667,0.40176576630083155,0.4023064057533196,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821,0.4028499298445821],\"y\":[0.36,0.35689675976570445,0.35114212345289325,0.34749166118202157,0.3509455582697571,0.3447846682057398,0.3405846394769092,0.340647287028117,0.3358308392150274,0.33043757569448745,0.32504553842003875,0.3199853296346537,0.31251772030789604,0.3106286510634277,0.3062346085800426,0.30322926855702204,0.2955033313953594,0.2883759507756196,0.2829309091633094,0.27979967170336395,0.27493683685065745,0.2709371695360704,0.2683435032064413,0.25900817383396957,0.2514081632274434,0.2601615287941996,0.26378567080738874,0.26005616744272764,0.2572750388327133,0.2531744219554746,0.24912026786303354,0.23683362387395274,0.23520332498385502,0.23041219775500849,0.21518826914147912,0.2043237296891352,0.19328820853552853,0.19045138253638547,0.19248721582775713,0.18665463881692027,0.1809194937314053,0.180861536391447,0.17842732189993454,0.17705183614576464,0.1794837150587703,0.1764060973482339,0.17429752123917486,0.17244829921771743,0.172962161671458,0.16867569286757908,0.16308705207033075,0.15998198317032297,0.16185197123192246,0.15695285717420293,0.15784460223936903,0.15270042081103308,0.15613214048599874,0.15085434826489244,0.14971817147445737,0.1539578502993495,0.15406693496518747,0.15256866960639215,0.15013390834716964,0.15320528278823342,0.1567862699920882,0.15462057175038213,0.15538880624056953,0.15308304907157194,0.14851794186976466,0.15451783298329475,0.15214008211790106,0.14555632471247848,0.1456944099797211,0.14807095197384498,0.1435975257840798,0.1413544551945371,0.13633446092547283,0.13429844311017355,0.13719792039197104,0.13414597493070068,0.13252228398690502,0.12554933930134637,0.12102973766439343,0.1244173192836551,0.1259449012075015,0.11841813498696306,0.11203477529459809,0.11179566758147434,0.10878132761604192,0.10399888432433188,0.09940406717375838,0.09304723551037801,0.08989559204143834,0.08861681574436425,0.08819167290103666,0.08207516538759171,0.0910485936234493,0.08983683021760042,0.08800651341961004,0.09273802531704915,0.08928659206912153,0.09050193699970936,0.08498005134028824,0.08436528026827907,0.0803483082056404,0.07436526560729373,0.06724782172701664,0.06592468126520502,0.06745626287741215,0.05464842163172145,0.05091459754318047,0.049771047768147725,0.04501458859924124,0.041813410025766766,0.03411872322869067,0.03221665357929398,0.030318033264121556,0.031513551075423,0.027594354158089012,0.030062150521052846,0.025221594856621687,0.016420981435345566,0.02112701531179944,0.02955827302572704,0.024909196140145903,0.011625883153722176,0.014072675540419882,0.01758062097803342,0.013063321060785785,0.01479168630947296,0.015050430097155672,0.014620283167004639,0.01034866167741112,0.010206630153978756,0.007082001713462689,0.005918180713601674,0.007661937217787479,0.007127632425168745,0.00934147215436703,0.007394962430496011,0.010367764763561745,0.01322668238742504,0.004924976764630159,0.0032073885217557066,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858,0.0014806359058020858],\"z\":[0.060602664947509766,0.059866545069423904,0.05848228157968441,0.05759123610320436,0.05843455756035555,0.05692399895860907,0.05587781854749121,0.05589352101436697,0.05398823110390455,0.05173554091830272,0.04910242608816626,0.0463166744941436,0.042131360341266265,0.04107163501591433,0.03860515619864481,0.03691696432553218,0.03162879816187996,0.026272447424713845,0.022303192809672915,0.020118828321072507,0.017945667158265836,0.016239909191379968,0.015173171421020428,0.0110321387198641,0.008412037465608254,0.01146328000223405,0.013278073194156397,0.01140975928209067,0.010426121806963317,0.009012321633658133,0.007642240156023201,0.002389486399741141,0.0012317428956190911,-0.0021246606771938453,-0.013423623809775614,-0.023086787980778508,-0.030572437881316653,-0.03216830378364888,-0.031020616673700215,-0.03434158731319919,-0.0377056310318865,-0.03774012561037023,-0.038546031213851084,-0.03880160582285777,-0.03834988340300089,-0.03892165819967322,-0.0393139838473244,-0.039658444518531524,-0.039562689015583546,-0.040362313187991765,-0.041407800383567325,-0.041989376480287596,-0.041639301582890545,-0.042427094291731,-0.042299486819680564,-0.043021238434298906,-0.042543614229230434,-0.04327177151169864,-0.04342373726347948,-0.04284802676251321,-0.04283290233760122,-0.043039266981723806,-0.04336832848673136,-0.04295194303997536,-0.04245081684053388,-0.04275590034699779,-0.042648384003150935,-0.04296875109632578,-0.043582427632834375,-0.042770220098945905,-0.04309775616114146,-0.04396590392760163,-0.043948280545451676,-0.043641043358617196,-0.04421320036295533,-0.04449019289346946,-0.04613755140366006,-0.04693308498333898,-0.045801854628008724,-0.046992883193534424,-0.04763163536214752,-0.05041505183719368,-0.05225407294644611,-0.05087309185503684,-0.05025540502624102,-0.0532791992317348,-0.05554179305325082,-0.055621318322879004,-0.056591491291511094,-0.058007650184100513,-0.059255955509552295,-0.060915157468601364,-0.061560293565423946,-0.061788503741788806,-0.061860084448816584,-0.06265294685839982,-0.06133792377441338,-0.06157120476437226,-0.061890590247248856,-0.06098365748264638,-0.06167139127541908,-0.061445315911652996,-0.062331648156188364,-0.062407983769405026,-0.06279657906279376,-0.06588295234408136,-0.0700289763354744,-0.0707722539353654,-0.06991098589022678,-0.07649139664614321,-0.07820843973262799,-0.078728492754544,-0.08086229894177001,-0.08227179739500369,-0.0841099835851453,-0.08448550558716522,-0.08487798742190407,-0.08462879567104759,-0.08547180242872085,-0.08493223111483397,-0.08601867243865888,-0.08808467126237898,-0.0870271221129834,-0.08503998223243389,-0.08609272430502604,-0.08911258559919383,-0.0885877588717994,-0.0878364586061598,-0.08880418204754767,-0.0884336582249341,-0.08837821724229829,-0.08847038876537747,-0.08938590816979665,-0.08941574543463336,-0.0900761674864906,-0.09032411657234765,-0.08995301158544312,-0.09006646769343336,-0.08959783629231759,-0.09000967384526654,-0.0893818963010545,-0.0887691469965674,-0.09053655902766214,-0.09090577633467811,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161,-0.09127930364247161],\"type\":\"scatter3d\"}],                        {\"legend\":{\"bgcolor\":\"LightSteelBlue\",\"bordercolor\":\"Black\",\"borderwidth\":2,\"font\":{\"color\":\"black\",\"size\":14},\"title\":{\"font\":{\"family\":\"Times New Roman\"}},\"traceorder\":\"reversed\",\"x\":0,\"y\":1},\"scene\":{\"camera\":{\"center\":{\"x\":0,\"y\":0,\"z\":0},\"eye\":{\"x\":1.25,\"y\":-1.25,\"z\":1.25},\"up\":{\"x\":0,\"y\":0,\"z\":1}}},\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"SubGD optimization trajectory on random loss surface\"}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Setup optimization problem\n",
    "shape = (50, 50)\n",
    "scale = 100.0\n",
    "octaves = 6\n",
    "persistence = 0.5\n",
    "lacunarity = 2.0\n",
    "surface = generate_perlin_noise_loss_surface(shape, scale, octaves, persistence, lacunarity)\n",
    "interp_noise_loss = interpolate_loss_surface(surface, shape)\n",
    "\n",
    "## Do optimization\n",
    "# use noise loss + noise gradient steps\n",
    "theta_0 = np.array([0.29,0.36])\n",
    "gd_trajectory = do_gradient_descent(interp_noise_loss, theta_0=theta_0, lr=0.01, num_steps=200, gradient_noise_std=0.0)\n",
    "sgd_trajectory = do_gradient_descent(interp_noise_loss, theta_0=theta_0, lr=0.01, num_steps=200, gradient_noise_std=0.7, seed=2)\n",
    "# calculate subspace\n",
    "optim_trajectory = gd_trajectory\n",
    "theta_0 = optim_trajectory[0].theta\n",
    "theta_star = optim_trajectory[-1].theta\n",
    "subspace = theta_star - theta_0\n",
    "subspace = subspace / np.linalg.norm(subspace, ord=2)\n",
    "subgd_trajectory = do_gradient_descent(interp_noise_loss, theta_0=theta_0, lr=0.01, num_steps=200, gradient_noise_std=0.7, subspace=subspace)\n",
    "\n",
    "\n",
    "# Plot results with Matplotlib\n",
    "gd_style = {'lw': 3, 'c': '#bc5090', 'label': 'GD'}\n",
    "sgd_style = {'lw': 3, 'c': '#ffa600', 'label': 'SGD'}\n",
    "subgd_style = {'lw': 3, 'c': '#003f5c', 'label': 'SubGD'}\n",
    "styles = [gd_style, sgd_style, subgd_style]\n",
    "trajectories = [gd_trajectory, sgd_trajectory, subgd_trajectory]\n",
    "\n",
    "fig = plot_interp_loss_surface(interp_noise_loss, shape, trajectories, styles, xlim=[0,0.6],ylim=[0,0.6])\n",
    "\n",
    "# Plot results with Plotly\n",
    "gd_style = {'lw': 10, 'c': '#bc5090', 'label': 'Gradient Descent'}\n",
    "sgd_style = {'lw': 10, 'c': '#ffa600', 'label': 'Stochastic Gradient Descent'}\n",
    "subgd_style = {'lw': 10, 'c': '#003f5c', 'label': 'SubGD'}\n",
    "styles_pl = [gd_style, sgd_style, subgd_style]\n",
    "optimum = gd_trajectory[-1]\n",
    "fig = plotly_interp_loss_surface(interp_noise_loss, shape, trajectories, optimum, styles_pl, xlim=[0,0.6],ylim=[0,0.6])\n",
    "# Note that include_plotlyjs is used as cdn so that the static site generator can read it and present it on the browser. This is not typically required.\n",
    "html = plotly.offline.plot(fig, filename='3D-SubGD-SGD-GD-plotly.html',include_plotlyjs='cdn')\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "efb944956e68c5a4a3f866131a290f604672ac43206fde6373334f1e4d6c02e5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
