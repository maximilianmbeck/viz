{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import copy\n",
    "from IPython.core.display import HTML, display\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import noise\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "%matplotlib qt \n",
    "# for this run pip install pyqt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_loss_surface(shape, scale, octaves, persistence, lacunarity):\n",
    "    surface = np.zeros(shape)\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            surface[i][j] = noise.pnoise2(i/scale, \n",
    "                                        j/scale, \n",
    "                                        octaves=octaves, \n",
    "                                        persistence=persistence, \n",
    "                                        lacunarity=lacunarity, \n",
    "                                        repeatx=1024, \n",
    "                                        repeaty=1024, \n",
    "                                        base=42)\n",
    "    return surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate grid interpolation\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "\n",
    "def interpolate_loss_surface(surface, shape):\n",
    "    lin_x = np.linspace(0, 1, shape[0], endpoint=False)\n",
    "    lin_y = np.linspace(0, 1, shape[1], endpoint=False)\n",
    "    interp_loss_surface = RegularGridInterpolator((lin_x, lin_y), surface)\n",
    "    return interp_loss_surface\n",
    "\n",
    "\n",
    "def plot_interp_loss_surface(interp_surface, shape, optim_trajectory=None):\n",
    "\n",
    "    lin_x = np.linspace(0, 1, shape[0], endpoint=False)\n",
    "    lin_y = np.linspace(0, 1, shape[1], endpoint=False)\n",
    "    x, y = np.meshgrid(lin_x, lin_y)\n",
    "    xy = np.stack([x, y], axis=2)\n",
    "    z = interp_surface(xy)\n",
    "    fig = plt.figure()\n",
    "    # contour plot\n",
    "    ax0 = plt.subplot(1, 2, 1)\n",
    "    ax0.contourf(x, y, z, cmap='terrain')\n",
    "    # 3d plot\n",
    "    ax1 = plt.subplot(1, 2, 2, projection='3d')\n",
    "    ax1.plot_surface(x, y, z, alpha=0.6, cmap='terrain')\n",
    "    if optim_trajectory:\n",
    "        thetas = np.stack([x.theta for x in optim_trajectory])\n",
    "        ax0.plot(thetas[:, 0], thetas[:, 1], 'o-', lw=3, c='r')\n",
    "        loss_values = np.array([x.value for x in optim_trajectory]) + 0.02 # add offset such that it is plot on top\n",
    "        ax1.plot3D(thetas[:, 0],\n",
    "                   thetas[:, 1],\n",
    "                   loss_values[:, 0],\n",
    "                   'o-',\n",
    "                   lw=3,\n",
    "                   c='r')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (50, 50)\n",
    "scale = 100.0\n",
    "octaves = 6\n",
    "persistence = 0.5\n",
    "lacunarity = 2.0\n",
    "surface = generate_loss_surface(shape, scale, octaves, persistence, lacunarity)\n",
    "interp_noise_loss = interpolate_loss_surface(surface, shape)\n",
    "# fig = plot_interp_loss_surface(interp_noise_loss, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize quadratic loss for testing\n",
    "def quadratic_loss(x):\n",
    "    assert len(x) == 2\n",
    "    z = x[0]**2 + x[1]**2\n",
    "    return z\n",
    "lin_x = np.linspace(0,1,shape[0],endpoint=False)\n",
    "lin_y = np.linspace(0,1,shape[1],endpoint=False)\n",
    "x,y = np.meshgrid(lin_x, lin_y, sparse=False)\n",
    "z = x**2+y**2\n",
    "interp_quadratic_loss = interpolate_loss_surface(z, shape)\n",
    "# fig = plot_interp_loss_surface(interp_quadratic_loss, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, NamedTuple, List\n",
    "\n",
    "class DescentStep(NamedTuple):\n",
    "    theta: np.ndarray\n",
    "    value: float\n",
    "\n",
    "class GradientDescent(object):\n",
    "    def __init__(self, loss_fn: Callable[[np.ndarray], np.ndarray], \n",
    "                    theta: np.ndarray, lr: float, gradient_noise_std: float=0.1, fd_h: float = 1e-3):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.theta = theta\n",
    "        self.lr = lr # stepsize\n",
    "        self.finite_difference_h = fd_h\n",
    "        self.gradient_noise_std = gradient_noise_std\n",
    "        self.rng = np.random.default_rng()\n",
    "\n",
    "        self.at_boundary = False\n",
    "        self.last_step = DescentStep(theta_0, loss_fn(theta_0))\n",
    "\n",
    "    def _compute_gradient(self):\n",
    "        # forward finite difference gradient\n",
    "        h = self.finite_difference_h\n",
    "        f = self.loss_fn\n",
    "        x = self.theta\n",
    "        gradient = np.zeros_like(x)\n",
    "        try:\n",
    "            for i in range(len(gradient)):\n",
    "                h_vec = np.zeros_like(x)\n",
    "                h_vec[i] = h\n",
    "                gradient[i] = (f(x+h_vec) - f(x))/(h)\n",
    "        except ValueError:\n",
    "            print(f'Try to compute gradient at function support boundary at {str(x)}. Setting gradient to zero!')\n",
    "            gradient = np.zeros_like(x)\n",
    "        return gradient\n",
    "\n",
    "    def _safe_decent_step_creation(self, theta):\n",
    "        try:\n",
    "            loss = self.loss_fn(theta)\n",
    "            return DescentStep(theta.copy(), loss)\n",
    "        except ValueError:\n",
    "            print('Reached function support boundary!')\n",
    "            self.at_boundary = True\n",
    "            return None \n",
    "\n",
    "    def _get_descent_step(self, theta):\n",
    "        descent_step = self._safe_decent_step_creation(self.theta)\n",
    "        if descent_step is None or self.at_boundary:\n",
    "            print('At function support boundary, using last step.')\n",
    "            return copy.deepcopy(self.last_step)\n",
    "        else:\n",
    "            self.last_step = descent_step\n",
    "        return descent_step\n",
    "\n",
    "    def step(self):\n",
    "        if not self.at_boundary:\n",
    "            grad = self._compute_gradient()\n",
    "            self.theta = self.theta - self.lr * grad\n",
    "        return self._get_descent_step(self.theta)\n",
    "\n",
    "        \n",
    "    def noisy_step(self):\n",
    "        if not self.at_boundary:\n",
    "            grad_noise = self.rng.normal(loc=0, scale=self.gradient_noise_std)\n",
    "            grad = self._compute_gradient() + grad_noise\n",
    "            self.theta = self.theta - self.lr * grad\n",
    "        return self._get_descent_step(self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start from theta_0: [0.8 0.6], Loss: [1.]\n",
      "Step 0: theta: [0.638 0.478] Loss: [0.6356]\n",
      "Step 1: theta: [0.512 0.384] Loss: [0.40976]\n",
      "Step 2: theta: [0.41  0.306] Loss: [0.26192]\n",
      "Step 3: theta: [0.328 0.244] Loss: [0.16728]\n",
      "Step 4: theta: [0.262 0.194] Loss: [0.1064]\n",
      "Step 5: theta: [0.208 0.156] Loss: [0.06776]\n",
      "Step 6: theta: [0.166 0.126] Loss: [0.0436]\n",
      "Step 7: theta: [0.132 0.1  ] Loss: [0.02752]\n",
      "Step 8: theta: [0.106 0.078] Loss: [0.01744]\n",
      "Step 9: theta: [0.084 0.064] Loss: [0.01128]\n"
     ]
    }
   ],
   "source": [
    "# use quadratic loss\n",
    "theta_0 = np.array([0.8,0.6])\n",
    "interp_loss = interp_quadratic_loss\n",
    "optim = GradientDescent(loss_fn=interp_loss, theta=theta_0, lr=0.1)\n",
    "optim_trajectory = []\n",
    "print(f'Start from theta_0: {theta_0}, Loss: {interp_loss(theta_0)}')\n",
    "optim_trajectory.append(DescentStep(theta_0.copy(), interp_loss(theta_0)))\n",
    "for i in range(10):\n",
    "    step = optim.step()\n",
    "    optim_trajectory.append(step)\n",
    "    print(f'Step {i}: theta: {step.theta} Loss: {step.value}')\n",
    "\n",
    "fig = plot_interp_loss_surface(interp_loss, shape, optim_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start from theta_0: [0.58 0.63], Loss: [0.09264963]\n",
      "Step 0: theta: [0.58327346 0.6281772 ] Loss: [0.09125477]\n",
      "Step 1: theta: [0.58651957 0.62640353] Loss: [0.08989509]\n",
      "Step 2: theta: [0.58973907 0.62467858] Loss: [0.08856937]\n",
      "Step 3: theta: [0.59293267 0.62300194] Loss: [0.08727638]\n",
      "Step 4: theta: [0.59610111 0.62137323] Loss: [0.08601495]\n",
      "Step 5: theta: [0.59924511 0.61979207] Loss: [0.0847517]\n",
      "Step 6: theta: [0.6025331  0.61793574] Loss: [0.08299507]\n",
      "Step 7: theta: [0.60634857 0.61501361] Loss: [0.08071798]\n",
      "Step 8: theta: [0.61007866 0.61220295] Loss: [0.07856727]\n",
      "Step 9: theta: [0.61372665 0.60950127] Loss: [0.07653538]\n",
      "Step 10: theta: [0.6172957  0.60690616] Loss: [0.07461517]\n",
      "Step 11: theta: [0.62078894 0.60441532] Loss: [0.07277625]\n",
      "Step 12: theta: [0.62450904 0.60194044] Loss: [0.07070625]\n",
      "Step 13: theta: [0.62842694 0.59916825] Loss: [0.06819665]\n",
      "Step 14: theta: [0.632586 0.59489 ] Loss: [0.06434913]\n",
      "Step 15: theta: [0.63718788 0.58993997] Loss: [0.05954535]\n",
      "Step 16: theta: [0.64230209 0.58451363] Loss: [0.05417639]\n",
      "Step 17: theta: [0.64590019 0.57886305] Loss: [0.04991906]\n",
      "Step 18: theta: [0.64936217 0.57482259] Loss: [0.04709428]\n",
      "Step 19: theta: [0.65280602 0.57079767] Loss: [0.04429449]\n",
      "Step 20: theta: [0.65623181 0.56678819] Loss: [0.04151946]\n",
      "Step 21: theta: [0.65963961 0.56279409] Loss: [0.03876897]\n",
      "Step 22: theta: [0.66338199 0.55881527] Loss: [0.03585509]\n",
      "Step 23: theta: [0.66747197 0.55604061] Loss: [0.03332318]\n",
      "Step 24: theta: [0.67178014 0.55294433] Loss: [0.03040356]\n",
      "Step 25: theta: [0.67633179 0.54950927] Loss: [0.02702891]\n",
      "Step 26: theta: [0.68115355 0.5457163 ] Loss: [0.02302704]\n",
      "Step 27: theta: [0.68709237 0.54161475] Loss: [0.01777532]\n",
      "Step 28: theta: [0.69310274 0.53740961] Loss: [0.01321793]\n",
      "Step 29: theta: [0.69892492 0.53644846] Loss: [0.00978251]\n",
      "Step 30: theta: [0.70466682 0.53597361] Loss: [0.00699493]\n",
      "Step 31: theta: [0.70927808 0.53552317] Loss: [0.00484536]\n",
      "Step 32: theta: [0.71389565 0.53500813] Loss: [0.00268331]\n",
      "Step 33: theta: [0.71852043 0.53442839] Loss: [0.00050708]\n",
      "Step 34: theta: [0.72315334 0.53378385] Loss: [-0.00068732]\n",
      "Step 35: theta: [0.7246313  0.53302782] Loss: [-0.00096613]\n",
      "Step 36: theta: [0.72613102 0.53222924] Loss: [-0.00125827]\n",
      "Step 37: theta: [0.72765373 0.5313875 ] Loss: [-0.00156468]\n",
      "Step 38: theta: [0.72920067 0.53050193] Loss: [-0.00188634]\n",
      "Step 39: theta: [0.73077309 0.52957184] Loss: [-0.00222431]\n",
      "Step 40: theta: [0.73237229 0.52859648] Loss: [-0.00257968]\n",
      "Step 41: theta: [0.73399956 0.5275751 ] Loss: [-0.00295359]\n",
      "Step 42: theta: [0.73565623 0.52650687] Loss: [-0.00334725]\n",
      "Step 43: theta: [0.73734365 0.52539096] Loss: [-0.00376193]\n",
      "Step 44: theta: [0.73906319 0.52422648] Loss: [-0.00419898]\n",
      "Step 45: theta: [0.74059801 0.52301251] Loss: [-0.00440469]\n",
      "Step 46: theta: [0.73877263 0.52183303] Loss: [-0.0044366]\n",
      "Step 47: theta: [0.74059458 0.52062742] Loss: [-0.00468672]\n",
      "Step 48: theta: [0.73852408 0.51944759] Loss: [-0.00450571]\n",
      "Step 49: theta: [0.7403972  0.51996403] Loss: [-0.00479596]\n",
      "Step 50: theta: [0.73826098 0.51887628] Loss: [-0.00434758]\n",
      "Step 51: theta: [0.74013245 0.52078144] Loss: [-0.0047635]\n",
      "Step 52: theta: [0.73807778 0.51955412] Loss: [-0.0044424]\n",
      "Step 53: theta: [0.73995121 0.51974639] Loss: [-0.0048301]\n",
      "Step 54: theta: [0.73800352 0.51930561] Loss: [-0.00438116]\n",
      "Step 55: theta: [0.73987624 0.52026635] Loss: [-0.00483152]\n",
      "Step 56: theta: [0.73826055 0.51902897] Loss: [-0.00437659]\n",
      "Step 57: theta: [0.74013246 0.52084444] Loss: [-0.00475576]\n",
      "Step 58: theta: [0.73808427 0.51961711] Loss: [-0.00445561]\n",
      "Step 59: theta: [0.73995787 0.51961461] Loss: [-0.00480618]\n",
      "Step 60: theta: [0.73797906 0.51958877] Loss: [-0.0044305]\n",
      "Step 61: theta: [0.73985258 0.51967551] Loss: [-0.00479808]\n",
      "Step 62: theta: [0.73829904 0.5194598 ] Loss: [-0.00446588]\n",
      "Step 63: theta: [0.7401722  0.51994095] Loss: [-0.00483961]\n",
      "Step 64: theta: [0.73803518 0.51890308] Loss: [-0.00431043]\n",
      "Step 65: theta: [0.73990673 0.5208076 ] Loss: [-0.0047702]\n",
      "Step 66: theta: [0.73821881 0.51956934] Loss: [-0.00447172]\n",
      "Step 67: theta: [0.74009228 0.51971243] Loss: [-0.00481296]\n",
      "Step 68: theta: [0.7379474  0.51938532] Loss: [-0.00438583]\n",
      "Step 69: theta: [0.73982034 0.52010044] Loss: [-0.00484159]\n",
      "Step 70: theta: [0.73841369 0.51886468] Loss: [-0.00437395]\n",
      "Step 71: theta: [0.74028513 0.52077028] Loss: [-0.00473348]\n",
      "Step 72: theta: [0.73822931 0.51955864] Loss: [-0.00447165]\n",
      "Step 73: theta: [0.74010275 0.51973469] Loss: [-0.00481497]\n",
      "Step 74: theta: [0.73795864 0.51933847] Loss: [-0.00437901]\n",
      "Step 75: theta: [0.73983144 0.52019809] Loss: [-0.0048316]\n",
      "Step 76: theta: [0.73838828 0.518962  ] Loss: [-0.00438774]\n",
      "Step 77: theta: [0.74026    0.52086753] Loss: [-0.00472684]\n",
      "Step 78: theta: [0.73821418 0.51965331] Loss: [-0.00448685]\n",
      "Step 79: theta: [0.74008789 0.51953662] Loss: [-0.00478027]\n"
     ]
    }
   ],
   "source": [
    "# use noise loss\n",
    "theta_0 = np.array([0.58,0.63])\n",
    "interp_loss = interp_noise_loss\n",
    "optim = GradientDescent(loss_fn=interp_loss, theta=theta_0, lr=0.01)\n",
    "optim_trajectory = []\n",
    "print(f'Start from theta_0: {theta_0}, Loss: {interp_loss(theta_0)}')\n",
    "optim_trajectory.append(DescentStep(theta_0.copy(), interp_loss(theta_0)))\n",
    "for i in range(80):\n",
    "    step = optim.step()\n",
    "    optim_trajectory.append(step)\n",
    "    print(f'Step {i}: theta: {step.theta} Loss: {step.value}')\n",
    "\n",
    "fig = plot_interp_loss_surface(interp_loss, shape, optim_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start from theta_0: [0.28 0.36], Loss: [0.06214212]\n",
      "Step 0: theta: [0.28153945 0.35826598] Loss: [0.06159741]\n",
      "Step 1: theta: [0.28309966 0.35649141] Loss: [0.06103576]\n",
      "Step 2: theta: [0.2846811  0.35469816] Loss: [0.0604607]\n",
      "Step 3: theta: [0.28628402 0.35288599] Loss: [0.05987189]\n",
      "Step 4: theta: [0.28790862 0.35105463] Loss: [0.05926901]\n",
      "Step 5: theta: [0.28955515 0.34920382] Loss: [0.05865171]\n",
      "Step 6: theta: [0.29122383 0.34733331] Loss: [0.05801964]\n",
      "Step 7: theta: [0.2929149  0.34544282] Loss: [0.05737244]\n",
      "Step 8: theta: [0.2946286  0.34353208] Loss: [0.05670976]\n",
      "Step 9: theta: [0.29636517 0.34160084] Loss: [0.0560312]\n",
      "Step 10: theta: [0.29812486 0.33964881] Loss: [0.05527856]\n",
      "Step 11: theta: [0.29990329 0.33709735] Loss: [0.05403922]\n",
      "Step 12: theta: [0.30457145 0.33347956] Loss: [0.05035463]\n",
      "Step 13: theta: [0.3097309  0.32963247] Loss: [0.04611303]\n",
      "Step 14: theta: [0.3150834  0.32552645] Loss: [0.04145187]\n",
      "Step 15: theta: [0.32064196 0.32115183] Loss: [0.03636552]\n",
      "Step 16: theta: [0.3258098  0.31652225] Loss: [0.03186731]\n",
      "Step 17: theta: [0.33093597 0.31282251] Loss: [0.02790152]\n",
      "Step 18: theta: [0.33600212 0.30920595] Loss: [0.02405671]\n",
      "Step 19: theta: [0.34100958 0.30567158] Loss: [0.02048524]\n",
      "Step 20: theta: [0.34440987 0.30224304] Loss: [0.01820086]\n",
      "Step 21: theta: [0.34767104 0.29895248] Loss: [0.01572946]\n",
      "Step 22: theta: [0.35088368 0.29227499] Loss: [0.01015143]\n",
      "Step 23: theta: [0.35436723 0.28546715] Loss: [0.00420704]\n",
      "Step 24: theta: [0.35812698 0.27851799] Loss: [-0.00187783]\n",
      "Step 25: theta: [0.36221834 0.2731962 ] Loss: [-0.00609295]\n",
      "Step 26: theta: [0.36466518 0.26791415] Loss: [-0.00937764]\n",
      "Step 27: theta: [0.36668692 0.26282903] Loss: [-0.01228949]\n",
      "Step 28: theta: [0.36829941 0.25790662] Loss: [-0.01399911]\n",
      "Step 29: theta: [0.3696643  0.25745871] Loss: [-0.01420489]\n",
      "Step 30: theta: [0.37102493 0.25702377] Loss: [-0.01440837]\n",
      "Step 31: theta: [0.37238143 0.25660178] Loss: [-0.01460965]\n",
      "Step 32: theta: [0.37373391 0.25619269] Loss: [-0.01480878]\n",
      "Step 33: theta: [0.3750825  0.25579646] Loss: [-0.01500584]\n",
      "Step 34: theta: [0.37642733 0.25541305] Loss: [-0.0152009]\n",
      "Step 35: theta: [0.3777685  0.25504244] Loss: [-0.01539404]\n",
      "Step 36: theta: [0.37910616 0.25468458] Loss: [-0.01558532]\n",
      "Step 37: theta: [0.38020116 0.25433944] Loss: [-0.01569784]\n",
      "Step 38: theta: [0.37928844 0.25399885] Loss: [-0.01563319]\n",
      "Step 39: theta: [0.37997187 0.25365545] Loss: [-0.0157355]\n",
      "Step 40: theta: [0.37913515 0.25331855] Loss: [-0.0156363]\n",
      "Step 41: theta: [0.3801572  0.25297369] Loss: [-0.01574825]\n",
      "Step 42: theta: [0.37927133 0.25263396] Loss: [-0.01567781]\n",
      "Step 43: theta: [0.3799908  0.25229039] Loss: [-0.01578397]\n",
      "Step 44: theta: [0.37913846 0.25195367] Loss: [-0.0156838]\n",
      "Step 45: theta: [0.38014572 0.25160884] Loss: [-0.01579561]\n",
      "Step 46: theta: [0.37928668 0.25126934] Loss: [-0.0157267]\n",
      "Step 47: theta: [0.3799709  0.25092592] Loss: [-0.01582734]\n",
      "Step 48: theta: [0.37918768 0.25058901] Loss: [-0.01573724]\n",
      "Step 49: theta: [0.38008242 0.25024465] Loss: [-0.01584719]\n",
      "Step 50: theta: [0.3792502  0.24990639] Loss: [-0.0157688]\n",
      "Step 51: theta: [0.38000999 0.24956262] Loss: [-0.01587619]\n",
      "Step 52: theta: [0.37919118 0.24922579] Loss: [-0.01578463]\n",
      "Step 53: theta: [0.38007308 0.24888146] Loss: [-0.01589405]\n",
      "Step 54: theta: [0.37926766 0.24854339] Loss: [-0.01581788]\n",
      "Step 55: theta: [0.37998821 0.24819979] Loss: [-0.01592139]\n",
      "Step 56: theta: [0.37922054 0.24786304] Loss: [-0.01583528]\n",
      "Step 57: theta: [0.38003676 0.24751899] Loss: [-0.01594294]\n",
      "Step 58: theta: [0.37925813 0.24718163] Loss: [-0.01586347]\n",
      "Step 59: theta: [0.37999576 0.24683794] Loss: [-0.0159682]\n",
      "Step 60: theta: [0.3792391  0.24650127] Loss: [-0.01588446]\n",
      "Step 61: theta: [0.38001372 0.24615739] Loss: [-0.01599061]\n",
      "Step 62: theta: [0.37926186 0.24582049] Loss: [-0.01591072]\n",
      "Step 63: theta: [0.37998935 0.24547683] Loss: [-0.01601322]\n",
      "Step 64: theta: [0.37927201 0.2451401 ] Loss: [-0.01593536]\n",
      "Step 65: theta: [0.37997817 0.24479654] Loss: [-0.01603474]\n",
      "Step 66: theta: [0.37929596 0.24445969] Loss: [-0.0159617]\n",
      "Step 67: theta: [0.37995422 0.24411636] Loss: [-0.0160547]\n",
      "Step 68: theta: [0.37933155 0.24377929] Loss: [-0.01598944]\n",
      "Step 69: theta: [0.37992032 0.2434363 ] Loss: [-0.01607346]\n",
      "Step 70: theta: [0.37937539 0.2430989 ] Loss: [-0.01601814]\n",
      "Step 71: theta: [0.37988028 0.24275633] Loss: [-0.01609152]\n",
      "Step 72: theta: [0.37942344 0.24241855] Loss: [-0.0160473]\n",
      "Step 73: theta: [0.37983821 0.24207643] Loss: [-0.01610937]\n",
      "Step 74: theta: [0.37947171 0.24173826] Loss: [-0.01607642]\n",
      "Step 75: theta: [0.37979786 0.2413966 ] Loss: [-0.01612749]\n",
      "Step 76: theta: [0.37951682 0.24105804] Loss: [-0.0161051]\n",
      "Step 77: theta: [0.37976212 0.24071681] Loss: [-0.01614621]\n",
      "Step 78: theta: [0.37955642 0.24037791] Loss: [-0.01613305]\n",
      "Step 79: theta: [0.37973278 0.24003706] Loss: [-0.01616574]\n",
      "Step 80: theta: [0.37958931 0.23969788] Loss: [-0.0162408]\n",
      "Step 81: theta: [0.37971653 0.23855085] Loss: [-0.01660091]\n",
      "Step 82: theta: [0.37965765 0.23554323] Loss: [-0.01749893]\n",
      "Step 83: theta: [0.37979882 0.23253453] Loss: [-0.01841909]\n",
      "Step 84: theta: [0.37986396 0.22952843] Loss: [-0.01932928]\n",
      "Step 85: theta: [0.37999587 0.22652352] Loss: [-0.02024471]\n",
      "Step 86: theta: [0.38017356 0.22352104] Loss: [-0.02115271]\n",
      "Step 87: theta: [0.38052747 0.22050827] Loss: [-0.02207928]\n",
      "Step 88: theta: [0.38106142 0.21747435] Loss: [-0.02393455]\n",
      "Step 89: theta: [0.38167443 0.21085849] Loss: [-0.02835691]\n",
      "Step 90: theta: [0.38241499 0.20423082] Loss: [-0.03281382]\n",
      "Step 91: theta: [0.38328332 0.19758886] Loss: [-0.03682966]\n",
      "Step 92: theta: [0.38417809 0.1929302 ] Loss: [-0.0390705]\n",
      "Step 93: theta: [0.38496635 0.188292  ] Loss: [-0.04127557]\n",
      "Step 94: theta: [0.38564858 0.18367181] Loss: [-0.04344952]\n",
      "Step 95: theta: [0.38622517 0.17906723] Loss: [-0.04531837]\n",
      "Step 96: theta: [0.38670356 0.17726133] Loss: [-0.0456298]\n",
      "Step 97: theta: [0.38715433 0.17566352] Loss: [-0.04590432]\n",
      "Step 98: theta: [0.38758067 0.17407259] Loss: [-0.04617457]\n",
      "Step 99: theta: [0.38798267 0.17248819] Loss: [-0.04644079]\n",
      "Step 100: theta: [0.38836045 0.17090993] Loss: [-0.04670324]\n",
      "Step 101: theta: [0.38871408 0.16933745] Loss: [-0.04696216]\n",
      "Step 102: theta: [0.38904367 0.16777038] Loss: [-0.0472178]\n",
      "Step 103: theta: [0.3893493  0.16620835] Loss: [-0.04747041]\n",
      "Step 104: theta: [0.38963103 0.16465099] Loss: [-0.04772021]\n",
      "Step 105: theta: [0.38988895 0.16309794] Loss: [-0.04796745]\n",
      "Step 106: theta: [0.39012312 0.16154884] Loss: [-0.04821235]\n",
      "Step 107: theta: [0.3903336  0.16000332] Loss: [-0.04845514]\n",
      "Step 108: theta: [0.39052044 0.15846101] Loss: [-0.04876055]\n",
      "Step 109: theta: [0.39070212 0.15650254] Loss: [-0.04914729]\n",
      "Step 110: theta: [0.39087728 0.15454468] Loss: [-0.04953357]\n",
      "Step 111: theta: [0.39104594 0.1525874 ] Loss: [-0.0499194]\n",
      "Step 112: theta: [0.39120808 0.15063067] Loss: [-0.0503048]\n",
      "Step 113: theta: [0.39136372 0.14867449] Loss: [-0.05068979]\n",
      "Step 114: theta: [0.39151286 0.14671882] Loss: [-0.05107438]\n",
      "Step 115: theta: [0.3916555  0.14476365] Loss: [-0.05145859]\n",
      "Step 116: theta: [0.39179163 0.14280896] Loss: [-0.05184244]\n",
      "Step 117: theta: [0.39192127 0.14085471] Loss: [-0.05222594]\n",
      "Step 118: theta: [0.39204441 0.1389009 ] Loss: [-0.05273989]\n",
      "Step 119: theta: [0.39216451 0.1357577 ] Loss: [-0.0537293]\n",
      "Step 120: theta: [0.39228408 0.13261452] Loss: [-0.05471868]\n",
      "Step 121: theta: [0.39240311 0.12947136] Loss: [-0.05570803]\n",
      "Step 122: theta: [0.39252159 0.12632822] Loss: [-0.05669737]\n",
      "Step 123: theta: [0.39263954 0.1231851 ] Loss: [-0.05768667]\n",
      "Step 124: theta: [0.39275694 0.120042  ] Loss: [-0.05867595]\n",
      "Step 125: theta: [0.39287381 0.11689892] Loss: [-0.05916245]\n",
      "Step 126: theta: [0.39310022 0.11537707] Loss: [-0.0594004]\n",
      "Step 127: theta: [0.3933804  0.11384723] Loss: [-0.0596438]\n",
      "Step 128: theta: [0.39371462 0.11230749] Loss: [-0.05989387]\n",
      "Step 129: theta: [0.39410324 0.11075595] Loss: [-0.06015183]\n",
      "Step 130: theta: [0.39454667 0.10919067] Loss: [-0.06041896]\n",
      "Step 131: theta: [0.3950454  0.10760973] Loss: [-0.06069655]\n",
      "Step 132: theta: [0.39559998 0.10601117] Loss: [-0.06098598]\n",
      "Step 133: theta: [0.39621104 0.10439301] Loss: [-0.06128865]\n",
      "Step 134: theta: [0.39687926 0.10275327] Loss: [-0.06160605]\n",
      "Step 135: theta: [0.39760541 0.10108993] Loss: [-0.06193972]\n",
      "Step 136: theta: [0.39839033 0.09940092] Loss: [-0.06223399]\n",
      "Step 137: theta: [0.39920903 0.09825715] Loss: [-0.06238725]\n",
      "Step 138: theta: [0.39991172 0.09750326] Loss: [-0.06250056]\n",
      "Step 139: theta: [0.4003192  0.09675492] Loss: [-0.06257764]\n",
      "Step 140: theta: [0.40075907 0.09597708] Loss: [-0.06266073]\n",
      "Step 141: theta: [0.40127249 0.09515766] Loss: [-0.06275821]\n",
      "Step 142: theta: [0.40186339 0.09428968] Loss: [-0.06287332]\n",
      "Step 143: theta: [0.40253637 0.09336584] Loss: [-0.06300984]\n",
      "Step 144: theta: [0.4032967  0.09237836] Loss: [-0.06317226]\n",
      "Step 145: theta: [0.4041504  0.09131899] Loss: [-0.06336592]\n",
      "Step 146: theta: [0.40510427 0.09017889] Loss: [-0.06359717]\n",
      "Step 147: theta: [0.40616594 0.0889486 ] Loss: [-0.06387359]\n",
      "Step 148: theta: [0.40734395 0.08761792] Loss: [-0.06420426]\n",
      "Step 149: theta: [0.40864778 0.08617586] Loss: [-0.06459999]\n",
      "Step 150: theta: [0.41008796 0.08461051] Loss: [-0.06507375]\n",
      "Step 151: theta: [0.41167616 0.08290898] Loss: [-0.06564106]\n",
      "Step 152: theta: [0.41342524 0.08105728] Loss: [-0.06632049]\n",
      "Step 153: theta: [0.41534942 0.0790402 ] Loss: [-0.06715788]\n",
      "Step 154: theta: [0.41726328 0.07660533] Loss: [-0.06806588]\n",
      "Step 155: theta: [0.41889736 0.0743805 ] Loss: [-0.06878612]\n",
      "Step 156: theta: [0.42027579 0.07234344] Loss: [-0.06924042]\n",
      "Step 157: theta: [0.41712687 0.07045507] Loss: [-0.06942156]\n",
      "Step 158: theta: [0.41805424 0.06821457] Loss: [-0.06998567]\n",
      "Step 159: theta: [0.41872418 0.06608063] Loss: [-0.0704695]\n",
      "Step 160: theta: [0.4191489  0.06402367] Loss: [-0.07090061]\n",
      "Step 161: theta: [0.41874155 0.06201551] Loss: [-0.0713056]\n",
      "Step 162: theta: [0.41869917 0.05996054] Loss: [-0.07173335]\n",
      "Step 163: theta: [0.41842215 0.05785799] Loss: [-0.07240624]\n",
      "Step 164: theta: [0.41798282 0.0546941 ] Loss: [-0.07343729]\n",
      "Step 165: theta: [0.41729923 0.05149631] Loss: [-0.07452348]\n",
      "Step 166: theta: [0.41636879 0.04824574] Loss: [-0.07569002]\n",
      "Step 167: theta: [0.4151874  0.04492334] Loss: [-0.07696373]\n",
      "Step 168: theta: [0.41374953 0.04150974] Loss: [-0.07837363]\n",
      "Step 169: theta: [0.41204813 0.03798514] Loss: [-0.07996526]\n",
      "Step 170: theta: [0.41036412 0.03426182] Loss: [-0.08159347]\n",
      "Step 171: theta: [0.40892765 0.03065045] Loss: [-0.08306953]\n",
      "Step 172: theta: [0.40773125 0.02713458] Loss: [-0.08442084]\n",
      "Step 173: theta: [0.40676858 0.02369824] Loss: [-0.08567237]\n",
      "Step 174: theta: [0.40603436 0.02032589] Loss: [-0.08684708]\n",
      "Step 175: theta: [0.40552434 0.01700236] Loss: [-0.08766504]\n",
      "Step 176: theta: [0.40507344 0.01471811] Loss: [-0.08820587]\n",
      "Step 177: theta: [0.4046511  0.01243948] Loss: [-0.08874171]\n",
      "Step 178: theta: [0.40425724 0.01016614] Loss: [-0.08927292]\n",
      "Step 179: theta: [0.40389179 0.00789772] Loss: [-0.08979981]\n",
      "Step 180: theta: [0.4035547  0.00563387] Loss: [-0.09032272]\n",
      "Step 181: theta: [0.40324591 0.00337423] Loss: [-0.09084198]\n",
      "Step 182: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 183: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 184: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 185: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 186: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 187: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 188: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 189: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 190: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 191: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 192: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 193: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 194: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 195: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 196: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 197: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 198: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 199: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n"
     ]
    }
   ],
   "source": [
    "# use noise loss\n",
    "theta_0 = np.array([0.28,0.36])\n",
    "interp_loss = interp_noise_loss\n",
    "optim = GradientDescent(loss_fn=interp_loss, theta=theta_0, lr=0.01)\n",
    "optim_trajectory = []\n",
    "print(f'Start from theta_0: {theta_0}, Loss: {interp_loss(theta_0)}')\n",
    "optim_trajectory.append(DescentStep(theta_0.copy(), interp_loss(theta_0)))\n",
    "for i in range(200):\n",
    "    step = optim.step()\n",
    "    optim_trajectory.append(step)\n",
    "    print(f'Step {i}: theta: {step.theta} Loss: {step.value}')\n",
    "\n",
    "fig = plot_interp_loss_surface(interp_loss, shape, optim_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start from theta_0: [0.28 0.36], Loss: [0.06214212]\n",
      "Step 0: theta: [0.27890246 0.35562898] Loss: [0.06105055]\n",
      "Step 1: theta: [0.2781087  0.35607166] Loss: [0.06089087]\n",
      "Step 2: theta: [0.27730642 0.35655191] Loss: [0.06073226]\n",
      "Step 3: theta: [0.26604639 0.34662224] Loss: [0.05614274]\n",
      "Step 4: theta: [0.26869973 0.35065553] Loss: [0.05737518]\n",
      "Step 5: theta: [0.26283677 0.34622398] Loss: [0.0552587]\n",
      "Step 6: theta: [0.26657336 0.35144534] Loss: [0.05688615]\n",
      "Step 7: theta: [0.26858368 0.35499579] Loss: [0.05792055]\n",
      "Step 8: theta: [0.2695422  0.35755185] Loss: [0.05855043]\n",
      "Step 9: theta: [0.27507622 0.36474299] Loss: [0.06146881]\n",
      "Step 10: theta: [0.27408742 0.36497824] Loss: [0.06121148]\n",
      "Step 11: theta: [0.27128405 0.36337908] Loss: [0.06006106]\n",
      "Step 12: theta: [0.27044858 0.36372833] Loss: [0.05987138]\n",
      "Step 13: theta: [0.26967998 0.36412526] Loss: [0.05971209]\n",
      "Step 14: theta: [0.27225694 0.36784887] Loss: [0.06118988]\n",
      "Step 15: theta: [0.26903563 0.36575565] Loss: [0.05982737]\n",
      "Step 16: theta: [0.26644068 0.36427051] Loss: [0.05875319]\n",
      "Step 17: theta: [0.25950315 0.35842482] Loss: [0.05553754]\n",
      "Step 18: theta: [0.2515731  0.35322746] Loss: [0.05195043]\n",
      "Step 19: theta: [0.25553631 0.36081686] Loss: [0.05405896]\n",
      "Step 20: theta: [0.25009399 0.35806412] Loss: [0.05143928]\n",
      "Step 21: theta: [0.23907316 0.35122904] Loss: [0.04755508]\n",
      "Step 22: theta: [0.23703002 0.35061629] Loss: [0.04748998]\n",
      "Step 23: theta: [0.23092432 0.34561571] Loss: [0.04726322]\n",
      "Step 24: theta: [0.22232036 0.33804769] Loss: [0.04604024]\n",
      "Step 25: theta: [0.23389044 0.3492211 ] Loss: [0.04737657]\n",
      "Step 26: theta: [0.23426153 0.35058809] Loss: [0.04734449]\n",
      "Step 27: theta: [0.23662267 0.35388277] Loss: [0.04728875]\n",
      "Step 28: theta: [0.23183334 0.34996852] Loss: [0.04723096]\n",
      "Step 29: theta: [0.22838935 0.34734482] Loss: [0.04704363]\n",
      "Step 30: theta: [0.23497822 0.35470262] Loss: [0.04719864]\n",
      "Step 31: theta: [0.23349227 0.35393744] Loss: [0.04718505]\n",
      "Step 32: theta: [0.22817203 0.34929284] Loss: [0.04702361]\n",
      "Step 33: theta: [0.22547772 0.34723186] Loss: [0.04682811]\n",
      "Step 34: theta: [0.22131917 0.34366699] Loss: [0.04637421]\n",
      "Step 35: theta: [0.21550032 0.33840465] Loss: [0.04619465]\n",
      "Step 36: theta: [0.21313285 0.33362901] Loss: [0.04561288]\n",
      "Step 37: theta: [0.20867182 0.32679846] Loss: [0.04478596]\n",
      "Step 38: theta: [0.20885057 0.3246457 ] Loss: [0.04440996]\n",
      "Step 39: theta: [0.21146597 0.32496699] Loss: [0.04430606]\n",
      "Step 40: theta: [0.21025984 0.32150356] Loss: [0.04380024]\n",
      "Step 41: theta: [0.21968823 0.32871086] Loss: [0.04437737]\n",
      "Step 42: theta: [0.2150689 0.3230478] Loss: [0.043782]\n",
      "Step 43: theta: [0.21531988 0.32113008] Loss: [0.04346264]\n",
      "Step 44: theta: [0.2127435  0.31641978] Loss: [0.04298294]\n",
      "Step 45: theta: [0.20717534 0.3087404 ] Loss: [0.0429883]\n",
      "Step 46: theta: [0.21188813 0.3111242 ] Loss: [0.04246213]\n",
      "Step 47: theta: [0.21680265 0.31346952] Loss: [0.04213731]\n",
      "Step 48: theta: [0.20703971 0.30087237] Loss: [0.04251711]\n",
      "Step 49: theta: [0.21102274 0.30172887] Loss: [0.04161089]\n",
      "Step 50: theta: [0.22478717 0.3120443 ] Loss: [0.04143711]\n",
      "Step 51: theta: [0.22024469 0.30490964] Loss: [0.04005013]\n",
      "Step 52: theta: [0.22157811 0.30335269] Loss: [0.03959629]\n",
      "Step 53: theta: [0.22675106 0.30530275] Loss: [0.03957395]\n",
      "Step 54: theta: [0.23047275 0.30543081] Loss: [0.03929734]\n",
      "Step 55: theta: [0.23681888 0.3077699 ] Loss: [0.03967796]\n",
      "Step 56: theta: [0.24655405 0.31303707] Loss: [0.04239515]\n",
      "Step 57: theta: [0.2422117  0.30517086] Loss: [0.03854136]\n",
      "Step 58: theta: [0.24233386 0.30163359] Loss: [0.03700923]\n",
      "Step 59: theta: [0.23868107 0.29418058] Loss: [0.03444561]\n",
      "Step 60: theta: [0.24206015 0.29262684] Loss: [0.03368848]\n",
      "Step 61: theta: [0.2445227 0.291456 ] Loss: [0.03322484]\n",
      "Step 62: theta: [0.23877837 0.28178015] Loss: [0.03031383]\n",
      "Step 63: theta: [0.23914269 0.27688131] Loss: [0.02893343]\n",
      "Step 64: theta: [0.23519086 0.26875379] Loss: [0.02781334]\n",
      "Step 65: theta: [0.22910464 0.2584406 ] Loss: [0.02628842]\n",
      "Step 66: theta: [0.233552   0.25718784] Loss: [0.02492369]\n",
      "Step 67: theta: [0.23325663 0.25081557] Loss: [0.02240011]\n",
      "Step 68: theta: [0.22847754 0.23955785] Loss: [0.0192411]\n",
      "Step 69: theta: [0.23340674 0.23651754] Loss: [0.01586097]\n",
      "Step 70: theta: [0.23421221 0.22826373] Loss: [0.01064782]\n",
      "Step 71: theta: [0.24320213 0.2285461 ] Loss: [0.00882144]\n",
      "Step 72: theta: [0.24846383 0.2269683 ] Loss: [0.00732366]\n",
      "Step 73: theta: [0.25168324 0.22403339] Loss: [0.00557781]\n",
      "Step 74: theta: [0.25670394 0.22351629] Loss: [0.00490757]\n",
      "Step 75: theta: [0.25556701 0.21739631] Loss: [0.0026737]\n",
      "Step 76: theta: [0.25871925 0.2171086 ] Loss: [0.00252496]\n",
      "Step 77: theta: [0.25641901 0.21175991] Loss: [0.00088981]\n",
      "Step 78: theta: [0.2538062  0.20644561] Loss: [-0.00104612]\n",
      "Step 79: theta: [0.25587328 0.20611866] Loss: [-0.00093632]\n",
      "Step 80: theta: [0.2575523  0.20567612] Loss: [-0.00088898]\n",
      "Step 81: theta: [0.25576942 0.20201313] Loss: [-0.00226419]\n",
      "Step 82: theta: [0.25403256 0.19861014] Loss: [-0.00355602]\n",
      "Step 83: theta: [0.25163672 0.1955619 ] Loss: [-0.00473428]\n",
      "Step 84: theta: [0.25634162 0.19962128] Loss: [-0.00289992]\n",
      "Step 85: theta: [0.26602048 0.20820842] Loss: [0.0002401]\n",
      "Step 86: theta: [0.26885364 0.20854531] Loss: [0.00041107]\n",
      "Step 87: theta: [0.27107528 0.20825304] Loss: [0.00039006]\n",
      "Step 88: theta: [0.27515393 0.20980004] Loss: [0.00093931]\n",
      "Step 89: theta: [0.26981704 0.20191361] Loss: [-0.00141606]\n",
      "Step 90: theta: [0.27163267 0.20116171] Loss: [-0.00158589]\n",
      "Step 91: theta: [0.26680436 0.19374772] Loss: [-0.00339951]\n",
      "Step 92: theta: [0.27429549 0.19938637] Loss: [-0.00198011]\n",
      "Step 93: theta: [0.2731602  0.19613726] Loss: [-0.00267666]\n",
      "Step 94: theta: [0.26599596 0.18721403] Loss: [-0.0048876]\n",
      "Step 95: theta: [0.25908053 0.17858107] Loss: [-0.00763848]\n",
      "Step 96: theta: [0.25776057 0.17522325] Loss: [-0.0091655]\n",
      "Step 97: theta: [0.25860168 0.17403294] Loss: [-0.00947344]\n",
      "Step 98: theta: [0.25215763 0.16546449] Loss: [-0.01360567]\n",
      "Step 99: theta: [0.25554282 0.16662805] Loss: [-0.01273044]\n",
      "Step 100: theta: [0.2624249  0.17118683] Loss: [-0.010188]\n",
      "Step 101: theta: [0.25608617 0.16158106] Loss: [-0.01453232]\n",
      "Step 102: theta: [0.25358001 0.15649573] Loss: [-0.01646609]\n",
      "Step 103: theta: [0.2497553  0.15044222] Loss: [-0.01851178]\n",
      "Step 104: theta: [0.25769522 0.15601574] Loss: [-0.01627635]\n",
      "Step 105: theta: [0.25169861 0.14750666] Loss: [-0.0192889]\n",
      "Step 106: theta: [0.24846217 0.14160263] Loss: [-0.02101538]\n",
      "Step 107: theta: [0.25436232 0.14467051] Loss: [-0.02009719]\n",
      "Step 108: theta: [0.25443306 0.14173415] Loss: [-0.02101578]\n",
      "Step 109: theta: [0.24694109 0.13104943] Loss: [-0.02322774]\n",
      "Step 110: theta: [0.2523423  0.13420569] Loss: [-0.02271043]\n",
      "Step 111: theta: [0.24905194 0.12865265] Loss: [-0.02376296]\n",
      "Step 112: theta: [0.24850769 0.12582787] Loss: [-0.02431898]\n",
      "Step 113: theta: [0.24209625 0.1171179 ] Loss: [-0.02685126]\n",
      "Step 114: theta: [0.23597854 0.10525311] Loss: [-0.03411793]\n",
      "Step 115: theta: [0.23040192 0.09641362] Loss: [-0.0408882]\n",
      "Step 116: theta: [0.23099389 0.09136994] Loss: [-0.04458167]\n",
      "Step 117: theta: [0.24102952 0.09521917] Loss: [-0.04015312]\n",
      "Step 118: theta: [0.24280346 0.08825925] Loss: [-0.04608835]\n",
      "Step 119: theta: [0.24781003 0.08443606] Loss: [-0.04953002]\n",
      "Step 120: theta: [0.24526434 0.07296367] Loss: [-0.0569774]\n",
      "Step 121: theta: [0.2515051 0.0731343] Loss: [-0.05730555]\n",
      "Step 122: theta: [0.25324119 0.06862044] Loss: [-0.05996289]\n",
      "Step 123: theta: [0.25312522 0.06206939] Loss: [-0.0636383]\n",
      "Step 124: theta: [0.25160076 0.05391922] Loss: [-0.06631092]\n",
      "Step 125: theta: [0.2412962  0.03983664] Loss: [-0.06923231]\n",
      "Step 126: theta: [0.24019841 0.03459948] Loss: [-0.07187861]\n",
      "Step 127: theta: [0.23519117 0.02370046] Loss: [-0.07808358]\n",
      "Step 128: theta: [0.23594276 0.01990325] Loss: [-0.08010926]\n",
      "Step 129: theta: [0.2362287  0.01579196] Loss: [-0.08279952]\n",
      "Step 130: theta: [0.23825188 0.01215647] Loss: [-0.08505683]\n",
      "Step 131: theta: [0.23360875 0.00151759] Loss: [-0.09229183]\n",
      "Step 132: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 133: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 134: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 135: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 136: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 137: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 138: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 139: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 140: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 141: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 142: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 143: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 144: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 145: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 146: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 147: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 148: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 149: theta: [2.38506123e-01 6.20605930e-05] Loss: [-0.09321799]\n"
     ]
    }
   ],
   "source": [
    "# use noise loss + noise gradient steps\n",
    "theta_0 = np.array([0.28,0.36])\n",
    "interp_loss = interp_noise_loss\n",
    "optim = GradientDescent(loss_fn=interp_loss, theta=theta_0, lr=0.01, gradient_noise_std=0.5)\n",
    "optim_trajectory = []\n",
    "print(f'Start from theta_0: {theta_0}, Loss: {interp_loss(theta_0)}')\n",
    "optim_trajectory.append(DescentStep(theta_0.copy(), interp_loss(theta_0)))\n",
    "for i in range(150):\n",
    "    step = optim.noisy_step()\n",
    "    optim_trajectory.append(step)\n",
    "    print(f'Step {i}: theta: {step.theta} Loss: {step.value}')\n",
    "\n",
    "fig = plot_interp_loss_surface(interp_loss, shape, optim_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "efb944956e68c5a4a3f866131a290f604672ac43206fde6373334f1e4d6c02e5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
