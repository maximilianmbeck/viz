{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import copy\n",
    "from IPython.core.display import HTML, display\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import noise\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "%matplotlib qt \n",
    "# for this run pip install pyqt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_loss_surface(shape, scale, octaves, persistence, lacunarity):\n",
    "    surface = np.zeros(shape)\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            surface[i][j] = noise.pnoise2(i/scale, \n",
    "                                        j/scale, \n",
    "                                        octaves=octaves, \n",
    "                                        persistence=persistence, \n",
    "                                        lacunarity=lacunarity, \n",
    "                                        repeatx=1024, \n",
    "                                        repeaty=1024, \n",
    "                                        base=42)\n",
    "    return surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate grid interpolation\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "\n",
    "def interpolate_loss_surface(surface, shape):\n",
    "    lin_x = np.linspace(0, 1, shape[0], endpoint=False)\n",
    "    lin_y = np.linspace(0, 1, shape[1], endpoint=False)\n",
    "    interp_loss_surface = RegularGridInterpolator((lin_x, lin_y), surface)\n",
    "    return interp_loss_surface\n",
    "\n",
    "\n",
    "def plot_interp_loss_surface(interp_surface, shape, optim_trajectory=None):\n",
    "\n",
    "    lin_x = np.linspace(0, 1, shape[0], endpoint=False)\n",
    "    lin_y = np.linspace(0, 1, shape[1], endpoint=False)\n",
    "    x, y = np.meshgrid(lin_x, lin_y)\n",
    "    xy = np.stack([x, y], axis=2)\n",
    "    z = interp_surface(xy)\n",
    "    fig = plt.figure()\n",
    "    # contour plot\n",
    "    ax0 = plt.subplot(1, 2, 1)\n",
    "    ax0.contourf(x, y, z, cmap='terrain')\n",
    "    # 3d plot\n",
    "    ax1 = plt.subplot(1, 2, 2, projection='3d')\n",
    "    ax1.plot_surface(x, y, z, alpha=0.6, cmap='terrain')\n",
    "    if optim_trajectory:\n",
    "        thetas = np.stack([x.theta for x in optim_trajectory])\n",
    "        ax0.plot(thetas[:, 0], thetas[:, 1], 'o-', lw=3, c='r')\n",
    "        loss_values = np.array([x.value for x in optim_trajectory]) #+ 0.02 # add offset such that it is plot on top\n",
    "        ax1.plot3D(thetas[:, 0],\n",
    "                   thetas[:, 1],\n",
    "                   loss_values[:, 0],\n",
    "                   'o-',\n",
    "                   lw=3,\n",
    "                   c='r')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (50, 50)\n",
    "scale = 100.0\n",
    "octaves = 6\n",
    "persistence = 0.5\n",
    "lacunarity = 2.0\n",
    "surface = generate_loss_surface(shape, scale, octaves, persistence, lacunarity)\n",
    "interp_noise_loss = interpolate_loss_surface(surface, shape)\n",
    "# fig = plot_interp_loss_surface(interp_noise_loss, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize quadratic loss for testing\n",
    "def quadratic_loss(x):\n",
    "    assert len(x) == 2\n",
    "    z = x[0]**2 + x[1]**2\n",
    "    return z\n",
    "lin_x = np.linspace(0,1,shape[0],endpoint=False)\n",
    "lin_y = np.linspace(0,1,shape[1],endpoint=False)\n",
    "x,y = np.meshgrid(lin_x, lin_y, sparse=False)\n",
    "z = x**2+y**2\n",
    "interp_quadratic_loss = interpolate_loss_surface(z, shape)\n",
    "# fig = plot_interp_loss_surface(interp_quadratic_loss, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, NamedTuple, List\n",
    "\n",
    "class DescentStep(NamedTuple):\n",
    "    theta: np.ndarray\n",
    "    value: float\n",
    "\n",
    "class GradientDescent(object):\n",
    "    def __init__(self, loss_fn: Callable[[np.ndarray], np.ndarray], \n",
    "                    theta: np.ndarray, lr: float, gradient_noise_std: float=0.1, fd_h: float = 1e-3):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.theta = theta\n",
    "        self.lr = lr # stepsize\n",
    "        self.finite_difference_h = fd_h\n",
    "        self.gradient_noise_std = gradient_noise_std\n",
    "        self.rng = np.random.default_rng()\n",
    "\n",
    "        self.at_boundary = False\n",
    "        self.last_step = DescentStep(theta_0, loss_fn(theta_0))\n",
    "\n",
    "    def _compute_gradient(self):\n",
    "        # forward finite difference gradient\n",
    "        h = self.finite_difference_h\n",
    "        f = self.loss_fn\n",
    "        x = self.theta\n",
    "        gradient = np.zeros_like(x)\n",
    "        try:\n",
    "            for i in range(len(gradient)):\n",
    "                h_vec = np.zeros_like(x)\n",
    "                h_vec[i] = h\n",
    "                gradient[i] = (f(x+h_vec) - f(x))/(h)\n",
    "        except ValueError:\n",
    "            print(f'Try to compute gradient at function support boundary at {str(x)}. Setting gradient to zero!')\n",
    "            gradient = np.zeros_like(x)\n",
    "        return gradient\n",
    "\n",
    "    def _safe_decent_step_creation(self, theta):\n",
    "        try:\n",
    "            loss = self.loss_fn(theta)\n",
    "            return DescentStep(theta.copy(), loss)\n",
    "        except ValueError:\n",
    "            print('Reached function support boundary!')\n",
    "            self.at_boundary = True\n",
    "            return None \n",
    "\n",
    "    def _get_descent_step(self, theta):\n",
    "        descent_step = self._safe_decent_step_creation(self.theta)\n",
    "        if descent_step is None or self.at_boundary:\n",
    "            print('At function support boundary, using last step.')\n",
    "            return copy.deepcopy(self.last_step)\n",
    "        else:\n",
    "            self.last_step = descent_step\n",
    "        return descent_step\n",
    "\n",
    "    def step(self):\n",
    "        if not self.at_boundary:\n",
    "            grad = self._compute_gradient()\n",
    "            self.theta = self.theta - self.lr * grad\n",
    "        return self._get_descent_step(self.theta)\n",
    "\n",
    "        \n",
    "    def noisy_step(self):\n",
    "        if not self.at_boundary:\n",
    "            grad_noise = self.rng.normal(loc=0, scale=self.gradient_noise_std)\n",
    "            grad = self._compute_gradient() + grad_noise\n",
    "            self.theta = self.theta - self.lr * grad\n",
    "        return self._get_descent_step(self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start from theta_0: [0.8 0.6], Loss: [1.]\n",
      "Step 0: theta: [0.638 0.478] Loss: [0.6356]\n",
      "Step 1: theta: [0.512 0.384] Loss: [0.40976]\n",
      "Step 2: theta: [0.41  0.306] Loss: [0.26192]\n",
      "Step 3: theta: [0.328 0.244] Loss: [0.16728]\n",
      "Step 4: theta: [0.262 0.194] Loss: [0.1064]\n",
      "Step 5: theta: [0.208 0.156] Loss: [0.06776]\n",
      "Step 6: theta: [0.166 0.126] Loss: [0.0436]\n",
      "Step 7: theta: [0.132 0.1  ] Loss: [0.02752]\n",
      "Step 8: theta: [0.106 0.078] Loss: [0.01744]\n",
      "Step 9: theta: [0.084 0.064] Loss: [0.01128]\n"
     ]
    }
   ],
   "source": [
    "# use quadratic loss\n",
    "theta_0 = np.array([0.8,0.6])\n",
    "interp_loss = interp_quadratic_loss\n",
    "optim = GradientDescent(loss_fn=interp_loss, theta=theta_0, lr=0.1)\n",
    "optim_trajectory = []\n",
    "print(f'Start from theta_0: {theta_0}, Loss: {interp_loss(theta_0)}')\n",
    "optim_trajectory.append(DescentStep(theta_0.copy(), interp_loss(theta_0)))\n",
    "for i in range(10):\n",
    "    step = optim.step()\n",
    "    optim_trajectory.append(step)\n",
    "    print(f'Step {i}: theta: {step.theta} Loss: {step.value}')\n",
    "\n",
    "fig = plot_interp_loss_surface(interp_loss, shape, optim_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start from theta_0: [0.58 0.63], Loss: [0.09264963]\n",
      "Step 0: theta: [0.58327346 0.6281772 ] Loss: [0.09125477]\n",
      "Step 1: theta: [0.58651957 0.62640353] Loss: [0.08989509]\n",
      "Step 2: theta: [0.58973907 0.62467858] Loss: [0.08856937]\n",
      "Step 3: theta: [0.59293267 0.62300194] Loss: [0.08727638]\n",
      "Step 4: theta: [0.59610111 0.62137323] Loss: [0.08601495]\n",
      "Step 5: theta: [0.59924511 0.61979207] Loss: [0.0847517]\n",
      "Step 6: theta: [0.6025331  0.61793574] Loss: [0.08299507]\n",
      "Step 7: theta: [0.60634857 0.61501361] Loss: [0.08071798]\n",
      "Step 8: theta: [0.61007866 0.61220295] Loss: [0.07856727]\n",
      "Step 9: theta: [0.61372665 0.60950127] Loss: [0.07653538]\n",
      "Step 10: theta: [0.6172957  0.60690616] Loss: [0.07461517]\n",
      "Step 11: theta: [0.62078894 0.60441532] Loss: [0.07277625]\n",
      "Step 12: theta: [0.62450904 0.60194044] Loss: [0.07070625]\n",
      "Step 13: theta: [0.62842694 0.59916825] Loss: [0.06819665]\n",
      "Step 14: theta: [0.632586 0.59489 ] Loss: [0.06434913]\n",
      "Step 15: theta: [0.63718788 0.58993997] Loss: [0.05954535]\n",
      "Step 16: theta: [0.64230209 0.58451363] Loss: [0.05417639]\n",
      "Step 17: theta: [0.64590019 0.57886305] Loss: [0.04991906]\n",
      "Step 18: theta: [0.64936217 0.57482259] Loss: [0.04709428]\n",
      "Step 19: theta: [0.65280602 0.57079767] Loss: [0.04429449]\n",
      "Step 20: theta: [0.65623181 0.56678819] Loss: [0.04151946]\n",
      "Step 21: theta: [0.65963961 0.56279409] Loss: [0.03876897]\n",
      "Step 22: theta: [0.66338199 0.55881527] Loss: [0.03585509]\n",
      "Step 23: theta: [0.66747197 0.55604061] Loss: [0.03332318]\n",
      "Step 24: theta: [0.67178014 0.55294433] Loss: [0.03040356]\n",
      "Step 25: theta: [0.67633179 0.54950927] Loss: [0.02702891]\n",
      "Step 26: theta: [0.68115355 0.5457163 ] Loss: [0.02302704]\n",
      "Step 27: theta: [0.68709237 0.54161475] Loss: [0.01777532]\n",
      "Step 28: theta: [0.69310274 0.53740961] Loss: [0.01321793]\n",
      "Step 29: theta: [0.69892492 0.53644846] Loss: [0.00978251]\n",
      "Step 30: theta: [0.70466682 0.53597361] Loss: [0.00699493]\n",
      "Step 31: theta: [0.70927808 0.53552317] Loss: [0.00484536]\n",
      "Step 32: theta: [0.71389565 0.53500813] Loss: [0.00268331]\n",
      "Step 33: theta: [0.71852043 0.53442839] Loss: [0.00050708]\n",
      "Step 34: theta: [0.72315334 0.53378385] Loss: [-0.00068732]\n",
      "Step 35: theta: [0.7246313  0.53302782] Loss: [-0.00096613]\n",
      "Step 36: theta: [0.72613102 0.53222924] Loss: [-0.00125827]\n",
      "Step 37: theta: [0.72765373 0.5313875 ] Loss: [-0.00156468]\n",
      "Step 38: theta: [0.72920067 0.53050193] Loss: [-0.00188634]\n",
      "Step 39: theta: [0.73077309 0.52957184] Loss: [-0.00222431]\n",
      "Step 40: theta: [0.73237229 0.52859648] Loss: [-0.00257968]\n",
      "Step 41: theta: [0.73399956 0.5275751 ] Loss: [-0.00295359]\n",
      "Step 42: theta: [0.73565623 0.52650687] Loss: [-0.00334725]\n",
      "Step 43: theta: [0.73734365 0.52539096] Loss: [-0.00376193]\n",
      "Step 44: theta: [0.73906319 0.52422648] Loss: [-0.00419898]\n",
      "Step 45: theta: [0.74059801 0.52301251] Loss: [-0.00440469]\n",
      "Step 46: theta: [0.73877263 0.52183303] Loss: [-0.0044366]\n",
      "Step 47: theta: [0.74059458 0.52062742] Loss: [-0.00468672]\n",
      "Step 48: theta: [0.73852408 0.51944759] Loss: [-0.00450571]\n",
      "Step 49: theta: [0.7403972  0.51996403] Loss: [-0.00479596]\n",
      "Step 50: theta: [0.73826098 0.51887628] Loss: [-0.00434758]\n",
      "Step 51: theta: [0.74013245 0.52078144] Loss: [-0.0047635]\n",
      "Step 52: theta: [0.73807778 0.51955412] Loss: [-0.0044424]\n",
      "Step 53: theta: [0.73995121 0.51974639] Loss: [-0.0048301]\n",
      "Step 54: theta: [0.73800352 0.51930561] Loss: [-0.00438116]\n",
      "Step 55: theta: [0.73987624 0.52026635] Loss: [-0.00483152]\n",
      "Step 56: theta: [0.73826055 0.51902897] Loss: [-0.00437659]\n",
      "Step 57: theta: [0.74013246 0.52084444] Loss: [-0.00475576]\n",
      "Step 58: theta: [0.73808427 0.51961711] Loss: [-0.00445561]\n",
      "Step 59: theta: [0.73995787 0.51961461] Loss: [-0.00480618]\n",
      "Step 60: theta: [0.73797906 0.51958877] Loss: [-0.0044305]\n",
      "Step 61: theta: [0.73985258 0.51967551] Loss: [-0.00479808]\n",
      "Step 62: theta: [0.73829904 0.5194598 ] Loss: [-0.00446588]\n",
      "Step 63: theta: [0.7401722  0.51994095] Loss: [-0.00483961]\n",
      "Step 64: theta: [0.73803518 0.51890308] Loss: [-0.00431043]\n",
      "Step 65: theta: [0.73990673 0.5208076 ] Loss: [-0.0047702]\n",
      "Step 66: theta: [0.73821881 0.51956934] Loss: [-0.00447172]\n",
      "Step 67: theta: [0.74009228 0.51971243] Loss: [-0.00481296]\n",
      "Step 68: theta: [0.7379474  0.51938532] Loss: [-0.00438583]\n",
      "Step 69: theta: [0.73982034 0.52010044] Loss: [-0.00484159]\n",
      "Step 70: theta: [0.73841369 0.51886468] Loss: [-0.00437395]\n",
      "Step 71: theta: [0.74028513 0.52077028] Loss: [-0.00473348]\n",
      "Step 72: theta: [0.73822931 0.51955864] Loss: [-0.00447165]\n",
      "Step 73: theta: [0.74010275 0.51973469] Loss: [-0.00481497]\n",
      "Step 74: theta: [0.73795864 0.51933847] Loss: [-0.00437901]\n",
      "Step 75: theta: [0.73983144 0.52019809] Loss: [-0.0048316]\n",
      "Step 76: theta: [0.73838828 0.518962  ] Loss: [-0.00438774]\n",
      "Step 77: theta: [0.74026    0.52086753] Loss: [-0.00472684]\n",
      "Step 78: theta: [0.73821418 0.51965331] Loss: [-0.00448685]\n",
      "Step 79: theta: [0.74008789 0.51953662] Loss: [-0.00478027]\n"
     ]
    }
   ],
   "source": [
    "# use noise loss\n",
    "theta_0 = np.array([0.58,0.63])\n",
    "interp_loss = interp_noise_loss\n",
    "optim = GradientDescent(loss_fn=interp_loss, theta=theta_0, lr=0.01)\n",
    "optim_trajectory = []\n",
    "print(f'Start from theta_0: {theta_0}, Loss: {interp_loss(theta_0)}')\n",
    "optim_trajectory.append(DescentStep(theta_0.copy(), interp_loss(theta_0)))\n",
    "for i in range(80):\n",
    "    step = optim.step()\n",
    "    optim_trajectory.append(step)\n",
    "    print(f'Step {i}: theta: {step.theta} Loss: {step.value}')\n",
    "\n",
    "fig = plot_interp_loss_surface(interp_loss, shape, optim_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start from theta_0: [0.28 0.36], Loss: [0.06214212]\n",
      "Step 0: theta: [0.28153945 0.35826598] Loss: [0.06159741]\n",
      "Step 1: theta: [0.28309966 0.35649141] Loss: [0.06103576]\n",
      "Step 2: theta: [0.2846811  0.35469816] Loss: [0.0604607]\n",
      "Step 3: theta: [0.28628402 0.35288599] Loss: [0.05987189]\n",
      "Step 4: theta: [0.28790862 0.35105463] Loss: [0.05926901]\n",
      "Step 5: theta: [0.28955515 0.34920382] Loss: [0.05865171]\n",
      "Step 6: theta: [0.29122383 0.34733331] Loss: [0.05801964]\n",
      "Step 7: theta: [0.2929149  0.34544282] Loss: [0.05737244]\n",
      "Step 8: theta: [0.2946286  0.34353208] Loss: [0.05670976]\n",
      "Step 9: theta: [0.29636517 0.34160084] Loss: [0.0560312]\n",
      "Step 10: theta: [0.29812486 0.33964881] Loss: [0.05527856]\n",
      "Step 11: theta: [0.29990329 0.33709735] Loss: [0.05403922]\n",
      "Step 12: theta: [0.30457145 0.33347956] Loss: [0.05035463]\n",
      "Step 13: theta: [0.3097309  0.32963247] Loss: [0.04611303]\n",
      "Step 14: theta: [0.3150834  0.32552645] Loss: [0.04145187]\n",
      "Step 15: theta: [0.32064196 0.32115183] Loss: [0.03636552]\n",
      "Step 16: theta: [0.3258098  0.31652225] Loss: [0.03186731]\n",
      "Step 17: theta: [0.33093597 0.31282251] Loss: [0.02790152]\n",
      "Step 18: theta: [0.33600212 0.30920595] Loss: [0.02405671]\n",
      "Step 19: theta: [0.34100958 0.30567158] Loss: [0.02048524]\n",
      "Step 20: theta: [0.34440987 0.30224304] Loss: [0.01820086]\n",
      "Step 21: theta: [0.34767104 0.29895248] Loss: [0.01572946]\n",
      "Step 22: theta: [0.35088368 0.29227499] Loss: [0.01015143]\n",
      "Step 23: theta: [0.35436723 0.28546715] Loss: [0.00420704]\n",
      "Step 24: theta: [0.35812698 0.27851799] Loss: [-0.00187783]\n",
      "Step 25: theta: [0.36221834 0.2731962 ] Loss: [-0.00609295]\n",
      "Step 26: theta: [0.36466518 0.26791415] Loss: [-0.00937764]\n",
      "Step 27: theta: [0.36668692 0.26282903] Loss: [-0.01228949]\n",
      "Step 28: theta: [0.36829941 0.25790662] Loss: [-0.01399911]\n",
      "Step 29: theta: [0.3696643  0.25745871] Loss: [-0.01420489]\n",
      "Step 30: theta: [0.37102493 0.25702377] Loss: [-0.01440837]\n",
      "Step 31: theta: [0.37238143 0.25660178] Loss: [-0.01460965]\n",
      "Step 32: theta: [0.37373391 0.25619269] Loss: [-0.01480878]\n",
      "Step 33: theta: [0.3750825  0.25579646] Loss: [-0.01500584]\n",
      "Step 34: theta: [0.37642733 0.25541305] Loss: [-0.0152009]\n",
      "Step 35: theta: [0.3777685  0.25504244] Loss: [-0.01539404]\n",
      "Step 36: theta: [0.37910616 0.25468458] Loss: [-0.01558532]\n",
      "Step 37: theta: [0.38020116 0.25433944] Loss: [-0.01569784]\n",
      "Step 38: theta: [0.37928844 0.25399885] Loss: [-0.01563319]\n",
      "Step 39: theta: [0.37997187 0.25365545] Loss: [-0.0157355]\n",
      "Step 40: theta: [0.37913515 0.25331855] Loss: [-0.0156363]\n",
      "Step 41: theta: [0.3801572  0.25297369] Loss: [-0.01574825]\n",
      "Step 42: theta: [0.37927133 0.25263396] Loss: [-0.01567781]\n",
      "Step 43: theta: [0.3799908  0.25229039] Loss: [-0.01578397]\n",
      "Step 44: theta: [0.37913846 0.25195367] Loss: [-0.0156838]\n",
      "Step 45: theta: [0.38014572 0.25160884] Loss: [-0.01579561]\n",
      "Step 46: theta: [0.37928668 0.25126934] Loss: [-0.0157267]\n",
      "Step 47: theta: [0.3799709  0.25092592] Loss: [-0.01582734]\n",
      "Step 48: theta: [0.37918768 0.25058901] Loss: [-0.01573724]\n",
      "Step 49: theta: [0.38008242 0.25024465] Loss: [-0.01584719]\n",
      "Step 50: theta: [0.3792502  0.24990639] Loss: [-0.0157688]\n",
      "Step 51: theta: [0.38000999 0.24956262] Loss: [-0.01587619]\n",
      "Step 52: theta: [0.37919118 0.24922579] Loss: [-0.01578463]\n",
      "Step 53: theta: [0.38007308 0.24888146] Loss: [-0.01589405]\n",
      "Step 54: theta: [0.37926766 0.24854339] Loss: [-0.01581788]\n",
      "Step 55: theta: [0.37998821 0.24819979] Loss: [-0.01592139]\n",
      "Step 56: theta: [0.37922054 0.24786304] Loss: [-0.01583528]\n",
      "Step 57: theta: [0.38003676 0.24751899] Loss: [-0.01594294]\n",
      "Step 58: theta: [0.37925813 0.24718163] Loss: [-0.01586347]\n",
      "Step 59: theta: [0.37999576 0.24683794] Loss: [-0.0159682]\n",
      "Step 60: theta: [0.3792391  0.24650127] Loss: [-0.01588446]\n",
      "Step 61: theta: [0.38001372 0.24615739] Loss: [-0.01599061]\n",
      "Step 62: theta: [0.37926186 0.24582049] Loss: [-0.01591072]\n",
      "Step 63: theta: [0.37998935 0.24547683] Loss: [-0.01601322]\n",
      "Step 64: theta: [0.37927201 0.2451401 ] Loss: [-0.01593536]\n",
      "Step 65: theta: [0.37997817 0.24479654] Loss: [-0.01603474]\n",
      "Step 66: theta: [0.37929596 0.24445969] Loss: [-0.0159617]\n",
      "Step 67: theta: [0.37995422 0.24411636] Loss: [-0.0160547]\n",
      "Step 68: theta: [0.37933155 0.24377929] Loss: [-0.01598944]\n",
      "Step 69: theta: [0.37992032 0.2434363 ] Loss: [-0.01607346]\n",
      "Step 70: theta: [0.37937539 0.2430989 ] Loss: [-0.01601814]\n",
      "Step 71: theta: [0.37988028 0.24275633] Loss: [-0.01609152]\n",
      "Step 72: theta: [0.37942344 0.24241855] Loss: [-0.0160473]\n",
      "Step 73: theta: [0.37983821 0.24207643] Loss: [-0.01610937]\n",
      "Step 74: theta: [0.37947171 0.24173826] Loss: [-0.01607642]\n",
      "Step 75: theta: [0.37979786 0.2413966 ] Loss: [-0.01612749]\n",
      "Step 76: theta: [0.37951682 0.24105804] Loss: [-0.0161051]\n",
      "Step 77: theta: [0.37976212 0.24071681] Loss: [-0.01614621]\n",
      "Step 78: theta: [0.37955642 0.24037791] Loss: [-0.01613305]\n",
      "Step 79: theta: [0.37973278 0.24003706] Loss: [-0.01616574]\n",
      "Step 80: theta: [0.37958931 0.23969788] Loss: [-0.0162408]\n",
      "Step 81: theta: [0.37971653 0.23855085] Loss: [-0.01660091]\n",
      "Step 82: theta: [0.37965765 0.23554323] Loss: [-0.01749893]\n",
      "Step 83: theta: [0.37979882 0.23253453] Loss: [-0.01841909]\n",
      "Step 84: theta: [0.37986396 0.22952843] Loss: [-0.01932928]\n",
      "Step 85: theta: [0.37999587 0.22652352] Loss: [-0.02024471]\n",
      "Step 86: theta: [0.38017356 0.22352104] Loss: [-0.02115271]\n",
      "Step 87: theta: [0.38052747 0.22050827] Loss: [-0.02207928]\n",
      "Step 88: theta: [0.38106142 0.21747435] Loss: [-0.02393455]\n",
      "Step 89: theta: [0.38167443 0.21085849] Loss: [-0.02835691]\n",
      "Step 90: theta: [0.38241499 0.20423082] Loss: [-0.03281382]\n",
      "Step 91: theta: [0.38328332 0.19758886] Loss: [-0.03682966]\n",
      "Step 92: theta: [0.38417809 0.1929302 ] Loss: [-0.0390705]\n",
      "Step 93: theta: [0.38496635 0.188292  ] Loss: [-0.04127557]\n",
      "Step 94: theta: [0.38564858 0.18367181] Loss: [-0.04344952]\n",
      "Step 95: theta: [0.38622517 0.17906723] Loss: [-0.04531837]\n",
      "Step 96: theta: [0.38670356 0.17726133] Loss: [-0.0456298]\n",
      "Step 97: theta: [0.38715433 0.17566352] Loss: [-0.04590432]\n",
      "Step 98: theta: [0.38758067 0.17407259] Loss: [-0.04617457]\n",
      "Step 99: theta: [0.38798267 0.17248819] Loss: [-0.04644079]\n",
      "Step 100: theta: [0.38836045 0.17090993] Loss: [-0.04670324]\n",
      "Step 101: theta: [0.38871408 0.16933745] Loss: [-0.04696216]\n",
      "Step 102: theta: [0.38904367 0.16777038] Loss: [-0.0472178]\n",
      "Step 103: theta: [0.3893493  0.16620835] Loss: [-0.04747041]\n",
      "Step 104: theta: [0.38963103 0.16465099] Loss: [-0.04772021]\n",
      "Step 105: theta: [0.38988895 0.16309794] Loss: [-0.04796745]\n",
      "Step 106: theta: [0.39012312 0.16154884] Loss: [-0.04821235]\n",
      "Step 107: theta: [0.3903336  0.16000332] Loss: [-0.04845514]\n",
      "Step 108: theta: [0.39052044 0.15846101] Loss: [-0.04876055]\n",
      "Step 109: theta: [0.39070212 0.15650254] Loss: [-0.04914729]\n",
      "Step 110: theta: [0.39087728 0.15454468] Loss: [-0.04953357]\n",
      "Step 111: theta: [0.39104594 0.1525874 ] Loss: [-0.0499194]\n",
      "Step 112: theta: [0.39120808 0.15063067] Loss: [-0.0503048]\n",
      "Step 113: theta: [0.39136372 0.14867449] Loss: [-0.05068979]\n",
      "Step 114: theta: [0.39151286 0.14671882] Loss: [-0.05107438]\n",
      "Step 115: theta: [0.3916555  0.14476365] Loss: [-0.05145859]\n",
      "Step 116: theta: [0.39179163 0.14280896] Loss: [-0.05184244]\n",
      "Step 117: theta: [0.39192127 0.14085471] Loss: [-0.05222594]\n",
      "Step 118: theta: [0.39204441 0.1389009 ] Loss: [-0.05273989]\n",
      "Step 119: theta: [0.39216451 0.1357577 ] Loss: [-0.0537293]\n",
      "Step 120: theta: [0.39228408 0.13261452] Loss: [-0.05471868]\n",
      "Step 121: theta: [0.39240311 0.12947136] Loss: [-0.05570803]\n",
      "Step 122: theta: [0.39252159 0.12632822] Loss: [-0.05669737]\n",
      "Step 123: theta: [0.39263954 0.1231851 ] Loss: [-0.05768667]\n",
      "Step 124: theta: [0.39275694 0.120042  ] Loss: [-0.05867595]\n",
      "Step 125: theta: [0.39287381 0.11689892] Loss: [-0.05916245]\n",
      "Step 126: theta: [0.39310022 0.11537707] Loss: [-0.0594004]\n",
      "Step 127: theta: [0.3933804  0.11384723] Loss: [-0.0596438]\n",
      "Step 128: theta: [0.39371462 0.11230749] Loss: [-0.05989387]\n",
      "Step 129: theta: [0.39410324 0.11075595] Loss: [-0.06015183]\n",
      "Step 130: theta: [0.39454667 0.10919067] Loss: [-0.06041896]\n",
      "Step 131: theta: [0.3950454  0.10760973] Loss: [-0.06069655]\n",
      "Step 132: theta: [0.39559998 0.10601117] Loss: [-0.06098598]\n",
      "Step 133: theta: [0.39621104 0.10439301] Loss: [-0.06128865]\n",
      "Step 134: theta: [0.39687926 0.10275327] Loss: [-0.06160605]\n",
      "Step 135: theta: [0.39760541 0.10108993] Loss: [-0.06193972]\n",
      "Step 136: theta: [0.39839033 0.09940092] Loss: [-0.06223399]\n",
      "Step 137: theta: [0.39920903 0.09825715] Loss: [-0.06238725]\n",
      "Step 138: theta: [0.39991172 0.09750326] Loss: [-0.06250056]\n",
      "Step 139: theta: [0.4003192  0.09675492] Loss: [-0.06257764]\n",
      "Step 140: theta: [0.40075907 0.09597708] Loss: [-0.06266073]\n",
      "Step 141: theta: [0.40127249 0.09515766] Loss: [-0.06275821]\n",
      "Step 142: theta: [0.40186339 0.09428968] Loss: [-0.06287332]\n",
      "Step 143: theta: [0.40253637 0.09336584] Loss: [-0.06300984]\n",
      "Step 144: theta: [0.4032967  0.09237836] Loss: [-0.06317226]\n",
      "Step 145: theta: [0.4041504  0.09131899] Loss: [-0.06336592]\n",
      "Step 146: theta: [0.40510427 0.09017889] Loss: [-0.06359717]\n",
      "Step 147: theta: [0.40616594 0.0889486 ] Loss: [-0.06387359]\n",
      "Step 148: theta: [0.40734395 0.08761792] Loss: [-0.06420426]\n",
      "Step 149: theta: [0.40864778 0.08617586] Loss: [-0.06459999]\n",
      "Step 150: theta: [0.41008796 0.08461051] Loss: [-0.06507375]\n",
      "Step 151: theta: [0.41167616 0.08290898] Loss: [-0.06564106]\n",
      "Step 152: theta: [0.41342524 0.08105728] Loss: [-0.06632049]\n",
      "Step 153: theta: [0.41534942 0.0790402 ] Loss: [-0.06715788]\n",
      "Step 154: theta: [0.41726328 0.07660533] Loss: [-0.06806588]\n",
      "Step 155: theta: [0.41889736 0.0743805 ] Loss: [-0.06878612]\n",
      "Step 156: theta: [0.42027579 0.07234344] Loss: [-0.06924042]\n",
      "Step 157: theta: [0.41712687 0.07045507] Loss: [-0.06942156]\n",
      "Step 158: theta: [0.41805424 0.06821457] Loss: [-0.06998567]\n",
      "Step 159: theta: [0.41872418 0.06608063] Loss: [-0.0704695]\n",
      "Step 160: theta: [0.4191489  0.06402367] Loss: [-0.07090061]\n",
      "Step 161: theta: [0.41874155 0.06201551] Loss: [-0.0713056]\n",
      "Step 162: theta: [0.41869917 0.05996054] Loss: [-0.07173335]\n",
      "Step 163: theta: [0.41842215 0.05785799] Loss: [-0.07240624]\n",
      "Step 164: theta: [0.41798282 0.0546941 ] Loss: [-0.07343729]\n",
      "Step 165: theta: [0.41729923 0.05149631] Loss: [-0.07452348]\n",
      "Step 166: theta: [0.41636879 0.04824574] Loss: [-0.07569002]\n",
      "Step 167: theta: [0.4151874  0.04492334] Loss: [-0.07696373]\n",
      "Step 168: theta: [0.41374953 0.04150974] Loss: [-0.07837363]\n",
      "Step 169: theta: [0.41204813 0.03798514] Loss: [-0.07996526]\n",
      "Step 170: theta: [0.41036412 0.03426182] Loss: [-0.08159347]\n",
      "Step 171: theta: [0.40892765 0.03065045] Loss: [-0.08306953]\n",
      "Step 172: theta: [0.40773125 0.02713458] Loss: [-0.08442084]\n",
      "Step 173: theta: [0.40676858 0.02369824] Loss: [-0.08567237]\n",
      "Step 174: theta: [0.40603436 0.02032589] Loss: [-0.08684708]\n",
      "Step 175: theta: [0.40552434 0.01700236] Loss: [-0.08766504]\n",
      "Step 176: theta: [0.40507344 0.01471811] Loss: [-0.08820587]\n",
      "Step 177: theta: [0.4046511  0.01243948] Loss: [-0.08874171]\n",
      "Step 178: theta: [0.40425724 0.01016614] Loss: [-0.08927292]\n",
      "Step 179: theta: [0.40389179 0.00789772] Loss: [-0.08979981]\n",
      "Step 180: theta: [0.4035547  0.00563387] Loss: [-0.09032272]\n",
      "Step 181: theta: [0.40324591 0.00337423] Loss: [-0.09084198]\n",
      "Step 182: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 183: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 184: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 185: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 186: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 187: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 188: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 189: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 190: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 191: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 192: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 193: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 194: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 195: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 196: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 197: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 198: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 199: theta: [0.40296536 0.00111845] Loss: [-0.09135791]\n"
     ]
    }
   ],
   "source": [
    "# use noise loss\n",
    "theta_0 = np.array([0.28,0.36])\n",
    "interp_loss = interp_noise_loss\n",
    "optim = GradientDescent(loss_fn=interp_loss, theta=theta_0, lr=0.01)\n",
    "optim_trajectory = []\n",
    "print(f'Start from theta_0: {theta_0}, Loss: {interp_loss(theta_0)}')\n",
    "optim_trajectory.append(DescentStep(theta_0.copy(), interp_loss(theta_0)))\n",
    "for i in range(200):\n",
    "    step = optim.step()\n",
    "    optim_trajectory.append(step)\n",
    "    print(f'Step {i}: theta: {step.theta} Loss: {step.value}')\n",
    "\n",
    "fig = plot_interp_loss_surface(interp_loss, shape, optim_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start from theta_0: [0.28 0.36], Loss: [0.06214212]\n",
      "Step 0: theta: [0.28531776 0.36204428] Loss: [0.06162735]\n",
      "Step 1: theta: [0.28643368 0.36003912] Loss: [0.06115729]\n",
      "Step 2: theta: [0.28581477 0.35644442] Loss: [0.0605978]\n",
      "Step 3: theta: [0.27698963 0.34421153] Loss: [0.05860903]\n",
      "Step 4: theta: [0.2595599  0.32766386] Loss: [0.04970715]\n",
      "Step 5: theta: [0.25026392 0.31709125] Loss: [0.04465323]\n",
      "Step 6: theta: [0.25432416 0.31764088] Loss: [0.04538479]\n",
      "Step 7: theta: [0.24570206 0.30537311] Loss: [0.0388733]\n",
      "Step 8: theta: [0.25081778 0.3067029 ] Loss: [0.03985342]\n",
      "Step 9: theta: [0.2475895  0.29954303] Loss: [0.03637483]\n",
      "Step 10: theta: [0.23754481 0.28566207] Loss: [0.03182949]\n",
      "Step 11: theta: [0.23358001 0.27656933] Loss: [0.02995273]\n",
      "Step 12: theta: [0.23022583 0.26890683] Loss: [0.02879002]\n",
      "Step 13: theta: [0.22928522 0.26369508] Loss: [0.02772577]\n",
      "Step 14: theta: [0.22854052 0.25871609] Loss: [0.02649989]\n",
      "Step 15: theta: [0.23108484 0.25561581] Loss: [0.02480357]\n",
      "Step 16: theta: [0.23471137 0.2532246 ] Loss: [0.02304685]\n",
      "Step 17: theta: [0.2384802  0.25057789] Loss: [0.02102861]\n",
      "Step 18: theta: [0.24452884 0.24978689] Loss: [0.01917789]\n",
      "Step 19: theta: [0.24489196 0.24315655] Loss: [0.01609553]\n",
      "Step 20: theta: [0.25500816 0.24625922] Loss: [0.01497515]\n",
      "Step 21: theta: [0.25103886 0.23525625] Loss: [0.01090247]\n",
      "Step 22: theta: [0.25633499 0.23382575] Loss: [0.00922969]\n",
      "Step 23: theta: [0.24871261 0.22015059] Loss: [0.0039358]\n",
      "Step 24: theta: [0.2389682  0.20495976] Loss: [-0.00309781]\n",
      "Step 25: theta: [0.23738338 0.19710397] Loss: [-0.00623651]\n",
      "Step 26: theta: [0.24116398 0.19749684] Loss: [-0.00613933]\n",
      "Step 27: theta: [0.24291698 0.19847324] Loss: [-0.00557494]\n",
      "Step 28: theta: [0.24672956 0.20151743] Loss: [-0.00386724]\n",
      "Step 29: theta: [0.23609245 0.18824176] Loss: [-0.00840426]\n",
      "Step 30: theta: [0.23888512 0.18760685] Loss: [-0.00880895]\n",
      "Step 31: theta: [0.23748492 0.18276102] Loss: [-0.00993417]\n",
      "Step 32: theta: [0.23315955 0.17497189] Loss: [-0.01174355]\n",
      "Step 33: theta: [0.23547658 0.1734057 ] Loss: [-0.01242148]\n",
      "Step 34: theta: [0.24087919 0.17491606] Loss: [-0.01223422]\n",
      "Step 35: theta: [0.24707327 0.17983729] Loss: [-0.00952449]\n",
      "Step 36: theta: [0.24731991 0.17941792] Loss: [-0.00961488]\n",
      "Step 37: theta: [0.23387239 0.1649466 ] Loss: [-0.0147836]\n",
      "Step 38: theta: [0.23489236 0.16205836] Loss: [-0.01573725]\n",
      "Step 39: theta: [0.24158334 0.16483203] Loss: [-0.0151906]\n",
      "Step 40: theta: [0.23308275 0.15456484] Loss: [-0.01770429]\n",
      "Step 41: theta: [0.23439219 0.15255901] Loss: [-0.0183125]\n",
      "Step 42: theta: [0.23240614 0.14753159] Loss: [-0.01968706]\n",
      "Step 43: theta: [0.22859474 0.14093006] Loss: [-0.02181748]\n",
      "Step 44: theta: [0.22538732 0.13516302] Loss: [-0.02379542]\n",
      "Step 45: theta: [0.23143656 0.13925028] Loss: [-0.02210758]\n",
      "Step 46: theta: [0.22948336 0.13538408] Loss: [-0.02330476]\n",
      "Step 47: theta: [0.22235823 0.12657868] Loss: [-0.02688965]\n",
      "Step 48: theta: [0.22480431 0.12746665] Loss: [-0.02620849]\n",
      "Step 49: theta: [0.22426977 0.12548727] Loss: [-0.02690713]\n",
      "Step 50: theta: [0.22585828 0.12573599] Loss: [-0.02655429]\n",
      "Step 51: theta: [0.2249249  0.12356021] Loss: [-0.02737933]\n",
      "Step 52: theta: [0.23048668 0.12796992] Loss: [-0.02516015]\n",
      "Step 53: theta: [0.22507851 0.12149344] Loss: [-0.02797765]\n",
      "Step 54: theta: [0.22714176 0.12256605] Loss: [-0.02724616]\n",
      "Step 55: theta: [0.22484101 0.11934668] Loss: [-0.02883968]\n",
      "Step 56: theta: [0.20952032 0.1015476 ] Loss: [-0.04011852]\n",
      "Step 57: theta: [0.19517594 0.08177227] Loss: [-0.05031176]\n",
      "Step 58: theta: [0.18910007 0.06680288] Loss: [-0.05431432]\n",
      "Step 59: theta: [0.19345246 0.06486134] Loss: [-0.0561725]\n",
      "Step 60: theta: [0.19160467 0.05682968] Loss: [-0.05894004]\n",
      "Step 61: theta: [0.1870993  0.04500651] Loss: [-0.06371966]\n",
      "Step 62: theta: [0.19698976 0.04781769] Loss: [-0.06447796]\n",
      "Step 63: theta: [0.20520785 0.04918724] Loss: [-0.0648076]\n",
      "Step 64: theta: [0.20630802 0.04536519] Loss: [-0.06651599]\n",
      "Step 65: theta: [0.21729611 0.05163469] Loss: [-0.06455583]\n",
      "Step 66: theta: [0.21395945 0.04377467] Loss: [-0.0674666]\n",
      "Step 67: theta: [0.22676058 0.05223957] Loss: [-0.06487503]\n",
      "Step 68: theta: [0.23073183 0.05224905] Loss: [-0.06508548]\n",
      "Step 69: theta: [0.22951637 0.04725627] Loss: [-0.0666656]\n",
      "Step 70: theta: [0.22892125 0.0430597 ] Loss: [-0.06804215]\n",
      "Step 71: theta: [0.2221263  0.03283097] Loss: [-0.07361535]\n",
      "Step 72: theta: [0.22081894 0.02565659] Loss: [-0.07826874]\n",
      "Step 73: theta: [0.21648349 0.01580256] Loss: [-0.08431837]\n",
      "Step 74: theta: [0.21462779 0.00840003] Loss: [-0.08819883]\n",
      "Step 75: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 76: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 77: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 78: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 79: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 80: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 81: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 82: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 83: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 84: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 85: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 86: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 87: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 88: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 89: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 90: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 91: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 92: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 93: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 94: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 95: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 96: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 97: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 98: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 99: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 100: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 101: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 102: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 103: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 104: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 105: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 106: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 107: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 108: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 109: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 110: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 111: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 112: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 113: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 114: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 115: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 116: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 117: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 118: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 119: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 120: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 121: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 122: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 123: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 124: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 125: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 126: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 127: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 128: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 129: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 130: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 131: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 132: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 133: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 134: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 135: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 136: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 137: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 138: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 139: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 140: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 141: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 142: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 143: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 144: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 145: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 146: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 147: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 148: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n",
      "Reached function support boundary!\n",
      "At function support boundary, using last step.\n",
      "Step 149: theta: [0.21538915 0.00321725] Loss: [-0.09101263]\n"
     ]
    }
   ],
   "source": [
    "# use noise loss + noise gradient steps\n",
    "theta_0 = np.array([0.28,0.36])\n",
    "interp_loss = interp_noise_loss\n",
    "optim = GradientDescent(loss_fn=interp_loss, theta=theta_0, lr=0.01, gradient_noise_std=0.7)\n",
    "optim_trajectory = []\n",
    "print(f'Start from theta_0: {theta_0}, Loss: {interp_loss(theta_0)}')\n",
    "optim_trajectory.append(DescentStep(theta_0.copy(), interp_loss(theta_0)))\n",
    "for i in range(150):\n",
    "    step = optim.noisy_step()\n",
    "    optim_trajectory.append(step)\n",
    "    print(f'Step {i}: theta: {step.theta} Loss: {step.value}')\n",
    "\n",
    "fig = plot_interp_loss_surface(interp_loss, shape, optim_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for final plot:\n",
    "\n",
    "# - use starting point as in figure\n",
    "# - Crop perlin noise to contain roughly only the gradient trajectory\n",
    "# - for SGD: use noisy steps with std=0.7\n",
    "# - for SubGD: calculate subspace as line from start point to end point\n",
    "# - project gradient steps of SGD onto this line\n",
    "# - use plotly for final plot"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "efb944956e68c5a4a3f866131a290f604672ac43206fde6373334f1e4d6c02e5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
